{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"jounetsu","url":""},"pages":[],"posts":[{"title":"四.基础构建模块","slug":"四-基础构建模块","date":"2020-05-06T03:17:40.000Z","updated":"2020-05-14T03:32:39.854Z","comments":true,"path":"2020/05/06/四-基础构建模块/","link":"","permalink":"/2020/05/06/四-基础构建模块/","excerpt":"","text":"1.同步容器类​ 同步容器类包括Vector和Hashtable等，这些同步的封装器类是由Collections.synchronizedXxx等工厂方法创建的。这些类实现线程安全的方式是：将它们的状态封装起来，并对每个公有方法都进行同步，使得每次只有一个线程能访问容器的状态。 1.1同步容器类的问题​ 同步容器类都是线程安全的，但在某些情况下可能需要额外的客户端加锁来保护复合操作。容器上常见的复合操作包括：迭代（反复访问元素，直到遍历完容器中所有的元素）、跳转（根据制定顺序找到当前元素的下一个元素）以及条件运算。在同步容器类中，这些复合操作在没有客户端加锁的情况下仍然是线程安全的，但当其他线程并发地修改容器时，它们可能会表现出意料之外的行为。 2.并发容器​ 通过并发容器来代替同步容器，可以极大地提高伸缩性并降低风险。 2.1 ConcurrentHashMap​ 与HashMap一样，ConcurrentHashMap也是一个基于散列的Map，但它使用了一种完全不同的加锁策略来提供更高的并发性和伸缩性。ConcurrentHashMap并不是将每个方法都在同一个锁上同步并使得每次只能有一个线程访问容器，而是使用一种粒度更细的加锁机制来实现更大程序的共享，这种机制称为分段锁（Lock Striping）。 ​ ConcurrentHashMap与其他并发容器一起增强了同步容器类：它们提供的迭代器不会抛出ConcurrentModificationException，因此不需要在迭代过程中对容器加锁。ConcurrentHashMap返回的迭代器具有弱一致性（Weakly Consistent），而并非“及时失败”。弱一致性的迭代器可以容忍并发的修改，当创建迭代器时会遍历已有的元素，并可以（但是不保证）在迭代器被构造后将修改操作反映给容器。 ​ 在ConcurrentHashMap中没有实现对Map加锁以提供独占访问。在Hashtable和synchronizedMap中，获得Map的锁能防止其他线程访问这个Map。 2.2 CopyOnWriteArrayList​ CopyOnWriteArrayList用于替代同步List，在某些情况下它提供了更好的并发性能，并且在迭代期间不需要对容器进行加锁或复制。（类似地，CopyOnWriteArraySet的作用是替代同步Set） ​ “写入时复制（Copy-On-Write）”容器的线程安全性在于，只要正确地发布一个事实不可变的对象，那么在访问该对象时就不再需要进一步的同步。在每次修改时，都会创建并重新发布一个新的容器副本，从而实现可变性。“写入时复制”容器的迭代器保留一个指向底层基础数组的引用，这个数组当前位于迭代器的起始位置，由于它不会被修改，因此在对其进行同步时只需确保数组内容的可见性。因此，多个线程可以同时对这个容器进行迭代，而不会彼此干扰或者与修改容器的线程相互干扰。“写入时复制”容器返回的迭代器不会抛出ConcurrentModificationExecption，并且返回的元素与迭代器创建时的元素完全一致，而不必考虑之后修改操作所带来的影响。 3.阻塞队列和生产者-消费者模式​ 阻塞队列提供了可阻塞的put和take方法，以及支持定时的offer和poll方法。如果队列已经满了，那么put方法将阻塞直到有空间可用；如果队列为空，那么take方法将会阻塞直到有元素可用。队列可以是有界的也可以是无界的，无界队列永远都不会充满，所以无界队列的put方法永远也不会阻塞。 ​ 在基于阻塞队列构建的生产者-消费者设计中，当数据生成时，生产者把数据放入队列，而当消费者准备处理数据时，将从队列中获取数据。生产者不需要知道消费者的标识或数量，或者它们是否是唯一的生产者，而只需将数据放入队列即可。同样，消费者也不需要知道生产者是谁，或者工作来自何处。一种常见的生产者-消费者设计模式就是线程池与工作队列的组合，在Executor任务执行框架中就体现了这种模式。 ​ 阻塞队列提供了一个offer方法，如果数据项不能被添加到队列中，那么将返回一个失败状态。 ​ 在构建高可靠的应用程序时，有界队列是一种强大的资源管理工具：它们能抑制并防止产生过的工作项，使应用程序在负荷过载的情况下变得更加健壮。 ​ 在类库中包含了BlockingQueue的多种实现，其中，LinkedBlockingQueue和ArrayBlockingQueue是FIFO队列，二者分别与LinkedList和ArrayList类似，但比同步List拥有更好的并发性能。PriorityBlockingQueue是一个按优先级排序的队列。 ​ 最后一个BlockingQueue实现是SynchronousQueue，实际上它不是一个真正的队列，因为它不会为队列中元素维护存储空间。与其他队列不同的是，它维护一组线程，这些线程在等待把元素加入或移出队列。 3.1串行线程关闭​ 对于可变对象，生产者-消费者这种设计与阻塞队列一起，促进了串行线程封闭，从而将对象所有权从生产者交付给消费者。线程封闭对象只能由单个线程拥有，但可以通过安全发布对象来“转移”所有权。在转移所有权之后，也只有另一个线程能获得这个对象的访问限制，并且发布对象的线程不会再访问它。 3.2双端队列与工作密取​ 双端队列适用于另一种相关模式，即工作密取。在生产者-消费者设计中，所有消费者有一个共享的工作队列，而在工作密取设计中，每个消费者都有自己的双端队列。如果一个消费者完成了自己双端队列中的全部工作，那么它可以从其他消费者双端队列末尾秘密地获取工作。 4.阻塞方法与中断方法​ 线程可能会阻塞或暂停执行，原因有多种：等待I/O操作结束，等待获得一个锁，等待从Thread.sleep方法中醒来，或是等待另一个线程的计算结果。当线程阻塞时，它通常被挂起，并处于某种阻塞状态（BLOCKED、WAITING或TIMED_WAITING）。阻塞线程与执行时间很长的普通操作的差别在于，被阻塞的队列必须等待某个不受它控制的事件发生后才能继续执行。当某个外部事件发生时，线程被置回RUNNABLE状态，并可以再次被调度执行。 ​ Thread提供了interrupt方法，用于中断线程或者查询这个线程是否已经被中断。每个线程都有一个布尔类型的属性，表示线程的中断状态，当中断线程时讲设置这个状态。 ​ 中断是一种协作机制。一个线程不能强制其他线程停止正在执行的操作而去执行求他的操作。 ​ 挡在代码中调用了一个将抛出InterruptedExcepiton异常的方法时，自己的方法也就变成了一个阻塞方法，并且必须要处理对应中断的响应。对于库代码来说，有两种基本选择： ​ 传递InterruptedException。只需把InterruptedException传递给方法的调用者。传递InterruptedException的方法包括，根本不捕获该异常，或者捕获该异常，然后在执行某种简单的清理工作后再次抛出该异常。 ​ 恢复中断。有时候不能抛出InterruptedException，例如当代码是Runnable的一部分时。在这些情况下，必须捕获InterruptedException，并通过调用当前线程上的interrupt方法恢复中断状态，这样在调用栈中更高层的代码将看到引发了一个中断。 123456789101112public class TaskRunnable implements Runnable &#123; BlockingQueue&lt;Task&gt; queue; ... public void run() &#123; try &#123; processTask(queue.take()); &#125; catch (InterruptedException e) &#123; //恢复中断的状态 Thread.currentThread().interrupt(); &#125; &#125;&#125; 5.同步工具类5.1闭锁​ 闭锁是一种同步工具类，可以延迟线程的进度直到其到达终止状态。当闭锁到达结束状态之后，将不会再改变状态。闭锁可以确保某些活动直到其他活动都完成后才继续执行。 ​ CountDownLatch是一种灵活的闭锁实现，它可以使一个或多个线程等待一组事件发生。闭锁状态包括一个计数器，该计数器被初始化为一个正数，表示需要等待的事件数量。countDown方法递减计数器，表示有一个事件已经发生了，而await方法等待计数器达到零，这表示所有需要等待的事件都已经发生。如果计数器的值非零，那么await会一直阻塞到计数器为零，或者等待中的线程中断，或者等待超时。 123456789101112131415161718192021222324public long timeTasks(int nThreads, Runnable task) throws InterruptedException&#123; final CountDownLatch startGate = new CountDownLatch(1); final CountDownLatch endGate = new CountDownLatch(nThreads); for (int i = 0; i &lt; nThreads; i++) &#123; Thread t = new Thread(() -&gt; &#123; try &#123; startGate.await(); try&#123; task.run(); &#125; finally &#123; endGate.countDown(); &#125; &#125; catch (InterruptedException ignored) &#123;&#125; &#125;); t.start(); &#125; long start = System.nanoTime(); startGate.countDown(); endGate.await(); long end = System.nanoTime(); return end - start;&#125; #####5.2FutureTask ​ FutureTask也可以用做闭锁。FutureTask表示的计算是通过Callable来实现的，相当于一种可生成结果的Runnable，并且可以处于以下3种状态：等待运行（Waiting to run）,正在运行（Running）和运行完成（Completed）。 ​ Future.get()的行为取决于任务的状态。如果任务已经完成，那么get会立即返回结果，否则get将阻塞直到任务进入完成状态，然后返回结果或者抛出异常。 5.3信号量​ 计数信号量（Counting Semaphore）用来控制同时访问某个特定资源的操作数量，或者同时执行某个执行操作的数量。计数信号量还可以用来实现某种资源池，或者对容器施加边界。 ​ Semaphore中管理着一组虚拟的许可（permit），许可的初始数量可通过构造函数来指定。在执行操作时可以首先获得许可（只要还有剩余的许可），并在使用以后释放许可。如果没有许可，那么acquire将阻塞直到所有许可（或许直到被中断或操作超时）。release方法将返回一个许可给信号量。 1234567891011121314151617181920212223242526272829303132public class BoundedHashSet&lt;T&gt; &#123; private final Set&lt;T&gt; set; private final Semaphore sem; public BoundedHashSet(int bound) &#123; this.set = Collections.synchronizedSet(new HashSet&lt;T&gt;()); this.sem = new Semaphore(bound); &#125; public boolean add(T o) throws InterruptedException &#123; sem.acquire(); boolean wasAdded = false; try &#123; wasAdded = set.add(o); return wasAdded; &#125; finally &#123; if (!wasAdded) &#123; sem.release(); &#125; &#125; &#125; public boolean remove(T o) &#123; boolean wasRemove = set.remove(o); if (wasRemove) &#123; sem.release(); &#125; return wasRemove; &#125;&#125; 5.4栅栏​ 栅栏（Barrier）类似于闭锁，它能阻塞一组线程直到某个事件发生。栅栏与闭锁的关键区别在于，所有线程必须同时到达栅栏位置，才能继续执行。闭锁用于等待事件，而栅栏用于等待其他线程。 ​ CyclicBarrier可以使一定数量的参与方式反复地在栅栏位置汇集，它在并行迭代算法中非常有用：这种算法通常将一个问题拆分成一系列相互独立的子问题。当线程到达栅栏位置时将调用await方法，这个方法将阻塞直到所有线程到达栅栏位置。如果所有线程到达了栅栏位置，那么栅栏将打开，此时所有线程都被释放，而栅栏将重置以便下次使用。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"三.对象的组合","slug":"三-对象的组合","date":"2020-04-27T08:30:38.000Z","updated":"2020-05-06T03:16:24.195Z","comments":true,"path":"2020/04/27/三-对象的组合/","link":"","permalink":"/2020/04/27/三-对象的组合/","excerpt":"","text":"1.设计线程安全的类​ 在设计线程安全类的过程中，需要包含以下三个基本要素： 找出构成对象状态的所有变量。 找出约束状态变量的不变性条件。 建立对象状态的并发访问管理策略。 1.1收集同步需求​ 要确保类的线程安全性，就需要确保它的不变性条件不会在并发访问的情况下被破坏，这就需要对其状态进行推断。对象与变量都有一个状态空间，即所有可能的取值。状态空间越小，就越容易判断线程的状态。final类型的域使用得越多，就越能简化对象可能状态的分析过程。（在极端的情况中，不可变对象只有唯一的状态。） 1.2依赖状态的操作​ 类的不变性条件与后验条件约束了在对象上有哪些状态和状态转换是有效的。在某些对象的方法中还包含一些基于状态的先验条件。例如，不能从空队列中移除一个元素，在删除元素之前，队列必须处于非空的状态。如果在某个操作中包含有基于状态的先验条件，那么这个操作就被称为依赖状态的操作。 ​ 在单线程程序中，如果某个操作无法满足先验条件，那么就只能失败。但在并发程序中，先验条件可能会由于其他线程执行的操作而变成真。在并发程序中要一直等到先验条件为真，然后再执行该操作。 1.3状态的所有权​ 许多情况下，所有权与封装性总是相互关联的：对象封装它拥有的状态，反之也成立，即对它封装的状态拥有所有权。状态变量的所有者将决定采用何种加锁协议来维持变量状态的完整性。所有权意味着控制权。 2.实例封闭​ 封装简化了线程安全类的实现过程，它提供了一种实例封闭机制，通常也简称为“封闭”。当一个对象被封装到另一个对象中时，能够访问被封装对象的所有代码路径都是已知的。 ​ 将数据封装在对象内部，可以将数据的访问限制在对象的方法上，从而更容易确保线程在访问数据时总能持有正确的锁。 ​ 实例封闭式构建线程安全类的一种最简单方式，它还使得在锁策略的选择上拥有了更多的灵活性。只要自始至终都使用同一个锁，就可以保护状态。实例封闭还使得不同的状态变量可以由不同的锁来保护。 2.1Java监视器模式​ Java监视器模式仅仅是一种编写代码的约定，对于任何一种锁对象，只要自始至终都使用该锁对象，都可以用来保护对象的状态。 3.线程安全性的委托​ 如果一个变量是线程安全的，并且没有任何不变性条件来约束它的值，在变量的操作上也不存在任何不允许的状态转换，那么就可以安全地发布这个变量。 4.在现有的线程安全类中添加功能4.1客户端加锁机制123456789101112131415@hreadSafepublic class ListHelper&lt;E&gt; &#123; public List&lt;E&gt; list = Collections.synchronizedList(new ArrayList&lt;E&gt;()); ... public boolean pufIfAbsent(E x) &#123; synchronized (list) &#123; boolean absent = !list.contains(x); if (absent) &#123; list.add(x); &#125; return absent; &#125; &#125;&#125; 4.2组合12345678910111213141516@ThreadSafepublic class ImprovedList&lt;T&gt; implements List&lt;T&gt; &#123; private final List&lt;T&gt; list; public ImprovedList(List&lt;T&gt; list) &#123; this.list = list; &#125; public synchronized boolean putIfAbsent(T x) &#123; boolean contains = list.contains(x); if(contains) &#123; list.add(x); &#125; return !contains; &#125;&#125;","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"二.对象的共享","slug":"二-对象的共享","date":"2020-04-25T02:22:46.000Z","updated":"2020-04-28T02:24:33.705Z","comments":true,"path":"2020/04/25/二-对象的共享/","link":"","permalink":"/2020/04/25/二-对象的共享/","excerpt":"","text":"1.可见性1.1失效数据​ 在缺乏同步的程序中可能产生错误的一种情况：失效数据。当读线程查看某个变量时，可能会得到一个已经失效的值。更糟糕的是，失效值可能不会同时出现：一个线程可能获得某个变量的最新值，而获得另一个变量的失效值。 1.2非原子的64位操作​ 当线程在没有同步的情况下读取变量时，可能会得到一个失效值，但至少这个值是由之前某个线程设置的值，而不是一个随机值。这种安全性保证也被称为最低安全性。 ​ 最低安全性适用于绝大多数变量，但是存在一个例外：非volatile类型的64位数值变量(double和long)。Java内存模型要求，变量的读取操作和写入操作都必须是原子操作，但对于非volatile类型的long和double变量，JVM允许将64位的读操作或血操作分解为两个32位的操作。因此，即使不考虑失效数据问题，在多线程程序中使用共享且可变的long和double等类型的变量也是不安全的，除非用关键字volatile来生命它们，或者用锁保护起来。 1.3加锁与可见性​ 内置锁可以用于确保某个线程以一种可预测的方式来查看另一个线程的执行结果。 ​ 加锁的含义不仅仅局限于互斥行为，也包括内存可见性。为了确保所有线程都能看到共享变量的最新值，所有执行读操作或写操作的线程都必须在同一个锁上同步。 1.4Volatile变量​ Java语言提供了一种稍弱的同步机制，即volatile变量，用来确保将变量的更新操作通知到其他线程。当把变量声明为volatile类型后，编译器与处理器都会注意到这个变量是共享的，因此不会将该变量上的操作与其他内存一起重排序。volatile变量不会被缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。 ​ 仅当volatile变量能简化代码的实现以及对同步策略的验证时，才应该使用它们。如果在验证正确性时需要对可见性进行复杂的判断，那么就不要使用volatile变量。volatile变量的正确使用方式包括：确保它们自身状态的可见性，确保它们所引用对象的状态的可见性，以及标识一些重要的程序生命周期事件的发生（例如，初始化或关闭）。 ​ 加锁机制既可以确保可见性又可以确保原子性，而volatile变量只能确保可见性。 ​ 当且仅当满足以下所有条件时，才应该使用volatile变量： 对变量的写入操作不依赖变量的当前值，或者能确保只有单个线程更新变量的值 该变量不会与其他状态变量一起纳入不变性条件中。 在访问变量时不需要加锁。 2.发布与逸出​ “发布（Publish）一个对象的意思是指，使对象能够在当前作用域之外的代码中使用。当某个不应该发布的对象被发布时，这种情况就被称为逸出（Escape）。 3.线程封闭​ 当访问共享的可变数据时，通常需要使用同步。一种避免使用同步的方式就是不共享数据。如果仅在单线程内访问数据，就不需要同步。这种技术被称为线程封闭（Thread Confinement），它是实现线程安全性的最简单方式之一。当某个对象封闭在一个线程中时，这种用法将自动实现线程安全性，即使被封闭的对象本身不是线程安全的。 3.1Ad-hoc线程封闭​ Ad-hoc线程封闭是指，维护线程封闭性职责完全由程序实现来承担。 3.2栈封闭​ 栈封闭是线程封闭的一种特例，在栈封闭中，只能通过局部变量才能访问对象。 3.3ThreadLocal类​ 维持线程封闭性的一种更规范方法是使用ThreadLocal，这个类能使线程中的某个值与保持值的对象关联起来。ThreadLocal提供了get与set等访问接口或方法，这些方法为每个使用该变量的线程都存有一份独立的副本，因此get总是返回由当前执行线程在调用set时设置的最新值。 4.不变性​ 满足同步需求的另一种方法是使用不可变对象。如果某个对象在被创建后其状态就不能被修改，那么这个对象就被称为不可变对象。线程安全性是不可变对象的固有属性之一，不可变对象一定是线程安全的。 ​ 当满足以下条件时，对象才是不可变的： 对象创建以后其状态就不能修改。 对象的所有域都是final类型。 对象是正确创建的（在对象的创建期间，this引用没有逸出）。 4.1Final域​ final类型的域是不能修改的，在Java内存模型中，final域还有着特殊的语义。final域能确保初始化过程的安全性，从而可以不受限制地访问不可变对象，并在共享这些对象时无须同步。 5.安全发布5.1不可变对象与初始化安全性​ 任何线程都可以在不需要额外同步的情况下安全地访问不可变对象，即使在发布这些对象时没有使用同步。这种保证还将延伸到被正确创建对象中所有final类型的域。在没有额外同步的情况下，也可以安全地访问final类型的域。然而，如果final类型的域所指向的是可变对象，那么在访问这些域所指向的对象的状态时仍然需要同步。 5.2安全发布的常用模式​ 要安全地发布一个对象，对象的引用以及对象的状态必须同时对其他线程可见。一个正确构造的对象可以通过以下方式来安全地发布： 在静态初始化函数中初始化一个对象引用。 将对象的引用保存到volatile类型的域或者AtomicReferance对象中。 将对象的引用保存到某个正确构造对象的final类型域中。 将对象的引用保存到一个由锁保护的域中。 线程安全库中的容器类提供了以下的安全发布保证： 通过将一个键或值放入HashTable、synchronizedMap或者ConcurrentMap中，可以安全地将它发布给任何从这些容器中访问它的线程（无论是直接访问还是通过迭代器访问）。 通过将某个元素放入Vector、CopyOnWriteArrayList、CopyOnWriteArraySet、synchronizedList或者synchronizedSet中，可以将该元素安全地发布到任何从这些容器中访问该元素的线程。 通过将某个元素放入BlockingQueue或者ConcurrentLinkedQueue中，可以将该元素安全地发布到任何从这些队列中访问该元素的线程。 通常，要发布一个静态构造的对象，最简单和最安全的方式是使用静态的初始化器： 1public static Holder holder = new Holder(42); 5.3事实不可变对象​ 所有的安全发布机制都能确保，当对象的引用对所有访问该对象的线程可见时，对象发布时的状态对于所有线程也将是可见的，并且如果对象状态不会再改变，那么就足以确保任何访问都是安全的。 ​ 如果对象从技术上来看是可变的，但其状态再发布后不会再改变，那么把这种对象称为“事实不可变对象（Effectively Immutable Object）”。 ​ 在没有额外的同步的情况下，任何线程都可以安全地使用被安全发布的事实不可变对象。 5.4可变对象​ 如果对象在构造后可以修改，那么安全发布智能确保“发布当时”状态的可见性。对于可变对象，不仅在发布对象时需要使用同步，而且在每次对象访问时同样需要使用同步来确保后续修改操作的可见性。要安全地共享可变对象，这些对象就必须被安全地发布，并且必须是线程安全的或者由某个锁保护起来。 ​ 对象的发布需求取决于它的可变性： 不可变对象可以通过任意机制来发布。 事实不可变对象必须通过安全方式来发布。 可变对象必须通过安全方式来发布，并且必须是线程安全的或者由某个锁保护起来。 5.5安全地共享对象​ 在并发程序中使用和共享对象时，可以使用一些实用的策略，包括： ​ 线程封闭。线程封闭的对象只能由一个线程拥有，对象被封闭在该线程中，并且只能由这个线程修改。 ​ 只读共享。在没有额外同步的情况下，共享的只读对象可以由多个线程并发访问，但任何线程都不能修改它。共享的只读对象包括不可变对象和事实不可变对象。 ​ 线程安全共享。线程安全的对象在其内部实现同步，因此多个线程可以通过对象的公有接口来进行访问而不需要进一步的同步。 ​ 保护对象。被保护的对象只能通过持有特定的锁来访问。保护对象包括封装在其他线程安全对象中的对象，以及已发布的并且由某个特定锁保护的对象。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"一.线程安全性","slug":"一-线程安全性","date":"2020-04-20T06:12:46.000Z","updated":"2020-04-22T11:15:23.960Z","comments":true,"path":"2020/04/20/一-线程安全性/","link":"","permalink":"/2020/04/20/一-线程安全性/","excerpt":"","text":"1.定义​ 正确性的含义：某个类的行为与其规范完全一致。在良好的规范中通常会定义各种不变性条件来约束对象的状态，以及定义各种后验条件来描述对象操作的结果。 ​ 线程安全性：当多个线程访问某个类时，不管运行环境采用何种调度方式或者这些线程将如何交替执行，并且在调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么就称这个类是线程安全的。 ​ 无状态对象一定是线程安全的。 2.原子性2.1竞态条件​ 当某个计算的正确性取决于多个线程的交替执行时序时，那么就会发生竞态条件。最常见的竞态条件类型就是”先检查后执行（Check-Then-Act）“操作，即通过一个可能失效的观测结果来决定下一步的动作。 2.2复合操作​ 假定有两个操作A和B，如果以A的线程来看，当另一个线程执行B时，要么将B全部执行完，要么完全不执行B，那么A和B对彼此来说是原子的。原子操作是指，对于访问一个状态的所有操作（包括该操作本身）来说，这个操作是一个以原子方式执行的操作。 ​ 复合操作：包含了一组必须以原子方式执行的操作以及确保线程安全性。 3.加锁机制3.1内置锁​ Java提供了一种内置的锁机制来支持原子性：同步代码块。同步代码块包括两部分：一个作为锁的对象引用，一个作为由这个锁保护的代码。以关键字synchronized来修饰的方法就是一种横跨整个方法体的同步代码块，其中该同步代码块的锁就是方法调用所在的对象。 123synchronized(lock)&#123;//访问或修改由锁保护的共享状态&#125; ​ 每个Java对象都可以用做一个实现同步的锁，这些锁被称为内置锁或监视器锁。线程在进入同步代码块之前会自动获得锁，并且在退出同步代码块时自动释放锁，而无论是通过正常的控制路径退出，还是通过从代码中抛出异常退出。获得内置锁的唯一途径就是进入由这个锁保护的同步代码块或方法。 ​ Java的内置锁是互斥锁，意味着只有一个线程能持有这种锁。 3.2重入​ 当某个线程请求一个由其他线程持有的锁时，发出请求的线程就会阻塞。然而，由于内置锁是可重入的，因此如果某个线程试图获得一个已经由它自己持有的锁，那么这个请求就会成功。 ​ 重入的一种实现方法是，为每个锁关联一个获取计数值和一个所有者线程。当计数值为0时，这个锁就被认为是没有被任何线程持有。当线程请求一个未被持有的锁时，JVM将记下锁的持有者，并且将获取计数器置为1。如果同一个线程再次获取这个锁，计数值将递增，而当线程退出同步代码块时，计数器将递减。计数器值为0时，这个锁将被释放。 4.用锁来保护状态​ 由于锁能使其保护的代码路径以串行形式来访问，因此可以通过锁来构造一些协议以实现对共享状态的独占访问。只要始终遵循这些协议，就能确保状态的一致性。 ​ 对于可能被多个线程同时访问的可变状态变量，在访问它时都需要持有同一个锁，在这种情况下，我们称状态变量是由这个锁保护的。 ​ 对于每个包含多个变量的不变性条件，其中涉及的所有变量都需要由同一个锁来保护。 5.活跃与性能​ 当执行时间较长的计算或者可能无法快速完成的操作时（例如，网络I/O或者控制台I/O），一定不要持有锁。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"redis设计与实现-单机数据库的实现","slug":"redis设计与实现-单机数据库的实现","date":"2019-12-29T08:44:46.000Z","updated":"2020-01-15T07:07:17.231Z","comments":true,"path":"2019/12/29/redis设计与实现-单机数据库的实现/","link":"","permalink":"/2019/12/29/redis设计与实现-单机数据库的实现/","excerpt":"","text":"1. 数据库1.1 服务器中的数据库​ Redis服务器将所有数据库都保存在服务器状态redis.h/redisServer结构的db数组中，db数组的每个项都是一个redis.h/redisDb结构，每个redisDb结构代表一个数据库： 123456struct redisServer&#123; //... //一个数组，保存着服务器中的所有数据库 redisDb *db; //...&#125;; ​ 在初始化服务器时，程序会根据服务器状态的dbnum属性来决定应该创建多少个数据库: 123456struct redisServer&#123; //... //服务器的数据库数量 int dbnum; //...&#125;; ​ dbnum属性的值由服务器配置的database选项决定，默认情况下，该选项的值为16，所以Redis服务器默认会创建16个数据库。 1.2 切换数据库​ 每个Redis客户端都有自己的目标数据库，每当客户端执行数据库写命令或者数据库读命令的时候，目标数据库就会成为这些命令的操作对象。 ​ 默认情况下，Redis客户端的目标数据库为0号数据库，但客户端可以通过SELECT命令来切换目标数据库。 1.3 数据库键空间​ Redis是一个键值对数据库服务器，服务器中的每个数据库都由一个redis.h/redisDb结构表示，其中，redisDb结构的dict字典保存了数据库中的所有键值对，我们将这个字典称为键空间： 123456typedef struct redisDb&#123; //... //数据库键空间，保存着数据中的所有键值对 dict *dict; //...&#125;redisDb; ​ 键空间和用户所见的数据库是直接对应的： 键空间的键也就是数据库的键，每个键都是一个字符串对象。 键空间的值也就是数据库的值，每个值可以是字符串对象、列表对象、哈希表对象、集合对象和有序集合对象中的任意一种Redis对象。 1.4 读写键空间时的维护操作​ 当使用Redis命令对数据库进行读写时，服务器不仅会对键空间执行指定的读写操作，还会执行一些额外的维护操作，其中包括： 在读取一个键之后（读操作和写操作都要对键进行读取），服务器会根据键是否存在来更新服务器的键空间命中（hit）次数或键空间不命中（miss）次数，这两个值可以在INFO stats命令的keyspace_hits属性和keyspace_misses数量中查看。 在读取一个键之后，服务器会更新键的LRU（最后一次使用）时间，这个值可以用于计算键的闲置时间，是呀OBJECT idletime命令可以查看键key的闲置时间。 如果服务器在读取一个键时发现该键已经过期，那么服务器会先删除这个过期键，然后才执行与下的其他操作。 如果有客户端使用WATCH命令监视了某个键，那么服务器在对被监视的键进行修改之后，会将这个键标记为脏（dirty），从而让事务程序注意到这个键已经被修改过。 服务器每次修改一个键之后，都会对脏键计数器的值增1，这个计数器会触发服务器的持久化以及复制操作。 如果服务器开启了数据库通知功能，那么在对键进行修改之后，服务器将按配置发送相应的数据库通知。 1.5 设置键的生存时间或过期时间​ 通过EXPIRE命令或者PEXPIRE命令，客户端可以以秒或者毫秒精度为数据库中的某个值设置生存时间（Time To Live，TTL），在经过指定的秒数或者毫秒数之后，服务器就会自动删除生存时间为0的键。 ​ 与EXPIRE命令和PEXPIRE命令类似，客户端可以通过EXPIREAT命令或PEXPIREAT命令，以秒或者毫秒精度给数据中的某个键设置过期时间。 ​ 过期时间是一个UNIX时间戳，当键的过期时间来临时，服务器就会自动从数据库中删除这个键。 1.5.1 设置过期时间​ Redis有四个不同的命令可以用来设置键的生存时间（键可以存在多久）或过期时间（键什么时候会被删除）： EXPIRE命令用于将键key的生存时间设置为ttl秒。 PEXPIRE命令用于将键key的生存时间设置为ttl毫秒。 EXPIREAT命令用于将键key的过期时间设置为timestamp所指定的秒数时间戳。 PEXPIREAT命令用于将键key的过期时间设置为tempstamp所制定的毫秒数时间戳。 ​ EXPIRE、PEXPIRE、EXPIREAT三个命令都是使用PEXPIREAT命令来实现的：无论客户端执行的是以上四个命令中的哪一个，经过转换之后，最终的执行效果都和执行PEXPIREAT命令一样。 1.5.2 保存过期时间​ redisDb结构的expires字典保存了数据库中所有键的过期时间，这个字典被称为过期字典： 过期字典的键是一个指针，这个指针指向键空间中的某个键对象（也即是某个数据库键）。 过期字典的值是一个long long类型的整数，这个整数保存了键所指向的数据库键的过期时间——一个毫秒精度的UNIX时间戳。 123456typedef struct redisDb&#123; //... //过期字典，保存着键的过期时间 dict *expires; //...&#125;redisDb; 1.5.3 移除过期时间​ PERSIST命令可以移除一个键的过期时间。PERSIST命令就是PEXPIREAT命令的反操作：PERSIST命令可以在过期字典中查找给定的键，并解除键和值（过期时间）在过期字典中的关联。 1.5.4 计算并返回剩余生存时间​ TTL命令以秒为单位返回键的剩余生存时间，而PTTL命令则以毫秒为单位返回键的剩余生存时间。 1.5.5 过期键的判断​ 通过过期字典，程序可以用以下步骤检查一个给定键是否过期： 检查给定键是否存在于过期字典：如果存在，那么取得键的过期时间。 检查当前UNIX时间戳是否大于键的过期时间：如果是的话，那么键已经过期；否则的话，键未过期。 1.6 Redis的过期键删除策略​ Redis服务器使用的是惰性删除和定期删除两种策略。 1.6.1 惰性删除策略的实现​ 过期键的惰性删除策略由db.c/expirelfNeeded函数实现，所有读写数据库的Redis命令在执行之前都会调用expirelfNeeded函数对输入键进行检查： 如果输入键已经过期，那么expirelfNeeded函数将输入键从数据中删除。 如果输入键未过期，那么expirelfNeeded函数不做动作。 1.6.2 定期删除策略的实现​ 过期键的定期删除策略由redis.c/activeExpireCycle函数实现，每当Redis的服务器周期性操作redis.c/serverCron函数执行时，activeExpireCycle函数就会被调用，它在规定的时间内，分多次遍历服务器中的各个数据库，从数据库的expires字典中随机检查一部分键的过期时间，并删除其中的过期键。 1.7 AOF、RDB和复制功能对过期键的处理1.7.1 生成RDB文件​ 在执行SAVE命令或者BGSAVE命令创建一个新的RDB文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的RDB文件中。 1.7.2 载入RDB文件​ 在启动Redis服务器时，如果服务器开启了RDB功能，那么服务器将对RDB文件进行载入： 如果服务器以主服务器模式运行，那么在载入RDB文件时，程序会对文件中保存的键进行检查，未过期的键会被载入到数据库中，而过期键则会被忽略，所以过期键对载入RDB文件的主服务器不会造成影响。 如果服务器以服务器模式运行，那么在载入RDB文件时，文件中保存的所有键，不论是否过期，都会被载入到数据中。不过，因为主从服务器在进行数据同步的时候，从服务器的数据库就会被清空，所以一般来讲，过期键对载入RD B文件的从服务器也不会造成影响。 1.7.3 AOF文件写入​ 当服务器以AOF持久化模式运行时，如果数据库中的某个键已经过期，但它还没有被惰性删除或者定期删除，那么AOF文件不会因为这个过期键而产生任何影响。 ​ 当过期键被惰性删除或者定期删除之后，程序会向AOF文件追加（append）一条DEL命令，来显示地记录该键已被删除。 1.7.4 AOF重写​ 和生成RDB文件时类似，在执行AOF重写的过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中。使用BGREWRITEAOF命令。 1.7.5 复制​ 当服务器运行在复制模式下时，从服务器的过期键删除动作由主服务器控制： 主服务器在删除一个过期键之后，会显示地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键。 从服务器在执行客户端发送的读命令时，即是碰到过期键也不会将过期键删除，而是继续像处理未过期的键一样来处理过期键。 从服务器只有在接到主服务器发来的DEL命令之后，才会删除过期键。 1.8 数据库通知​ 数据库通知是Redis 2.8版本新增加的功能，这个功能可以让客户端通过订阅给定的频道或者模式，来获知数据库中键的变化，以及数据库中命令的执行情况。 2. RDB持久化​ RDB持久化功能所生成的RDB文件是一个经过压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态。 ​ 因为RDB文件是保存在硬盘里面的，所以即使Redis服务器进程退出，甚至运行Redis服务器的计算机停机，但只要RDB文件仍然存在，Redis服务器就可以用它来还原数据库状态。 2.1 RDB文件的创建与载入​ 有两个Redis命令可以用于生成RDB文件，一个是SAVE，另一个是BGSAVE。 ​ SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求。 ​ 和SAVE命令直接阻塞服务器进程的做法不同，BSAVE命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程继续处理命令请求。 ​ 和使用SAVE命令或者BGSAVE命令创建RDB文件不同，RDB文件的载入工作是在服务器启动时自动执行的，所以Redis并没有专门用于载入RDB文件的命令，只要Redis服务器在启动时检测到RDB文件存在，它就会自动载入RDB文件。 ​ 因为AOF文件的更新频率通常比RDB文件的更新频率高，所以： 如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态。 只有在AOF持久化功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态。 2.1.1 SAVE命令执行时的服务器状态​ 当SAVE命令执行时，Redis服务器会被阻塞，所以当SAVE命令正在执行时，客户端发送的所有命令请求都会被拒绝。 ​ 只有在服务器执行完SAVE命令、重新开始接受命令请求之后，客户端发送的命令才会被处理。 2.1.2 BGSAVE命令执行时的服务器状态​ 因为BGSAVE命令的保持工作是由子进程执行的，所以在子进程创建RDB文件的过程中，Redis服务器仍然可以继续处理客户端的命令请求，但是，在BGSAVE命令执行期间，服务器处理SAVE、BGSAVE、BGREWRITEAOF三个命令的方式和平时有所不同。 ​ 首先，在BGSAVE命令执行期间，客户端发送的SAVE命令会被拒绝，服务器禁止SAVE命令和BGSAVE命令同时执行是为了避免父进程和子进程同时执行两个rdbSave调用，防止产生竞争条件。 ​ 其次，在BGSAVE命令执行期间，客户端发送的BGSAVE命令会被服务器拒绝，u因为同时执行两个BGSAVE命令也会产生竞争条件。 ​ 最后，BGREWRITEAOF和BGSAVE两个命令不能同时执行： 如果BGSAVE命令正在执行，那么客户端发送的BGREWRITEAOF命令会被延迟到BGSAVE命令执行完毕之后执行。 如果BGREWRITEAOF命令正在执行，那么客户端发送的BGSAVE命令会被服务器拒绝。 2.1.3 RDB文件载入时的服务器状态​ 服务器在载入RDB文件期间，会一直处于阻塞状态，直到载入工作完成为止。 2.2 自动间隔性保存​ 因为BGSAVE命令可以在不阻塞服务器进程的情况下执行，所以Redis允许用户通过设置服务器配置的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令。 ​ 用户可以通过save选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行BGSAVE命令。 2.2.1 设置保存条件​ 当Redis服务器启动时，用户可以通过指定配置文件或传入启动参数的方式设置save选项，如果用户没有主动设置save选项，那么服务器会为save选项设置默认条件： 123save 900 1save 300 10save 60 10000 ​ 接着，服务器程序会根据save选项设置的保存条件，设置服务器状态redisServer结构的saveparams属性： 123456struct redisServer&#123; //... //记录了保存条件的数组 struct saveparam *saveparams; //...&#125;; ​ saveparams属性是一个数组，数组中的每一个元素都是一个saveparam结构，每个saveparam结构都保存了一个save选项设置的保存条件： 123456struct saveparam&#123; //秒数 time_t seconds; //修改数 int changes;&#125;; 2.2.2 dirty计数器和lastsave属性​ 除了saveparams数组之外，服务器状态还维持着一个ditry计数器，以及一个lastsave属性： dirty计数器记录距离上一次成功执行SAVE命令或者BGSAVE命令之后，服务器对数据库状态（服务器中的所有数据库）进行了多少次修改（包括写入、删除、更新等操作）。 lastsave属性是一个UNIX时间戳，记录了服务器上一次成功执行SAVE命令或者BGSAVE命令的时间。 12345678struct redisServer&#123; //... //修改计数器 long long ditry; //上一次执行保存的时间 time_t lastsave; //...&#125;; 2.2.3 检查保存条件是否满足​ Redis的服务器周期性操作函数serverCron默认每隔100毫秒就会执行一次，该函数用于对正在运行的服务器进行维护，它的其中一项工作就是检查save选项所设置的保存条件是否已经满足，如果满足的话，就执行BGSAVE命令。 ​ 程序会遍历并检查saveparams数组中的所有保存条件，只要有任意一个条件被满足，那么服务器就会执行BGSAVE命令。 2.3 RDB文件结构1REDIS|db_version|databases|EOF|check_sum ​ RDB文件的最开头时REDIS部分，这个部分的长度为5字节，保存“REDIS”五个字符。通过这五个字符，程序可以在载入文件时，快速检查所载入的文件是否是RDB文件。 ​ db_version长度为4字节，它的值是一个字符串表示的整数，这个整数记录了RDB的版本号。 ​ databases部分包含着零个或任意多个数据库，以及各个数据库中的键值对数据： 如果服务器的数据库状态为空（所有数据库都是空的），那么这个部分也为空，长度为0字节。 如果服务器的数据库状态为非空（有至少一个数据库非空），那么这个部分也为非空，根据数据库所保持键值对的数量、类型和内容不同，这个部分的长度也会有所不同。 ​ EOF常量的长度为1字节，这个常量标志着RDB文件正文内容的结束，当读入程序遇到这个值的时候，它知道所有数据库的所有键值对都已经载入完毕了。 ​ check_sum是一个8字节长的无符号整数，保存着一个校验和，这个校验和是程序通过对REDIS、db_version、databases、EOF四个部分的内容进行计算得出的。服务器在载入RDB时，会将载入数据所计算出的校验和与check_sum所记录的校验和进行对比，以此来检查RDB文件是否有出错或者损坏的情况出现。 2.3.1 databases部分​ 一个RDB文件的databases部分可以保存任意多个非空数据库。 ​ 例如，如果服务器的0号数据库和3号数据库非空，那么服务器将创建如下图所示的RDB文件，图中的database 0代表0号数据库中的所有键值对数据，而database 3代表3号数据库中的所有键值对数据。 1REDIS|db_version|database 0|database 3|EOF|check_sum ​ 每个非空数据库在RDB文件中都可以保存为SELECTDB、db_number、key_value_pairs三个部分。 1SELECTDB|db_number|key_value_pairs ​ SELECTDB常量的长度为1字节，当读入程序遇到这个值的时候，它知道接下来要读入的将是一个数据库号码。 ​ db_number保存着一个数据库号码，根据号码的大小不同，这个部分的长度可以是1字节、2字节或者5字节。当程序读儒db_number部分之后，服务器会调用SELECT命令，根据读入的数据库号码进行数据库切换，使得之后读入的键值对可以载入到正确的数据库中。 ​ key_value_pairs部分保存了数据库中的所有键值对数据，如果键值对带有过期时间，那么过期时间也会和键值对保存在一起。 2.3.2 key_value_pairs部分​ RDB文件中的每个key_value_pairs部分都保存了一个或以上数量的键值对，如果键值对带有过期时间的话，那么键值对的过期时间也会被保存在内。 ​ 不带过期时间的键值对在RDB文件中由TYPE、key、value三部分组成。 1TYPE|key|value ​ 带有过期时间的键值对在RDB文件中的结构： 1EXPIRETIME_MS|ms|TYPE|key|value 3. AOF持久化​ 与RDB持久化通过保存数据库中的键值对来记录数据库状态不同，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态。 3.1 AOF持久化的实现​ AOF持久化功能的实现可以分为命令追加、文件写入、文件同步三个步骤。 3.1.1 命令追加​ 当AOF持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾。 3.1.2 AOF文件的写入与同步​ 因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到aof_buf缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用flushAppendOnlyFile函数，考虑是否需要将aof_buf缓冲区中的内容写入和保存到AOF文件里面。 appendfsync选项的值 flushAppendOnlyFile函数的行为 always 将aof_buf缓冲区中的所有内容写入并同步到AOF文件 everysec 将aof_buf缓冲区中的所有内容写入并同步到AOF文件，如果上次同步AOF文件的时间距离现在超过一秒钟，那么在此对AOF文件进行同步，并且这个同步操作是由一个线程专门负责执行 no 将aof_buf缓冲区中的所有内容写入到AOF文件，但并不对AOF文件进行同步，何时同步由操作系统来决定 ​ 系统提供了fsync和datasync两个同步函数，它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里吗，从而确保写入数据的安全性。 3.2 AOF文件的载入与数据还原","categories":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}],"tags":[]},{"title":"redis设计与实现-数据结构与对象","slug":"redis设计与实现-数据结构与对象","date":"2019-12-01T05:15:24.000Z","updated":"2019-12-29T08:41:36.836Z","comments":true,"path":"2019/12/01/redis设计与实现-数据结构与对象/","link":"","permalink":"/2019/12/01/redis设计与实现-数据结构与对象/","excerpt":"","text":"1. 简单动态字符串​ Redis没有直接使用C语言传统的字符串表示，而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型，并将SDS用作Redis的默认字符串表示。 ​ 除了用来保存数据库中的字符串值之外，SDS还被用作缓冲区：AOF模块中的AOF缓冲区，以及客户端状态中的输入缓冲区，都是由SDS实现的。 1.1 SDS的定义123456789struct sdshdr&#123; //记录buf数组中已使用字节的数量 //等于SDS所保存字符串的长度 int len; //记录buf数组中未使用字节的数量 int free; //字节数组，用于保存字符串 char buf[];&#125;; 1.2 SDS与C字符串的区别通过未使用空间(free)，SDS实现了空间预分配和惰性空间释放两种优化策略。 空间预分配 空间预分配用于优化SDS的字符串增长操作：当SDS的API对一个SDS进行修改，并且需要对SDS进行空间扩展的时候，程序不仅会为SDS分配修改所必须要的空间，还会为SDS分配额外的未使用空间。 其中，额外分配的未使用空间数量由以下公式决定： 如果对SDS进行修改之后，SDS的长度（也就是len属性的值）将小于1MB，那么程序分配和len属性同样大小的未使用空间，这是SDS len属性的值和free形同。 如果对SDS进行修改之后，SDS的长度将大于等于1MB，那么程序会分配1MB的未使用空间。 惰性空间释放 惰性空间释放用于优化SDS的字符串缩短操作：当SDS的API需要缩短SDS保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用。 C字符串 SDS 获取字符串长度的复杂度为O(N) 获取字符串长度的复杂度为O(1) API是不安全的，可能会造成缓冲区溢出 API是安全的，不会造成缓冲区溢出 修改字符串长度N次必然需要执行N次内存重分配 修改字符串长度N次最多需要执行N次内存重分配 只能保存文本数据 可以保存文本或者二进制数据 可以使用所有&lt;string.h&gt;库中的函数 可以使用一部分&lt;string.h&gt;库中的函数 1.3 SDS API 函数 作用 时间复杂度 sdsnew 创建一个包含给定C字符串的SDS O(N),N为给定C字符串的长度 sdsempty 创建一个不包含任何内容的空SDS O(1) sdsfree 释放给定的SDS O(N),N为被释放SDS的长度 sdslen 返回SDS的已使用空间节数 O(1),使用SDS的len属性获得 sdsvail 返回SDS的未使用空间节数 O(1),使用SDS的free属性获得 sdsdup 创建一个给定SDS的副本(copy) O(N),N为给定SDS的长度 sdsclear 清空SDS保存的字符串内容 因为惰性空间释放策略,O(1) sdscat 将给定C字符串拼接到SDS字符串的末尾 O(N),N为被拼接C字符串的长度 sdscatsds 将给定SDS字符串拼接到另一个SDS字符串的末尾 O(N),N为被拼接SDS字符串的长度 sdscpy 将给定的C字符串复制到SDS里面，覆盖SDS原有的字符串 O(N),N为被复制C字符串的长度 sdsgrowzero 用空字符将SDS扩展至给定长度 O(N),N为扩展新增的字节数 sdsrange 保留SDS给定区间内的数据，不在区间内的数据会被覆盖或清除 O(N),N为被保留数据的字节数 sdstrim 接受一个SDS和一个C字符串作为参数，从SDS中移除所有在C字符串中出现过的字符串 O(N^2),N为给定C字符串的长度 sdscmp 对比两个SDS字符串是否相同 O(N),N为两个SDS中较短的那个SDS的长度 2. 链表​ 每个链表节点使用一个listNode结构来表示： 12345678typedes struct listNode&#123; //前置节点 struct listNode *prev; //后置节点 struct listNode *next; //节点的值 void*value;&#125;listNode; ​ 多个listNode可以通过prev和next指针组成双端链表。 ​ 使用list来持有链表，操作会更方便： 1234567891011121314typedef struct list&#123; //表头节点 listNode *head; //表尾节点 listNode *tail; //链表所包含的节点数量 unsigned long len; //节点值复制函数 void *(*dup)(void *ptr); //节点值释放函数 void (*free)(void *ptr); //节点值对比函数 int (*match)(void *ptr,void *key);&#125;list; ​ Redis的链表实现的特性如下： 双端：链表节点带有prev和next指针，获取某个节点的前置节点和后置节点的复杂度都是O(1)。 无环：表头节点的prev指针和表尾节点的next指针都指向NULL，对链表的访问以NULL为终点。 带表头指针和表尾指针：通过list结构的head指针和tail指针，程序获取链表的表头节点和表尾节点的复杂度为O(1)。 带链表长度计数器：程序使用list结构和len属性来对list持有的链表节点进行计数，程序获取链表中节点数量的复杂度为O(1)。 多态：链表节点使用void*指针来保存节点值，并且可以通过list结构的dup、f ree、match三个属性为节点值设置类型特点函数，所以链表可以用于保存各种不同类型的值。 3. 字典​ 字典，又称为符号表(Symbol table)、关联数组(associative array)或映射(map)，是一种用于保存键值对的抽象数据结构。 3.1 字典的实现​ Redis的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。 哈希表 1234567891011typedef struct dictht&#123; //哈希表数组 dictEntry **table; //哈希表大小 unsigned long size; //哈希表大小掩码，用于计算索引值 //总是等于size-1 unsigned long sizemask; //该哈希表已有节点的数量 unsigned long used;&#125;dictht; table属性是一个数组，数组中的每个元素都是一个指向dictEntry结构的指针，每个dictEntry结构都保存着一个键值对。size属性记录了哈希表的大小，也即是table数组的大小，而used属性则纪录了哈希表目前已有节点（键值对）的数量。sizemask属性的值总是等于size-1，这个属性和哈希值一起决定一个键应该被放到table数组的哪个索引上面。 哈希表节点 哈希表节点使用dictEntry结构表示，每个dictEntry结构都保存着一个键值对： 123456789101112typedef struct dictEntry&#123; //键 void *key; //值 union&#123; void *val; uint64_tu64; int64_ts64; &#125;v; //指向下个哈希表节点，形成链表 struct dictEntry *next；&#125;dictEntry; key属性保存着键值对中的键，而v属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个uint64_t整数，又或者是一个int64_t整数。 next属性是指向另一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对连接在一起，以此来解决键冲突的问题。 字典 1234567891011typedef struct dict&#123; //类型特定函数 dictType *type; //私有数据 void *privdata; //哈希表 dictht ht[2]; //rehash索引 //当rehash不在进行时，值为-1 int trehashidx;/* rehashing not in progress if rehashidx == -1*/&#125;dict; ​ type属性时一个指向dictTYpe结构的指针，每个dictType结构保存了一簇用于操作特定类型键值对的函数，Redis会为用途不同的字典设置不同的类型特定参数。 ​ 而privdata属性则保存了需要传给那些类型特定函数的可选参数。 ​ ht属性是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表，一般情况下，字典只使用ht[0]哈希表，ht[1]哈希表只会在对ht[0]哈希表进行rehash时使用。 3.2 哈希算法​ 当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，就包含新键值对的哈希表节点放到哈希表数组的指定索引上面。 ​ Redis计算哈希值和索引值的方法如下： 1234hash = dict-&gt;type-&gt;hashFunction(key);#使用哈希表的sizemask属性和哈希值，计算出索引值#根据情况不同，ht[x]可以是ht[0]或者ht[1]index = hash &amp; dict-&gt;ht[x].sizemask; 3.3 解决键冲突​ 当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时，称这些键发生了冲突（collision）。 ​ Redis的哈希表使用链地址法(separate chaining)来解决键冲突，每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。 ​ 因为dictEntry节点组成的链表没有指向链尾表尾的指针，所以为了速度考虑，程序总是将新节点添加到链表的表头位置（复杂度为O(1)），排在其他已有节点的前面。 3.4 rehash​ 随着操作的不断执行，哈希表保存的键值对会逐渐地增多或者减少，为了让哈希表的负载因子（load factor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或者收缩。 ​ 扩展和收缩哈希表的工作可以通过rehash操作来完成，Redis对字典的哈希表执行rehash的步骤如下： 1)为字典的ht[1]哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及ht[0]当前包含的键值对数量（也即是ht[0].used属性的值）： 如果执行的是扩展操作，那么ht[1]的大小为第一个大于等于ht[0].used*2的2^n； 如果执行的是收缩操作，那么ht[1]的大小为第一个大于等于ht[0].used的2^n。 2)将保存在ht[0]中的所有键值对rehash到ht[1]上面：rehash指的是重新计算键的哈希值和索引值，然后将键值对放置到ht[1]哈希表的指定位置上。 3)当ht[0]包含的所有键值对都迁移到了ht[1]之后（ht[0]变为空表），释放ht[0]，将ht[1]设置为ht[0]，并在ht[1]新创建一个空白哈希表，为下一次rehash做准备。 哈希表的扩展与收缩 ​ 当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的负载因子大于等于1。 服务器目前正在执行BGSAVE命令或者BSREWRITEAOF命令，并且哈希表的负载因子大于等于5。 其中哈希表的负载因子可以通过公式： 12#负载因子=哈希表已保存节点数量/哈希表大小load_factor = ht[0].used / ht[0].size； ​ 当哈希表的负载因子小于0.1时，程序自动开始对哈希表进行收缩操作。 4. 跳表​ 跳表(skiplist)是一种有序数据结构，它通过在每个节点维持多个指向其他节点的指针，从而达到快速访问节点的目的。 ​ Redis使用跳表作为有序集合键的底层实现之一，如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员是比较长的字符串时，Redis就会使用跳跃表来作为有序集合键的底层实现。 ​ Redis只在两个地方用到了跳表，一个是实现有序集合键，另一个是在集群节点中作用内部数据结构。 4.1 跳表的实现​ Redis的跳表由zskiplistNode和zskiplist两个结构定义，其中zskiplistNode结构用于表示跳表节点，而zskiplist结构则用于保存跳表节点的相关信息。 跳表节点： 123456789101112131415typedef struct zskiplistNode&#123; //层 struct zskiplistLevel&#123; //前进指针 struct zskiplistNode *forward; //跨度 unsigned int span; &#125;level[]; //后退指针 struct zskiplistNode *backward; //分值 double score; //成员对象 robj *obj;&#125;zskiplistNode; 层 ​ 跳表节点的level数组可以包含的多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。 ​ 每次创建一个新跳表节点的时候，程序都根据幂次定律（power law，越大的数出现的概率越小）随机生成一个介于1和32之间的值作为level数组的大小，这个大小就是层的“高度”。 前进指针 ​ 每个层都有一个指向表尾方向的前进指针(level[i].forward属性)，用于从表头向表尾方向访问节点。 跨度 ​ 层的跨度(level[i].span属性)用于记录两个节点之间的距离： 两个节点之间的跨度越大，他们相距得就越远。 指向NULL的所有前进指针的跨度都为0，因为它们没有连向任何节点。 后退指针 ​ 节点的后退指针(backward属性)用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。 分值和成员 ​ 节点的分值(score属性)是一个double类型的浮点数，跳表中的所有节点都按分值从小到大来排序。 ​ 节点的成员对象(obj属性)是一个指针，它指向一个字符串对象，而字符串对象则保存着一个SDS值。 4.2 跳表​ 仅靠多个跳表节点就可以组成一个跳表。 ​ 但通过使用一个zskiplist结构来持有这些节点，程序可以更方便地对整个跳表进行处理，比如快速访问跳表的表头节点和表尾节点，或者快速地获取跳表节点的数量（也即是跳表的长度）。 12345678typedef struct zskiplist&#123; //表头节点和表尾节点 structz skiplistNode *header, *tail; //表中节点的数量 unsigned long length; //表中层数最大的节点的层数 int level;&#125;zskiplist; ​ header和tail指针分别指向跳跃表的表头和表尾节点，通过这两个指针，程序定位表头节点和表尾节点的时间复杂度为O(1)。 ​ 通过使用length属性来记录节点的数量，程序可以在O(1)复杂度内返回跳跃表的长度。 ​ level属性泽用于在O(1)复杂度内获取跳跃表中层高最大的那个节点的层数量，表头节点的层高并不计算在内。 5. 整数集合​ 整数集合(intset)是集合健的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis就会使用整数集合作为集合键的底层实现。 5.1 整数集合的实现​ 整数集合(intset)是Redis用于保存整数值的集合抽象数据结构，它可以保存类型为int16_t、int32_t或者int64_t的整数值，并且保证集合中不会出现重复元素。 12345678typedef struct intset&#123; //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[];&#125;intset; ​ contents数组是整数集合的底层实现：整数集合的每个元素都是contents数组的一个数组项(item)，各个项在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复元素。 ​ length属性记录了整数集合包含的元素数量，也即是contents数组的长度。 ​ 虽然intset结构将contents属性声明为int8_t类型的数组，但实际上contents数组并不保存任何int8_t类型的值，contents数组的真正类型取决于encoding属性的值。 5.2 升级​ 每当要将新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级（upgrdae），然后才能将新元素添加到整数集合里面。 ​ 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性质不变。 将新元素添加到底层数组里面。 ​ 因为每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中已有的所有元素进行类型转换，所以向整数集合添加新元素的时间复杂度为O(N)。 5.3 升级的好处​ 整数集合的升级策略有两个好处，一个是提升整数集合的灵活性，另一个是尽可能地节约内存。 5.4 降级​ 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。 6. 压缩列表​ 压缩列表(ziplist)是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么Redis就会使用压缩列表来做列表键的底层实现。 ​ 当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么Redis就会使用压缩列表来做哈希键的底层实现。 6.1 压缩列表的构成​ 压缩列表是Redis为了节约内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型(sequential)数据结构。一个压缩列表可以包含任意多个节点(entry)，每个节点可以保存一个字节数组或者一个整数值。 ​ 压缩列表的各个组成部分 zlbytes｜zltail｜zllen｜ entry1 ｜entry2 ｜ … ｜ entryN ｜zlend ​ 各个部分的详细说明 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数 zltail uint32_t 4字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节 zllen uint16_t 2字节 记录了压缩列表包含的节点数量 entryX 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定 zlend uint8_t 1字节 特殊值0xFF(十进制255)，用于标记压缩列表的末端 6.2 压缩列表节点的构成​ 每个压缩列表节点可以保存一个字节数组或者一个整数值，其中，字节数组可以是以下三种长度的其中一种： 长度小于等于63(26-1)字节的字节数组； 长度小于等于16383(214-1)字节的字节数组； 长度小于等于4294967295(232-1)字节的字节数组； 而整数值则可以是以下六种长度的其中一种： 4位长，介于0至12之间的无符号整数； 1字节长的有符号整数； 3字节长的有符号整数； int16_t类型整数; int32_t类型整数; int64_t类型整数。 每个压缩列表节点都由previous_entry_length、encoding、content三个部分组成。 previous_entry_length 节点的previous_entry_length属性以字节为单位，记录了压缩列表中前一个节点的长度。previous_entry_length属性的长度可以是1字节或者5字节。 encoding 节点的encoding属性记录了节点的content属性所保存数据的类型以及长度 content 节点的content属性负责保存节点的值，节点值可以是一个字节数组或者整数，值的类型和长度由节点的encoding属性决定。 7. 对象​ Redis基础上述数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象。 7.1 对象的类型和编码​ Redis使用对象来表示数据库中的键和值，每当在Redis的数据库中新创建一个键值对时，至少会创建两个对象，一个对象作用于键值对的键（键对象），另一个对象用作键值对的值（值对象）。 ​ Redis中的每个对象都由一个redisObject结构表示，该结构中和保存数据有关的三个属性分别是type属性、encoding属性和ptr属性： 12345678typedef struct redisObject&#123; //类型 unsigned type:4; //编码 unsigned encoding:4; //指向底层实现数据结构的指针 void *prt;&#125;robj; 类型 对象的type属性记录了对象的类型。（字符串对象、列表对象、哈希对象、集合对象、有序集合对象） 编码和底层实现 对象的ptr指针指向对象的底层实现数据结构，而这些数据结构由对象的encoding属性决定。 7.2 字符串对象​ 字符串对象的编码可以是int、raw或者embstr。 ​ 如果字符串对象保存的是一个字符串值，并且这个字符串值的长度大于32字节，那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值，并将对象的编码设置为raw；如果这个字符串值的长度小于等于32字节，那么字符串对象将使用embstr编码的方式来保存这个字符串值。 ​ embstr编码是专门用于保存短字符串的一种优化编码方式，这种编码和raw编码一样，都会使用redisObject结构和sdshdr结构来表示字符串对象，但raw编码会调用两次内存分配函数来分别创建redisObject结构和sdshdr结构，而embstr编码则通过调用一次内存分配函数来分配一块连续的空间，空间中依次包含redisObject和sdshdr结构。 7.3 列表对象​ 列表对象的编码可以是ziplist或者linkedlist。 ​ ziplist编码的列表对象使用压缩列表作为底层实现，每个压缩列表节点（entry）保存了一个列表元素。 ​ linkedlist编码的列表对象使用双端列表作为底层实现，每个双端链表节点（node）都保存了一个字符串对象，而每个字符串对象都保存了一个列表元素。 ​ linkedlist编码的列表对象在底层的双端链表结构中包含了多个字符串对象，这种嵌套字符串对象的行为在哈希对象、集合对象和有序集合对象中都会出现，字符串对象是Redis五种类型的对象中唯一一种会被其他四种类型对象嵌套的对象。 7.3.1 编码转换 ​ 当列表对象可以同时满足以下两个条件时，列表对象使用ziplist编码： 列表对象保存的所有字符串元素的长度都小于64字节； 列表对象保存的元素数量小于512个；不能满足这两个条件的列表对象需要使用linkedlist编码。 7.4 哈希对象​ 哈希对象的编码可以是ziplist或者hashtable。 ​ ziplist编码的哈希对象使用压缩列表作为底层实现，每当有新的键值对要加入到哈希对象时，程序会先将保存了键的压缩列表节点推入到压缩列表表尾，然后再将保存了值的压缩列表节点推入到压缩列表表尾，因此： 保存了同一键值对的两个节点总是紧挨在一起，保存键的节点在前，保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向，而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 ​ hashtable编码的哈希对象使用字典作为底层实现，哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象，对象中保存了键值对的键； 字典的每个值都是一个字符串对象，对象中保存了键值对的值。 7.4.1 编码转换 ​ 当哈希对象可以同时满足以下两个条件时，哈希对象使用ziplist编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于64字节； 哈希对象保存的键值对的数量小于512个；不能满足这两个条件的哈希对象需要使用hashtable编码。 7.5 集合对象​ 集合对象的编码可以是intset或者hashtable。 ​ intset编码的集合对象使用整数集合作为底层实现，集合对象所包含的所有元素都被保存在整数集合里。 ​ hashtable编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，而字典的值则全部被设置为NULL。 7.5.1 编码转换 ​ 当集合对象可以同时满足以下两个条件时，对象使用intset编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过512个。 ​ 不能满足这两个条件的集合对象需要使用hashtable编码。 7.6 有序集合对象​ 有序集合对象的编码可以是ziplist或者skiplist。 ​ ziplist编码的压缩列表对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员（member），而第二个元素则保存元素的分值（score）。 ​ 压缩列表内的集合元素按分值从小到大排序，分值较小的元素被放置在靠近表头的方向，而分值较大的元素则被放置在靠近表尾的方向。 ​ skiplist编码的有序集合对象使用zset结构作为底层实现，一个zset结构同时包含一个字典和一个跳表: 1234typedef struct zset&#123; zskiplist *zsl; dict *dict;&#125;zset; ​ zset结构中的zsl跳表按分值从小到大保存了所有集合元素，每个跳表节点都保存了一个集合元素：跳表节点的object属性保存了元素的成员，而跳表节点score属性则保存了元素的分值。ZRANK，ZRANGE等命令给予跳表API实现的。 ​ 除此之外，zset结构的dict字典为有序集合创建了一个从成员到分值的映射，字典中的每个键值对都保存了一个集合元素：字典的键保存了元素的成员，字典的值则保存了元素的分值。通过字典，程序可以用O(1)复杂度查找给定成员的分值，ZSCORE命令根据这一特性实现的。 ​ 字典和跳表会共享元素的成员和分值，所以不会造成任何数据重复，也不会因此而浪费任何内存。 7.6.1 编码转换 ​ 当有序集合对象可以同时满足以下两个条件时，对象使用ziplist编码： 有序集合保存的元素数量小于128个； 有序集合保存的所有元素成员的长度都小于64字节； ​ 不能满足以上两个条件的有序集合将使用skiplist编码。 7.7 内存回收​ 因为C语言并不具备自动内存回收功能，所以Redis在字节的对象系统中构建了一个引用计数技术实现的内存回收机制。 ​ 每个对象的引用计数信息由redisObject结构的refcount属性记录 123456typedef struct redisObject&#123; //.. //引用计数 int refcount; //..&#125;robj; ​ 对象的引用计数信息会随着对象的使用状态而不断变化： 在创建一个新对象时，引用计数的值会被初始化为1； 当对象被一个新程序使用时，它的引用计数值会被增一； 当对象不再被一个程序使用时，它的引用计数值会被减一； 当对象的引用计数值变为0时，对象所占用的内存会被释放。","categories":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}],"tags":[]},{"title":"类文件结构","slug":"类文件结构","date":"2019-11-05T10:50:12.000Z","updated":"2019-12-23T23:28:28.943Z","comments":true,"path":"2019/11/05/类文件结构/","link":"","permalink":"/2019/11/05/类文件结构/","excerpt":"","text":"1. Class类文件的结构​ Class文件是一组以8位字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑地排列在Class文件之中，中间没有添加任何分隔符。 ​ Class文件格式采用一种类似C语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：无符号数和表。 1.1 魔数与Class文件的版本​ 每个Class文件的头4个字节称为魔数，它唯一的作用是确定这个文件是否为一个能被虚拟机接受的Class文件。Class文件的魔数值为：0xCAFEBABE。 ​ 紧接着魔数的4个字节存储的是Class文件的版本号：第5和第6个字节是次版本号，第7和第8个字节是主版本号。高版本的JDK能向下兼容以前版本的Class文件，但不能运行以后版本的Class文件，即使文件格式未发生变化，虚拟机也必须拒绝执行超过其版本号的Class文件。 1.2 常量池​ 紧接着主次版本号之后的是常量池入口，常量池可以理解为Class文件之中的资源仓库，它是Class文件结构中与其他项目关联最多的数据类型，也是占用Class文件空间最大的数据项目之一，同时它还是在Class文件中第一个出现的表类型数据项目。 ​ 由于常量池中的常量数目是不固定的，所以在常量池的入库需要放置一项u2类型的数据，代表常量池容量计数值，这个容量计数是从1而不是0开始的。Class结构中只有常量池的容量计数是从1开始的。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"基于 Kafka 和 ZooKeeper 的分布式消息队列","slug":"基于-Kafka-和-ZooKeeper-的分布式消息队列","date":"2019-10-24T03:20:35.000Z","updated":"2019-11-06T12:01:12.466Z","comments":true,"path":"2019/10/24/基于-Kafka-和-ZooKeeper-的分布式消息队列/","link":"","permalink":"/2019/10/24/基于-Kafka-和-ZooKeeper-的分布式消息队列/","excerpt":"","text":"1.Kafka总体架构 ​ 一个典型的Kafka体系架构包括若干个Producer（消息生产者），若干broker（作为Kafka节点的服务器），若干Consumer（Group），以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群配置、选举Leader以及在consumer group发生变化时进行Rebalance（消费者负载均衡）。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费信息。 ​ Kafka内部结构如下： Producer：生产者，即消息发送者，push消息到Kafka集群中的broker中； Broker：Kafka集群由多个Kafka实例（Server）组成，每个实例构成一个broker； Topic：producer向Kafka集群push的消息会被归于某一类别，即Topic，这本质上只是一个逻辑概念，面向对象的是producer和consumer，producer只需要关注将消息push到哪一个Topic中，而consumer只需要关心自己订阅了哪个Topic； Partition：每一个Topic又被分为多个Partitions，即物理分区；出于负载均衡的考虑。同一个Topic中的Partitions分别存储与Kafka集群的多个broker上；而为了提高可靠性，这些Patitions可以由Kafka机制中的replicas来设置备份的数量； Consumer：消费者，从Kafka集群的broker中pull消息、消费消息； Consumer group：high-level consumer API中，每个consumer都属于一个consumer-group，每条消息只能被consumer-group中的一个consumer消费，但可以被多个consumer-group消费； replicas：partition的副本，保障partition的高可用； leader：replicas中的一个角色，producer和consumer只跟leader交互； follower：replicas中的一个角色，从leader中复制数据，作为副本，一个leader挂掉，会从它的followers中选举出一个新的leader继续提供服务； controller：Kafka集群中的其中一个服务器，用来进行leader election以及各种failover； ZooKeeper：Kafka通过ZooKeeper来存储集群的meta信息等。 1.1 Topic &amp; Partition​ 一个topic可以认为是一类消息，每个topic将被分成多个partition，每个partition在存储层面是append log文件。任何发布到此partition的消息都会被追加到log文件的尾部，每条消息在文件中的位置称为offset(偏移量)，offset为一个long型的数字，它唯一标记一条信息。Kafka机制中，producer push来的消息是追加到partition中的，这是一种随机写磁盘的机制，效率远高于随机写内存。 1.2Kafka为什么要将Topic进行分区​ 负载均衡、水平扩展。 ​ 在创建topic时可以在$KAFKA_HOME/config/server.properties 中指定这个partition的数量，也可以在topic创建之后去修改partition的数量。 1234# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=3 ​ 在发送一条消息时，可以指定这个消息的key，producer根据这个key和partition机制来判断这个消息发送到哪个partition。partition机制可以通过指定producer的partition.class这一参数来指定（即支持自定义），该class必须实现Kafka.producer.Partitioner接口。 2.Kafka高可靠性实现基础解读2.1Kafka文件存储机制​ partition还可以细分为segment，一个partition物理上由多个segment组成。在Kafka文件存储中，同一个topic下有多个不同的partition，每个partition为一个目录，partition的名称规则为：topic名称+有序序号，第一个序号从0开始计，最大的序号为partition数量减1，partition是实际物理上的概念，而topic是逻辑上的概念。 ​ 每个partition（目录）相当于一个巨型文件被平均分配到多个大小相等的segment（段）数据文件中（每个segment文件中消息数量不一定相等），这种特性也方便old segment的删除，即方便已被消费的消息的清理，提高磁盘的利用率。每个partition只需要支持顺序读写就行，segment的文件生命周期由服务端配置参数（log.segment.bytes，log.roll.{ms,hours}等若干参数）决定。 ​ segment文件由两部分组成，分别为”.index”文件和”.log”文件，分别表示为segment索引文件和数据文件。这两个文件的命令规则为：partition全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值，数值大小为64位，20位数字字符长度，没有数字用0填充。 2.2复制原理和同步方式​ 为了提高消息的可靠性，Kafka每个topic的partition有N个副本，其中N（大于等于1）是topic的复制因子的个数。Kafka通过多副本机制实现故障自动转移，当Kafka集群中出现broker失效时，副本机制可保证服务可用。对于任何一个partition,它的N个replicas中，其中一个replica为leader，其他都为follower，leader负责处理partition的所有读写请求，follower则负责被动地去复制leader上的数据。 ​ Kafka机制中，leader将负责维护和跟踪一个ISR（In-Sync Replicas）列表，即同步副本队列，这个列表里面的副本与leader保持同步，状态一致。如果新的leader从ISR列表中的副本中选出，那么就可以保证新leader为优选。 2.3同步副本ISR​ 默认情况下Kafka的replica数量为1，即每个partition都只有唯一的leader，无follower，没有容灾能力。为了确保消息的可靠性，生成环境中，通常将其值（由broker的参数offsets.topic.replication.factor指定）大小设置为大于1。所有的副本统称为Assigned Replicas，即AR。ISR是AR中的一个子集，由leader维护ISR列表，follower从leader同步数据有一些延迟（由参数replica.lag.max.ms 设置超时阈值），超过阈值的follower将被剔除出ISR，存入OSR(Outof-Sync Replicas)列表，新加入的follower也会先存放在OSR中。AR = ISR + OSR。 3.全程解析（Producer-kafka-consumer）3.1 producer发布消息​ producer采用push模式将消息发布到broker，每条消息都被append到patition中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要搞，保障kafka吞吐率）。produccer发送消息到broker时，会根据分区算法选择将其存储到哪一个partition。 其路由机制为： 指定了partition，则直接使用； 未指定partition但指定key，通过对key进行hash选出一个partition； partition和key都未指定，使用轮询选出一个partition。 写入流程： producer先从Zookeeper的”/brokers/…/state”节点找到该partition的leader； producer将消息发送给该leader； leader将消息写入本地log； follower从leader pull消息，写入本地log后leader发送ACK； leader收到所有ISR中的replica的ACK后，增加HW(high watermark，最后commit的offset)并向producer发送ACK。 3.2 Broker存储消息​ 物理上把topic分成一个或多个partition，每个partition物理上对应一个文件夹（该文件存储该partition的所有消息和索引文件） 3.3 Consumer消费消息​ high-level cocnsumer API提供了consumer group的语义，一个消息只能被group内的一个consumer所消费，且consumer消费信息时不关注offset，最后一个offset由ZooKeeper保存（下次消费时，该group中的consumer将从offset记录的位置开始消费）。 注意： 如果消费线程大于partition数量，则有些线程将收不到消息； 如果partition数量大于消费线程数，则有些线程多收到多个partition的消息； 如果一个线程消费多个partition，则无法保证收到的消息的顺序，而一个partition内的消息是有序的。 consumer采用pull模式从broker中读取数据。 ​ push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络堵塞。而pull模式则可以根据consumer的消息能力以适当的速率消费消息。 ​ 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"/categories/学习笔记/"}],"tags":[]},{"title":"垃圾收集器与内存分配策略","slug":"垃圾收集器与内存分配策略","date":"2019-10-16T10:41:33.000Z","updated":"2019-10-24T03:19:46.547Z","comments":true,"path":"2019/10/16/垃圾收集器与内存分配策略/","link":"","permalink":"/2019/10/16/垃圾收集器与内存分配策略/","excerpt":"","text":"1.对象存活1.1引用计数算法​ 给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。 ​ 缺点是难解决对象之间相互循环引用的问题。 1.2可达性算法​ 通过一些列的成为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径成为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。 ​ 在Java中，可作为GC Roots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI（即一般说的Native方法）引用的对象。 1.3引用强引用、软引用、弱引用、虚引用，这4种引用强度依次逐渐减弱。 强引用就是指在程序代码之中普遍存在的，类似“Object obj = new Object()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对方，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收后还没有足够的内存，才会抛出内存溢出异常。使用SoftReference类来实现软引用。 弱引用也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。使用WeakReference类来实现弱引用。 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。使用PhantomReference类来实现虚引用。 1.4回收方法区​ 方法区垃圾收集效率低。效率方法区的垃圾收集主要回收两部分：废弃常量和无用的类。 ​ 满足3个条件才能算是无用的类： 该类所有的实例都已被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 1.5判定一个对象的死亡过程​ 一个对象真正死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。 2.垃圾收集算法2.1标记-清除算法​ 最基础的收集算法是”标记-清除“算法，算法分为”标记“和”清除“两个阶段：首先标记出所需要回收的对象，在标记完成后统一回收所有被标记的对象。 ​ 主要不足有两个： 效率问题，标记和清除两个过程的效率都不高； 空间问题，标记清除后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 2.2复制算法​ 复制算法是为了解决效率问题而出现的，它将可用内存容量划分为大小相等的两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 ​ HotSpot虚拟机将内存划分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8 : 1，也就每次新生代中可用内存空间为整个新生代容量的90%。当Survivor空间不够用时，需要依赖其他内存(老年代)进行分配担保。 2.3标记-整理算法​ 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低，所以在老年代一般不能直接选用这种算法。 ​ 标记-整理算法的过程与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 2.4分代收集算法​ 把Java堆分为新生代和老年代，就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集都发现有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行担保，就必须使用标记-整理或者标记-清除算法。 3.内存分配与回收策略3.1对象优先在Eden分配​ 大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够的空间进行分配时，虚拟机将发起一次Minor GC。 3.2大对象直接进入老年代​ 所谓的大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。 3.3长期存活的对象将进入老年代​ 虚拟机给每个对象定义了一个对象年龄计数器。如果对象在Eden区出生并经过第一次Minor GC后然后存活，并且能被Survior容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor区中每“熬过”每一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度(默认为15岁)，就将会被晋升到老年代中。 3.4动态对象年龄判定​ 如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。 3.5空间担保分配​ 在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年大最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HnadlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"Java内存区域","slug":"Java内存区域","date":"2019-10-11T10:21:47.000Z","updated":"2019-10-14T11:31:40.824Z","comments":true,"path":"2019/10/11/Java内存区域/","link":"","permalink":"/2019/10/11/Java内存区域/","excerpt":"","text":"1. 运行时数据区域 1.1 程序计数器（Program Counter Register）​ 程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。 ​ 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，这类内存区域为”线程私有“的内存。 ​ 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是一个Native方法，这个计数器值则为空(Undefined)。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 1.2 Java虚拟机栈(Java Virtual Machine Stacks)​ 是线程私有的，生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 ​ 局部变量表存放了编译期可知的各种基本数据类型(boolean, byte, char, short, int, long, float, double)、对象引用(reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)和returnAddress类型(指向了一条字节码指令的地址)。 ​ 局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 ​ 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展(当前大部分的Java虚拟机都可以动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈)，如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 1.3 本地方法栈(Native Method Stack)​ 本地方法栈与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法(也就是字节码)服务，而本地方法栈则为虚拟机使用到的Native方法服务。有的虚拟机(譬如Sun HotSpot虚拟机)直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 1.4 Java堆(Java Heap)​ Java堆是Java虚拟机所管理的内存中最大的一块。Java堆事被所有线程共享的一块内存区域，在虚拟机启动时创建。此区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 ​ Java堆是垃圾收集器管理的主要区域，所以也称为GC堆(Garbage Collected Heap)。由于现在收集器基本采用分代收集算法，所以Java堆也可细分为：新生代和老年代。 ​ Java堆可以处理物理上不连续的空间中，只要逻辑上是连续的即可。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 1.5方法区(Method Area)​ 方法区与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 ​ Java虚拟机规范对方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载。 ​ 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 1.6运行时常量池(Runtime Constant Pool)​ 运行时常量池是方法区的一部分。Class文件中有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 ​ 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这个特性被利用的比较多的是String类的intern()方法。 ​ 当常量池无法再申请到内存时会抛出OutOfMemoryError异常。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"python爬虫-解析库的使用","slug":"python爬虫-解析库的使用","date":"2019-08-15T02:54:57.000Z","updated":"2019-08-19T08:43:59.233Z","comments":true,"path":"2019/08/15/python爬虫-解析库的使用/","link":"","permalink":"/2019/08/15/python爬虫-解析库的使用/","excerpt":"","text":"1.使用XPath1.XPath常用规则 表达式 描述 nodename 选取此节点的所有子节点 / 从当前节点选择直接子节点 // 从当前节点选取子孙节点 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 2.使用1.转换123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = etree.tostring(html)print(result.decode('utf-8')) 结果： 12345678910&lt;html&gt;&lt;body&gt;&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2.所有节点123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//*')print(result) 结果: 1[&lt;Element html at 0x7fa7683c9148&gt;, &lt;Element body at 0x7fa7683c90c8&gt;, &lt;Element div at 0x7fa7683c9088&gt;, &lt;Element ul at 0x7fa7683c9188&gt;, &lt;Element li at 0x7fa7683c91c8&gt;, &lt;Element a at 0x7fa7683c9248&gt;, &lt;Element li at 0x7fa7683c9288&gt;, &lt;Element a at 0x7fa7683c92c8&gt;, &lt;Element li at 0x7fa7683c9308&gt;, &lt;Element a at 0x7fa7683c9208&gt;, &lt;Element li at 0x7fa7683c9348&gt;, &lt;Element a at 0x7fa7683c9388&gt;, &lt;Element li at 0x7fa7683c93c8&gt;, &lt;Element a at 0x7fa7683c9408&gt;, &lt;Element ul at 0x7fa7683c9448&gt;] 3.子节点123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li/a')print(result) 结果： 1[&lt;Element a at 0x7fa8000e4408&gt;, &lt;Element a at 0x7fa8000e43c8&gt;, &lt;Element a at 0x7fa8000e44c8&gt;, &lt;Element a at 0x7fa8000e4508&gt;, &lt;Element a at 0x7fa8000e4548&gt;] 4.父节点使用..或者parent::来获取父节点 12345678910111213141516from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)# result = html.xpath('//a[@href=\"link4.html\"]/../@class')result = html.xpath('//a[@href=\"link4.html\"]/parent::*/@class')print(result) 结果： 1[&apos;item-1&apos;] 5.属性匹配123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[@class=\"item-0\"]')print(result) 结果： 1[&lt;Element li at 0x7fc52029c4c8&gt;, &lt;Element li at 0x7fc52029c448&gt;] 6.文本获取使用text()来获取节点中的文本 123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[@class=\"item-0\"]/a/text()')print(result) 结果： 1[&apos;first item&apos;, &apos;fifth item&apos;] 7.属性获取123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li/a/@href')print(result) 结果： 1[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link3.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;] 8.属性多值匹配使用contains() 12345678910111213141516from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;li class=\"li li-first\"&gt;&lt;a href=\"link.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class,\"li\")]/a/text()')print(result) 结果： 1[&apos;first item&apos;] 9.多属性匹配12345678910111213141516from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;li class=\"li li-first\" name=\"item\"&gt;&lt;a href=\"link.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class,\"li\") and @name=\"item\"]/a/text()')print(result) 结果： 1[&apos;first item&apos;] 10.按序选择序号是从1开头，不是以0开头 123456789101112131415161718192021from lxml import etreetext = &apos;&apos;&apos;&lt;div&gt;&lt;ul&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;&apos;&apos;&apos;html = etree.HTML(text)result = html.xpath(&apos;//li[1]/a/text()&apos;)print(result)result = html.xpath(&apos;//li[last()]/a/text()&apos;)print(result)result = html.xpath(&apos;//li[position()&lt;3]/a/text()&apos;)print(result)result = html.xpath(&apos;//li[last()-2]/a/text()&apos;)print(result) 结果： 1234[&apos;first item&apos;][&apos;fifth item&apos;][&apos;first item&apos;, &apos;second item&apos;][&apos;third item&apos;] 2.使用Beautiful Soup1.基本用法12345678910111213141516from bs4 import BeautifulSouphtml = '''&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body &gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story’'&gt;Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;! - Elsie ...&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;'''soup = BeautifulSoup(html, 'lxml')print(soup.prettify())print(soup.title.string) 结果： 123456789101112131415161718192021222324252627282930&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"title\" name=\"dromouse\"&gt; &lt;b&gt; The Dormouse's story &lt;/b&gt; &lt;/p&gt; &lt;p class=\"story’'&amp;gt;Once upon a time there were three little sisters; and their names were &amp;lt;a href=\" http:=\"\" id=\"link1\"&gt; , &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt; Lacie &lt;/a&gt; and &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt; Tillie &lt;/a&gt; ;and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt; ... &lt;/p&gt; &lt;/body&gt;&lt;/html&gt;The Dormouse's story 2.节点选择器1.选择元素节点选择只会选择第一个匹配的节点，其他的后面节点都会忽略 12345678910111213141516171819from bs4 import BeautifulSouphtml = '''&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body &gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story’'&gt;Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;! - Elsie ...&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;'''soup = BeautifulSoup(html, 'lxml')print(soup.title)print(type(soup.title))print(soup.title.string)print(soup.head)print(soup.p) 结果为： 12345&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;class &apos;bs4.element.Tag&apos;&gt;The Dormouse&apos;s story&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt; 2.提取信息 提取名称 1print(soup.title.name) 提取属性 12print(soup.p.attrs)print(soup.p.attrs['name']) 也可以不写attrs，直接在节点元素后面加中括号，传入属性名就可以获取属性值了。 1print(soup.p['name']) 获取内容 1print(soup.p.string) 3.嵌套选择123print(soup.head.title)print(type(soup.head.title))print(soup.head.title.string) 4.关联选择 子节点和子孙节点 选择节点元素之后，如果想要获取它的直接子节点，可以调用contents属性。 同样可以调用children属性得到相应的结果。 如果要得到所有的子孙节点的话，可以调用descendants属性。 父节点和祖先节点 如果要获取某个节点元素的父节点，可以调用parent属性。 如果要获取所有的祖先节点，可以调用parents属性。 兄弟节点 next_sibling和previous_sibling分别获取节点的下一个和上一个兄弟元素， next_siblings和previous_siblings则分别返回所有前面和后面的兄弟节点的生成器。 3.方法选择器 find_all() 查询所有符合条件的元素。 find_all(name, attrs, recursive, text, **kwargs) find 查询第一个匹配的元素 4.CSS选择器调用select()方法，传入相应的CSS选择器即可。 获取文本时，使用get_text()方法 3.使用pyquery1.初始化1.字符串初始化1234567891011121314from pyquery import PyQuery as pqhtml = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;'''doc = pq(html)print(doc('li')) 结果是： 12345&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 2.url初始化123456from pyquery import PyQuery as pqimport requestsdoc = pq('https://www.taobao.com')#或者使用以下方式#doc = pq(requests.get('https://www.taobao.com').text)print(doc('title')) 结果为： 1&lt;title&gt;淘宝网 - 淘！我喜欢&lt;/title&gt; 3.文件初始化123from pyquery import PyQuery as pqdoc = pq(fileName='demo.html')print(doc('li')) 2.基本CSS选择器123456789101112131415from pyquery import PyQuery as pqhtml = '''&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;'''doc = pq(html)print(doc('#container .list li'))print(type(doc('#container .list li'))) 结果为： 1234567&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; 3.查找节点1.子节点查找子节点时，需要用到find()方法，此时传入的参数是CSS选择器。 123456789101112131415161718192021from pyquery import PyQuery as pqimport requestshtml = '''&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;'''doc = pq(html)items = doc('.list')print(type(items))print(items)lis = items.find('li')print(type(lis))print(lis) 结果是： 1234567891011121314&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;ul class=&quot;list&quot;&gt;&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; find()的查找范围是节点的所有子孙节点，如果只想查找子节点，可以用children()方法。 2.父节点12345678910111213141516171819from pyquery import PyQuery as pqhtml = '''&lt;div class=\"wrap\"&gt;&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;'''doc = pq(html)items = doc('.list')container = items.parent()print(type(container))print(container) 结果是： 12345678910&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt; parents()方法会返回所有的祖先节点。 3.兄弟节点调用siblings()方法。 4.遍历调用items()方法。 123456from pyquery import PyQuery as pqdoc = pq(html)lis = doc('li').items()print(type(lis))for li in lis: print(li, type(li)) 结果： 1234567891011&lt;class &apos;generator&apos;&gt;&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; 5.获取信息1.获取属性提取到某个PyQuery类型的节点后，就可以调用attr()方法来获取属性。 1234567from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a, type(a))print(a.attr('href'))#也可使用attr属性来获取属性print(a.attr.href) 结果为： 12&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;link3.html 2.获取文本12345from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.text()) 结果： 12&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;third item 6.节点操作addClass、removeClass、attr、text和html等。","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[{"name":"python爬虫","slug":"python爬虫","permalink":"/tags/python爬虫/"}]},{"title":"python爬虫-基本库的使用","slug":"python爬虫-基本库的使用","date":"2019-06-26T12:22:36.000Z","updated":"2019-11-22T07:06:22.055Z","comments":true,"path":"2019/06/26/python爬虫-基本库的使用/","link":"","permalink":"/2019/06/26/python爬虫-基本库的使用/","excerpt":"","text":"1.使用urllib在python3中，统一使用urllib。它是python内置的HTTP请求库，不需要额外安装即可使用。它包含4个模块。 request：它是最基本的HTTP请求模块，可以用来模拟发送请求。 error：异常处理模块，如果出现请求错误，可以捕获这些异常，然后进行充实或其他操作以bao证程序不会意外终止。 parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。 robotparser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得较少。 1.1发送请求1.urlopen()urllib.request模块提供了最基本的构造HTTP请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理授权验证、重定向、浏览器cookies以及其他内容。 123456import urllib.requestimport sslssl._create_default_https_context = ssl._create_unverified_contextresponse = urllib.request.urlopen('https://www.baidu.com')print(response.read()) 在进行https爬取时，需要设置ssl，进行全局取消证书验证。 2.Request如果请求中需要加入Headers等信息，就可以利用更强大的Request类来构建。 1234567import urllib.requestimport sslssl._create_default_https_context = ssl._create_unverified_contextrequest = urllib.request.Request('https://www.baidu.com')response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) Request的构造方法如下： 1urllib.request.Request(url,data=None,headers&#123;&#125;,origin_req_host=None, unverifiable=False,method=None) 第一个参数url用于请求URL，这是必传参数，其他都是可选参数。 第二个参数data如果要传，必须传bytes类型的。如果它时字典，可以先用urllib.parse模块里的urlencode()编码。 第三个参数headers是一个字典，它就是请求头，可以在构造请求是通过headers参数直接构造，也可以通过调用请求实例的add_header()方法添加。 第四个参数origin_req_host指的是请求方的host名称或者IP地址。 第五个参数unverifiable表示这个请求是否是无法验证的，默认是False，意思是说用户没有足够权限来选择接收这个请求的结果。例如请求一个HTML文档中的图片，但是没有自动抓取图像的权限，这时unverifiable的值就是True。 第六个参数method是一个字符串，用来指示请求使用的方法，比如GET、POST和PUT等。 传入多个参数的请求： 12345678910111213from urllib import request, parseheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36', 'Host': 'httpbin.org'&#125;dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf-8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8')) 3.高级用法urllib.request模块里的BaseHandler类，它是所有其他Handler的父类，它提供了最基本的方法，例如default_open()、protocol_request()等。 HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误都会抛出HTTPErro t类型的异常。 HTTPRedirectHandler：用于处理重定向。 HTTPCookieProcessor：用于处理Cookies。 ProxyHandler：用于设置代理，默认代理为空。 HTTPPasswordMgr：用于管理mi码，它维护了用户名和mi码的表。 HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。 1234567# 生成Cookies文件filaName = 'cookies.txt'cookie = http.cookiejar.LWPCookieJar(filaName)handler = request.HTTPCookieProcessor(cookie)opener = request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True,ignore_expires=True) 1234567#读取Cookies文件并使用cookie = http.cookiejar.LWPCookieJar()cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)handler = request.HTTPCookieProcessor(cookie)opener = request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8')) 1.2处理异常1.URLErrorURLError类来自urllib库的error模块，它继承自OSError类，是error异常模块的基类，由request模块产生的一场都可以通过捕获这个类来处理。 123456import urllib.errorfrom urllib import requesttry: response = request.urlopen('http://bububu.com/index.htm')except urllib.error.URLError as e: print(e.reason) 2.HTTPError它是URLError的子类，专门用来处理HTTP请求错误，比如认证请求失败等。它有如下3个属性。 code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误。 reason：同父类一样，用于返回错误的原因。 headers：返回请求头。 12345678910import urllib.errorfrom urllib import requesttry: response = request.urlopen('http://bububu.com/index.htm')except urllib.error.HTTPError as e: print(e.reason, e.code, e.headers)except urllib.error.URLError as e: print(e.reason)else: print('Request Successfuly') 3.解析链接1.urlparse()该方法可以实现URL的识别和分段。它的api用法： 1urllib.parse.urlparse(url, scheme='', allow_fragments=True) url：必填项，即待解析的URL。 scheme：它是默认的协议（比如http或https等）。假如这个链接没有带协议信息，会将这个作为默认的协议。 allow_fragments：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，它会被解析为path、paramenters或者query的一部分，而fragment部分为空。 2.urlunparse()它接受的参数是一个可迭代对象，但是它的长度必须是6，否则会抛出参数数量不足或者过多的问题。 3.urlsplit()和urlparse()方法非常类似，只不过它不再单独解析params这一部分，只返回5个结果。params会合并到path中。 4.urlunsplit()与urlunparse()类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元祖等，唯一的区别是长度必须为5. 5.urljoin()提供一个base_url作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url的schem、netloc和path这3个内容对新链接缺失的部分进行补充，最后返回结果。 6.urlencode()在构造GET请求参数的时候非常有用，可以将参数转化为GET请求参数。 12345from urllib.parse import urlencodeparams = &#123;'name':'germey', 'age':22&#125;base_url = 'http://www.baidu.com?'url = base_url + urlencode(params)print(url) 结果为：http://www.baidu.com?name=germey&amp;age=22 7.parse_qs()如果有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典。 123from urllib.parse import parse_qsquery = 'name=germey&amp;age=22'print(parse_qs(query)) 结果为：{‘name’: [‘germey’], ‘age’: [‘22’]} 8.parse_qsl()它用于将参数转化为元组组成的列表。 123from urllib.parse import parse_qslquery = 'name=germey&amp;age=22'print(parse_qsl(query)) 结果为：[(‘name’, ‘germey’), (‘age’, ‘22’)] 9.quote()该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码。 1234from urllib.parse import quotekeyword = '壁纸'url = 'http://www.baidu.com/s?wd=' + quote(keyword)print(url) 结果为：http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8 10.unquote()它可以进行URL解码。 123from urllib.parse import unquoteurl = 'http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'print(unquote(url)) 结果为：http://www.baidu.com/s?wd=壁纸 4.分析Robots协议1.Robots协议Robots协议也称作爬虫协议，机器人协议，它的全名叫做网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做robots.txt的文本文件，一般放在网站的根目录下。 2.robotparser使用robotparser模块来解析robots.txt。该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。该类只需要在构造方法里传入robots.txt的链接即可，声明如下： 1urllib.robotparser.RobotFileParser(url='') 常用方法： set_url()：用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，那么就不要再使用这个方法设置了。 read()：读取robots.txt文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。 parse()：用来解析robots.txt，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。 can_fetch()：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。 mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓去的搜索爬虫是很有必要的，可能需要定期检查来抓取最新的robots.txt。 midified()：它同样对长时间分析和抓取的搜索爬虫很厚帮助，将当前时间设置为上次抓取和分析robots.txt的时间。 12345from urllib.robotparser import RobotFileParserrp = RobotFileParser()rp.set_url('http://www.jianshu.com/robots.txt')rp.read()print(rp.can_fetch('*','http://www.jianshu.com/p/b67554025d7d')) 2.使用requests2.1基本用法1.实例1234567import requestsr = requests.get('https://www.baidu.com')print(type(r))print(r.status_code)print(type(r.text))print(r.text)print(r.cookies) 2.GET请求 基本实例 123import requestsr = requests.get('http://httpbin.org/get')print(r.text) 结果： 1234567891011&#123; \"args\": &#123;&#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.21.0\" &#125;, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/get\"&#125; 返回结果中包含请求头、URL、IP等信息。 附加额外信息的写法 1234import requestsdata = &#123;'name':'geremy','age':22&#125;r = requests.get('http://httpbin.org/get', params=data)print(r.text) 结果为： 1234567891011121314&#123; \"args\": &#123; \"age\": \"22\", \"name\": \"geremy\" &#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.21.0\" &#125;, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/get?name=geremy&amp;age=22\"&#125; 调用json()方法，可将字符串的结果转化为字典 抓取网页 123456789import requestsimport reheaders = &#123; 'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'&#125;r = requests.get('https://www.zhihu.com/explore',headers=headers)pattern = re.compile('explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;', re.S)titles = re.findall(pattern, r.text)print(titles) 3.POST请求1234import requestsdata = &#123;'name':'germey','age':22&#125;r = requests.post('https://httpbin.org/post', data=data)print(r.text) 结果为： 1234567891011121314151617181920&#123; \"args\": &#123;&#125;, \"data\": \"\", \"files\": &#123;&#125;, \"form\": &#123; \"age\": \"22\", \"name\": \"germey\" &#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"18\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.21.0\" &#125;, \"json\": null, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/post\"&#125; form部分就是提交的数据，证明POST请求成功发送了。 2.1高级方法1.文件上传1234import requestsfiles = &#123;'file':open('favicon.ico','rb')&#125;r = requests.post('https://httpbin.org/post', files=files)print(r.text) 2.Cookies1234import requestsr = requests.get('https://www.baidu.com')for key, value in r.cookies.items(): print(key + '=' + value) 3.会话维持12345import requestss = requests.session()s.get('http://httpbin.org/cookies/set/number/123456789')r = s.get('http://httpbin.org/cookies')print(r.text) 运行结果为： 12345&#123; \"cookies\": &#123; \"number\": \"123456789\" &#125;&#125; 4.SSL证书验证12345import requestsimport logginglogging.captureWarnings(True)response = requests.get('https://www.12306.cn', verify=False)print(response.status_code) 5.代理设置12345678import requestsproxies=&#123; 'http':'http://124.207.82.166:8008', 'https':'http://124.207.82.166:8008'&#125;r = requests.get('https://www.baidu.com',proxies=proxies)print(r.status_code) 6.超时设置123import requestsr = requests.get('https://www.baidu.com',timeout=1)print(r.status_code) 7.身份认证123import requestsr = requests.get('http://localhost:9090/',auth=('username','password'))print(r.status_code) 8.Prepared Request1234567891011from requests import Request, sessionurl = 'http://httpbin.org/post'data = &#123;'name': 'germey'&#125;headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'&#125;s = Session()req = Request('POST', url, data=data, headers=headers)prepped = s.prepare_request(req)r = s.send(prepped)print(r.text) 引入Request，然后用url、data和headers参数构造了一个Request对象，这时需要再调用Session的prepare_request()方法将其转换为一个Prepared Request对象，然后调用send()方法发送即可，运行结果如下： 12345678910111213141516171819&#123; \"args\": &#123;&#125;, \"data\": \"\", \"files\": &#123;&#125;, \"form\": &#123; \"name\": \"germey\" &#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"11\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36\" &#125;, \"json\": null, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/post\"&#125; 3.抓取猫眼排行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import osimport requestsimport reimport jsonimport timefrom requests.exceptions import RequestExceptiondef get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko)' + 'Chrome/76.0.3809.100 Safari/537.36' &#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text return None except RequestException: return Nonedef parse_one_page(html): pattern = re.compile('&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=\"(.*?)\".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;' + '(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html) for item in items: yield &#123; 'index': item[0], 'image': item[1], 'title': item[2].strip(), 'actor': item[3].strip()[3:], 'time': item[4].strip()[5:], 'score': item[5].strip() + item[6].strip() &#125;def write_to_file(content): with open('reulst.txt', 'a', encoding='utf-8') as f: f.write(json.dumps(content, ensure_ascii=False)+'\\n')def main(offset): url = 'https://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) for item in parse_one_page(html): print(item) write_to_file(item)if __name__ == '__main__': for i in range(10): main(offset = i*10) time.sleep(1)","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[{"name":"python爬虫","slug":"python爬虫","permalink":"/tags/python爬虫/"}]},{"title":"正则表达式","slug":"正则表达式","date":"2019-06-18T11:55:04.000Z","updated":"2019-09-19T11:03:32.892Z","comments":true,"path":"2019/06/18/正则表达式/","link":"","permalink":"/2019/06/18/正则表达式/","excerpt":"","text":"一.Egrep元字符使用·表示空格 行的起始和结束「^」代表一行的开始，「$」代表结束。 「^cat$」：匹配的条件是，行开头，然后是字符c·a·t，然后是行末尾。只包含cat的行—没有多余的单词、空白字符……只有’cat’。 「^$」：匹配的条件是行开头，然后就行末尾。表示是空行。 「^」：匹配条件是行的开头。应用意义为无意义，每一行都有开头，所以都能匹配，空行也可以。 字符组匹配若干字符之一使用正则表达式结构体[…]，它容许使用者列出在某处期望匹配的字符，通常被称作字符组。正则表达式[ea]能匹配a或者e。 在字符组以外，普通字符(例如gr[ae]y中的g和r)都有接下来是的意思—首先匹配g，接下来是r。字符组的内容是在同一个位置能够匹配的若干字符，它的意思是或。 在字符组内部，字符组元字符’-‘表示一个范围：在&lt;H[1-6]&gt;与&lt;H[123456]&gt;是完全一样的。只有在字符组内部，连字符才是元字符，否则它就只能匹配普通的连字符号。如果连字符出现在字符组的开头，它表示的就只是一个普通字符，而不是一个范围。 排除型字符组用[^]取代[]，这个字符组就会匹配任何未列出的字符。例如[^1-6]匹配除了1到6以外的任何字符。 排除型字符组表示””匹配一个未列出的字符””，而不是”不要匹配列出的字符”。 用点号匹配任意字符元字符「.」是用来匹配任意字符的字符组的简便写法。 例如：需要搜索03/19/76、03-19-76或03.19.76，不怕麻烦的话用一个明确容许’/‘、’-‘、’.’的字符组来构建正则表达式，例如「03[-./]19[-./]76」。也可以使用「03.19.76」。 多选结构匹配任意子表达式「|」是一个非常简捷的元字符，它的意思是”或”。 「Bob」,「Robert」是两个正则表达式，「Bob|Robert」表示可以同时匹配其中任意一个的正则表达式。 「gr[ae]y」，可以写成「gray|grey」或者「gr(a|e)y」。 忽略大小写把-i写在正则表达式之前。 单词分界符「\\ &lt;」和「\\ &gt;」可以使用它们来匹配单词分界的位置。「&lt;」和「&gt;」本身并不是元字符，只有它们与斜线结合起来的时候，整个序列才具有特殊意义。 某些版本的egrp可能不支持。 可选项元素「?」代表可选项，把它加在一个字符的后面，就表示此处容许出现这个字符，它的出现并非匹配成功的必要条件。 color和colour可以用「colou?r」进行匹配。「u?」只作用于之前紧邻的元素。 其他量词：重复出现「+」和「」的作用与问号类似。元字符「+」表示”之前紧邻的元素出现一次或多次”，「 」表示”之前紧邻的元素出现任意多次，或者不出现”。换种说法就是「··· *」表示”匹配尽可能多的次数，如果实在无法匹配，也不要紧”。「···+」的意思与之类似，也是匹配尽可能多的次数，但如果连一次匹配都无法完成，就报告失败。 &lt;HR·SIZE=14&gt;匹配：「&lt;HR·+SIZE· = · [0-9]+· *&gt;」 如果size也是可选的，则为「&lt;HR(·+SIZE·=· [0-9]+)？· *&gt;」 规定重现次数的范围：区间某些版本的egrep能够使用元字符序列来自定义重现次数的区间:「···{min,max}」，这成为”区间量词”。例如，「···{3,12}」能够容许的重现次数在3到12之间。 括号及反向引用括号能够”记住”它们包含的子表达式匹配的文本。如果确切知道重复单词的第一个单词（比如是”the”），就能够明确无误的找到它，例如「the the」，但是还是会匹配到the theory的情况，所以加上单词分界符「\\ 」。 反向引用是正则表达式的特性之一，它容许我们匹配与表达式先前部分匹配的同样的文本。 先把「\\ 」中的第一个「the」替换为能够匹配任意单词的正则表达式「[A-Za-z]+」；然后在两端加上括号；最后把后一个’the’替换为特殊的元字符序列「\\1」，就得到了「\\ &lt;([A-Za-z]+)·+\\1\\ &gt;」。如果一个单词在某行末尾，另一个单词在下一行的开头，该表达式无法找到。 也可以在一个表达式中使用多个括号，再用「\\1」「\\2」「\\3」来表示第一、第二、第三组括号匹配的文本。「([a-z])([0-9])\\1\\2」中的「\\1」代表[a-z]匹配的内容，「\\2」代表[0-9]匹配的内容。 神奇的转义反斜线\\为转义符，它作用的元字符会失去特殊意义，成了普通字符。 如匹配ega.att.com，则为「ega\\ .att\\ .com」 还可以使用「\\ ([a-zA-Z]+\\ )」来匹配一个括号内的单词，如(very)。 二.常用的元字符和特性字符组简记法\\d 数字：等价于「[0-9]」。 \\D 非数字字符：等价于「[^\\d]」。 \\w 单词中的字符：一般等价于「[a-zA-Z0-9_]」。 \\W 非单词字符：等价于「[^\\w]」。 ​ \\s 空白字符：在支持ASCII的系统中，它通常等价于「[·\\f\\n\\r\\r\\t\\v]」。 \\S 非空白字符：等价于「[^\\s]」。 锚点及其他”零长度断言”行/字符串的起始位置：^、\\A行/字符串的结束为止：$、\\Z和\\z匹配的起始位置（或者是上一次匹配的结束位置）：\\G单词分界符：\\b、\\B、\\ &lt;、\\ &gt;单词分界符可以分成两类，一类中单词起始位置分界符和结束位置分界符是相同的(通常是\\ &lt;和\\ &gt;)，另一类则以统一的分界符来匹配（通常是\\b）。两类都提供了非单词分界符序列（通常是\\B）。 注释和模式修饰符模式修饰符：(?modifier)，例如(?i)和(?-i)「(?i)」会启用不区分大小写的匹配，而「(?-i)」会停用此功能。 模式作用范围：(?modifier:…)，例如(?i:···)「(?:(?i)very)」可以简写成「(?i:very)」 分组，捕获，条件判断和控制捕获/分组括号：(…)和\\1,\\2,…仅用于分组的括号：(?:…)命名捕获：(?&lt; Name&gt;…)Python和PHP的preg引擎，以及.NET引擎，都能够为捕获内容命名。Python和PHP使用的语法是「(?P&lt; name&gt;…)」，而.NET引擎使用「(?&lt; name&gt;…)」。 「\\b(?P&lt; Area&gt;\\d\\d\\d)-(?P&lt; Exch&gt;\\d\\d\\d)-(?P&lt; Num&gt;\\d\\d\\d\\d)\\b」 这个表达式会用美国电话号码的各个部分’’填充”Area、Exch和Num命名的内容。 固化分组：(?&gt;…)可以提高效率。 二.正则表达式实用技巧正则表达式的平衡法则好的正则表达式必须在这些方面求得平衡： 只匹配期望的文本，排除不期望的文本。 必须易于控制和理解。 如果是呀NFA引擎，必须保证效率（如果能够匹配，必须很快地返回匹配结果，如果不能匹配，应该在尽可能短的时间内报告匹配失败）。 若干简单的例子1.匹配连续行(续前)^\\w+=([ ^\\n\\ ]||\\.)* 2.匹配IP地址^([01]?\\d\\d?|2[0-4]\\d|25[0-5]).([01]?\\d\\d?|2[0-4]\\d|25[0-5]).([01]?\\d\\d?|2[0-4]\\d|25[0-5]).([01]?\\d\\d?|2[0-4]\\d|25[0-5])$ 3.处理文件名去掉文件名开头的路径 Unix: ^.*/ windows: ^.*\\ \\ 从路径中获取文件名 [^/]*$ 所在路径和文件名 ^(. )/( [ ^/ ] )$","categories":[{"name":"正则表达式","slug":"正则表达式","permalink":"/categories/正则表达式/"}],"tags":[]},{"title":"(九)Executor框架","slug":"Executor框架","date":"2019-04-11T13:18:59.000Z","updated":"2019-04-13T06:11:59.889Z","comments":true,"path":"2019/04/11/Executor框架/","link":"","permalink":"/2019/04/11/Executor框架/","excerpt":"","text":"简介Executor框架的结构​ Executor框架主要由3大部分组成。 任务。包括被执行任务需要实现的接口：Runnable接口或Callable接口。 任务的执行。包括任务执行机制的核心接口Executor，以及继承自Executor的ExecutorService接口。Executor框架有两个关键类实现了ExecutorService接口(ThreadPoolExecutor和ScheduledThreadPoolExecutor) 异步计算的结果。包括接口Future和实现Future接口的FutureTask类。 类和接口的简介。 Executor是一个接口，它是Executor框架的基础，它将任务的提交与任务的执行分离开来。 ThreadPoolExecutor是线程池的核心实现类，用来执行被提交的任务。 ScheduledThreadPoolExecutor是一个实现类，可以在给定的延迟后运行命令，或者定期执行命令。ScheduledThreadPoolExecutor比Timer更灵活，功能更强大。 Futrue接口和实现Future接口的FutureTask类，代表异步计算的结果。 Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。 Executor框架的成员(1)ThreadPoolExecutor ​ ThreadPoolExecutor通常使用工厂类Executors来创建。Executors可以创建3种类型的ThreadPoolExecutor:SingleThreadExecutor、FixedThreadPool和CachedThreadPool。 ​ 1)FixedThreadPool。适用于为了满足资源管理的需求，而需要限制当前线程数量的应用场景，它适用于负载比较重的服务器。 ​ 2)SingleThreadExecutor。适用于需要保证顺序地执行各个任务；并且在任意时间点，不会有多个线程是活动的应用场景。 ​ 3)CachedThreadPool。是大小无界的线程池，适用于执行很多的短期异步任务的小程序，或者是负载较轻的服务器。 (2)ScheduledThredPoolExecutor ​ ScheduledThreadPoolExecutor通常使用工厂类Executor来创建。Executors可以创建两种类型的ScheduledThreadPoolExecutor。 ​ 1)ScheduledThreadPoolExecutor。包含若干个线程的ScheduledThreadPoolExecutor。适用于需要多个后台线程执行周期任务，同时为了满足资源管理的需求而需要限制后台线程的数量的应用场景。 ​ 2)SingleThreadScheduledExecutor。只包含一个线程的ScheduledThreadPoolExecutor。适用于需要单个后台线程执行周期任务，同时需要保证顺序地执行各个任务的应用场景。 (3)Future接口 ​ Future接口和实现Future接口的FutureTask类用来表示异步计算的结果。当把Runnable接口或Callable接口的实现类提交(submit)给ThreadPoolExecutor或ScheduledThreadPoolExecutor时，会返回一个FutureTask对象。 (4)Runnable接口和Callable接口 ​ Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。它们之间的区别是Runnable不会返回结果，而Callable可以返回结果。 ThreadPoolExecutor详解​ Executor框架最核心的类是ThreadPoolExecutor，它是线程池的实现类，主要由4个组件构成。 corePool：核心线程池的大小。 maximumPool：最大线程池的大小。 BlockingQueue：用来暂时保存任务的工作队列。 RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和时(达到了最大线程池大小且工作队列已满)，execute()方法将要调用的Handler。 FixedThreadPool详解​ FixedThreadPool被称为可重用固定线程数的线程池。 ​ FixedThreadPool的corePoolSize和maximumPoolSize都被设置为创建FixedThreadPool时指定的参数nThreads。 ​ 当线程池中的线程数大于corePoolSize时，keepAliveTiem为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止。这里把keepAliveTime设置为0L，意味着多余的空闲线程会被立即终止。 SingleThreadExecutor详解​ SingleThreadExecutor是使用单个worker线程的Executor。SignleThreadExecutor的corePoolSize和maximumPoolSize被设置为1。其他参数与FixedThreadPool相同。SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工作队列(队列的容量为Integer.MAX_VALUE)。 CachedThreadPool详解​ CachedThreadPool是一个会根据需要创建新线程的线程池。 ​ CachedThreadPool的corePoolSize被设置为0，即corePool为空；maximumPoolSize被设置为Integer.MAX_VALUE，即maximumPool是无界的。这里把keepAliveTime设置为60L，意味着CachedThreadPool中的空闲线程等待新任务的最长时间为60秒，空闲线程超过60秒后将会被终止。 ​ FixedThreadPool和SingleThreadExecutor使用无界队列LinkeBlockingQueue作为线程池的工作队列。CachedThreadPool使用没有容量的SynchronousQueue作为线程池的工作队列，但CachedThreadPool的maximumPool是无界的。意味着如果主线程提交任务的速度高于maximumPool中线程处理任务的速度时，CachedThreadPool会不断创建新线程。极端情况下，CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。 ScheduledThreadPoolExecutor详解​ ScheduledThreadPoolExecutor继承自ThreadPoolExecutor。它主要用来在给定的延迟之后运行任务，或者定期执行任务。ScheduledThreadPoolExecutor的功能与Timer类似，Timer对应的是单个后台线程，而ScheduledThreadPoolExecutor可以在构造函数中指定多个对应的后台线程数。 ScheduledThreadPoolExecutor的运行机制​ ScheduledThreadPoolExecutor的执行主要分为两大部分。 当调用ScheduledThreadPoolExecutor的scheduleAtFixedRate()方法或者schedWithFixedDelay()方法时，会向ScheduledThreadPoolExecutor的DalayQueue添加一个实现了RunnableScheduledFutur接口的ScheduledFutureTask。 线程池中的线程从DelayQueue中获取ScheduledFutureTask，然后执行任务。 ScheduledThreadPoolExecutor的实现​ ScheduledFutureTask主要包含3个成员变量。 long型成员变量time，表示这个任务将要被执行的具体时间。 long型成员变量sequenceNumber，表示这个任务被添加到ScheduledThreadPoolExecutor中的序号。 long型成员变量period，表示任务执行的间隔周期。 DelayQueue封装了一个PriorityQueue，这个PriorityQueue会对队列中的ScheduledFutureTask进行排序。排序时，time小的排在前面(时间早的任务将被先执行)。如果两个ScheduledFutureTask的time相同，就比较sequenceNumber，sequenceNumber小的排在前面(也就是说，如果两个任务的执行时间相同，那么先提交的任务将被先执行)。 FutureTask详解​ Future接口和实现FutureTask类，代表异步计算的结果。 FutureTask简介​ FutureTask除了实现Future接口外，还实现了Runnable接口。因此，FutureTask可以交给Executor执行，也可以由调用线程直接执行(FutureTask.run())。根据FutureTask.run()方法被执行的时机，有3种状态。 未启动。FutureTask.run()方法还没有被执行之前，FutureTask处于未启动状态。当创建一个FutureTask，且没有执行FutureTask.run()方法之前，这个FutureTask处于未启动状态。 已启动。FutureTask.run()方法被执行的过程中，FutureTask处于已启动状态。 已完成。FutureTask.run()方法执行完后正常结束，或被取消(FutureTask.cancel(…))，或执行FutureTask.run()方法时抛出异常而异常结束，FutureTask处于已完成状态。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(八)Java中的线程池","slug":"Java中的线程池","date":"2019-04-10T07:45:25.000Z","updated":"2019-09-19T11:02:45.450Z","comments":true,"path":"2019/04/10/Java中的线程池/","link":"","permalink":"/2019/04/10/Java中的线程池/","excerpt":"","text":"使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的课管理型。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的实现原理​ 当提交一个新任务到线程池时，线程池的处理流程如下。 线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。如果核心线程池都在执行任务，则进入下个流程。 线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。 线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。 ThreadPoolExecutor执行execute方法分下面4种情况。 如果当前运行的线程少于corePoolSize，则创建新线程来执行任务(执行这一步骤需要获取全局锁)。 如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue。 如果无法将任务加入BlockingQueue(队列已满)，则创建新的线程来处理任务(执行这一步骤需要获取全局锁)。 如果创建新线程将使当前运行的线程超出maximumPoolSize，任务将被拒绝，并调用RejectedExecutionHandler.rejectedExecution()方法。 线程池的使用线程池的创建​ 通过ThreadPoolExecutor来创建一个线程池。 1new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, milliseconds, runnableTaskQueue, handler); 1.corePoolSize(线程池的基本大小)：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。 2.runnableTaskQueue(任务队列)：用于保存等待执行的任务的阻塞队列。 3.maximumPoolSize(线程池最大数量)：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。 4.ThreaFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建的线程设置更有意义的名字。使用开源框架guava提供的ThreadFactoryBuilder可以快速给线程池里的线程设置有意义的名字。 1new ThreadFactoryBuilder().setNameFormat(\"XX-task-%d\").build(); 5.RejectedExecutionHandler(饱和策略)：当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。 向线程池提交任务​ 可以使用两个方法向线程池提交任务，分别为execute()和submit()方法。 ​ execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。 123456threadsPool.execute(new Runnable()&#123; @Override public void run()&#123; //TODO Auto-generated method stub &#125; &#125;); ​ submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future()的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get(long timeout, TimeUnit unit)方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 1234567891011Future&lt;Object&gt; future = executor.submit(harReturnValuetask); try &#123; Object s = future.get(); &#125; catch (InterruptedException e) &#123; //处理中断异常 &#125; catch (ExecutionException e)&#123; //处理无法执行任务异常 &#125; finally &#123; //关闭线程池 executor.shutdown(); &#125; 关闭线程池​ 可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，而shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。 线程池的监控​ 可以通过以下属性队线程池进行监控。 taskCount：线程池需要执行的任务数量。 completedTaskCount：线程池在运行过程中已完成的任务数量，小于或等于taskCount。 largesPoolSize：线程池里曾经创建过的最大线程数量。通过这个数据可以知道线程池是否曾经满过。如该数值等于线程池的最大大小，则表示线程池曾经满过。 getPoolSize：线程池的线程数量。如果线程池不销毁的话，线程池里的线程不会自动销毁，所以这个大小只增不减。 getActiverCount：获取活动的线程数。 corePoolSize(线程池的基本大小)：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。 runnableTaskQueue(任务队列)：用于保存等待执行的任务的阻塞队列。 maximunPoolSize(线程池最大数量)：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。 ThreadFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。使用开源框架guava提供的ThreadFactoryBuilder可以快速给线程池里的线程设置有意义的名字。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(七)Java中的并发工具类","slug":"Java中的并发工具类","date":"2019-04-09T08:16:06.000Z","updated":"2019-04-10T07:42:50.409Z","comments":true,"path":"2019/04/09/Java中的并发工具类/","link":"","permalink":"/2019/04/09/Java中的并发工具类/","excerpt":"","text":"等待多线程完成的CountDownLatch​ CountDownLatch允许一个或多个线程等待其他线程完成操作。 12345678910111213141516public class CountDownLatchTest &#123; public static void main(String[] args) throws Exception&#123; CountDownLatch c = new CountDownLatch(2); new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(1); c.countDown(); System.out.println(2); c.countDown(); &#125; &#125;).start(); c.await(); System.out.println(\"3\"); &#125;&#125; ​ CountDonwLatch的构造函数接收一个int类型的参数作为计数器，如果想等待N个点完成，就传入N。 ​ 计数器必须大于等于0，只是等于0的时候，计数器就是零，调用await方法时不会阻塞当前线程。CountDownLatch不可能重新初始化或者修改CountDownLatch对象的内部计数器的值。一个线程调用countDown方法happen-before，另外一个线程调用await方法。 同步屏障CyclicBarrier​ CyclicBarrier要做的事情是，让一组线程到达一个屏障(也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。 1234567891011121314151617181920212223242526public class CyclicBarrierTest &#123; public static void main(String[] args) &#123; CyclicBarrier c = new CyclicBarrier(2); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(1); &#125; &#125;).start(); try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(2); &#125;&#125; ​ 因为主线程和子线程的调度是由CPU决定的，两个线程都有可能先执行。如果把new CyclicBarrier(2)修改成new CyclicBarrier(3)，则主线程和子线程会永远等待，因为没有第三个线程执行await方法，即没有第三个线程到达屏障，所以之前到达屏障的两个线程都不会继续执行。 1234567891011121314151617181920212223242526272829303132public class CyclicBarrierTest2 &#123; public static void main(String[] args) &#123; CyclicBarrier c = new CyclicBarrier(2, new A()); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(1); &#125; &#125;).start(); try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(2); &#125; static class A implements Runnable&#123; @Override public void run() &#123; System.out.println(3); &#125; &#125;&#125; ​ 因为CyclicBarrier设置了拦截线程的数量是2，所以必须等代码中的第一个线程和线程A都执行完后，才会继续执行主线程，然后输出2。 控制并发线程数的Semaphore​ Semaphore是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。 12345678910111213141516171819202122public class SemaphoreTest &#123; private static final int THREAD_COUNT = 30; private static ExecutorService threadPool = Executors.newFixedThreadPool(THREAD_COUNT); private static Semaphore s = new Semaphore(10); public static void main(String[] args) &#123; for (int i = 0; i &lt; THREAD_COUNT; i++) &#123; threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; s.acquire(); System.out.println(\"save data\"); s.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; threadPool.shutdown(); &#125;&#125; ​ 虽然有30个线程在执行，但是只允许10个并发执行。Semaphore的构造方法Semaphore(int permits)接受一个整型的数字，表示可用的许可证数量。Semaphore的用法很简单，首先线程使用Semaphore的acquire()方法获取一个许可证，使用完之后调用release()方法归还许可证。还可以用tryAcquire()方法尝试获取许可证。 线程间交换数据的Exchanger​ Exchanger是一个用于线程间协作的工具类。Exchanger用于进行线程间的数据交换。它提供一个同步点，在这个同步点，两个线程可以交换彼此的数据。这两个线程通过exchange方法交换数据，如果第一个线程先执行exchange()方法，它会一致等待第二个线程也执行exchange方法，当两个线程到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据传递给对方。 12345678910111213141516171819202122232425262728293031public class ExchangerTest &#123; public static void main(String[] args) &#123; Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;(); ExecutorService threadPool = Executors.newFixedThreadPool(2); threadPool.execute(new Runnable() &#123; @Override public void run() &#123; //A录入银行流水数据 String A = \"银行流水A\"; try &#123; exchanger.exchange(A); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); threadPool.execute(new Runnable() &#123; @Override public void run() &#123; String B = \"银行流水B\"; try &#123; String A = exchanger.exchange(\"B\"); System.out.println(\"A和B数据是否一致:\" + A.equals(B) + \",A录入的是:\" + A + \",B录入的是\" + B); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); threadPool.shutdown(); &#125;&#125;","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(六)Java中的12个原子操作类","slug":"Java中的12个原子操作类","date":"2019-04-09T03:39:46.000Z","updated":"2019-04-09T08:16:38.767Z","comments":true,"path":"2019/04/09/Java中的12个原子操作类/","link":"","permalink":"/2019/04/09/Java中的12个原子操作类/","excerpt":"","text":"原子更新基本类型类​ 使用原子的方式更新基本类型，Atomic包提供了以下3个类。 AtomicBoolean：原子更新布尔型 AtomicInteger：原子更新整型 AtomicLong：原子更新长整型 12345678910public class AtomicIntegerTest &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger = new AtomicInteger(1); System.out.println(atomicInteger.getAndIncrement()); System.out.println(atomicInteger.get()); atomicInteger.addAndGet(5); System.out.println(atomicInteger.get()); &#125;&#125; 原子更新数组​ 通过原子的方式更新数组里的某个元素，Atomic包提供了以下3个类。 AtomicIntegerArray：原子更新整型数组里的元素 AtomicLongArray：原子更新长整型数组里的元素 AtmoicReferenceArray：原子更新引用类型数组里的元素 12345678910public class AtomicIntegerArrayTest &#123; public static void main(String[] args) &#123; int[] value = new int[]&#123;1, 2&#125;; AtomicIntegerArray integerArray = new AtomicIntegerArray(value); integerArray.getAndSet(0, 3); System.out.println(integerArray.get(0)); System.out.println(value[0]); &#125;&#125; ​ 数组value通过构造方法传递进去，然后AtomicIntegerArray会将当前数组复制一份，所以当AtomicIntegerArray对内部的数组元素进行修改时，不会影响传入的数组。 原子更新引用类型​ 原子更新基本类型的AtomicInteger，只能更新一个变量，如果要原子更新多个变量，就需要使用这个原子更新引用类型提供的类。Atomic包提供了以下3个类。 AtomicReference：原子更新引用类型 AtomicReferenceFieldUpdater：原子更新引用类型里的字段 AtomicMarkableReference：原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和引用类型 12345678910111213141516171819202122232425public class AtomicReferenceTest &#123; public static void main(String[] args) &#123; AtomicReference&lt;User&gt; userAtomicReference = new AtomicReference&lt;&gt;(); User user = new User(\"张三\", 18); userAtomicReference.set(user); User updateUser = new User(\"李四\", 20); userAtomicReference.compareAndSet(user, updateUser); System.out.println(userAtomicReference.get().getName()); System.out.println(userAtomicReference.get().getOld()); &#125; static class User&#123; private String name; private int old; public User(String name, int old)&#123; this.name = name; this.old = old; &#125; public String getName() &#123; return name; &#125; public int getOld() &#123; return old; &#125; &#125;&#125; 原子更新字段类​ 如果需原子地更新某个类里的某个字段时，就需要使用原子更新字段类，Atomic包提供了以下3个类进行原子字段更新。 AtomicIntegerFieldUpdater：原子更新整型的字段的更新器 AtomicLongFieldUpdate：原子更新整型字段的更新器 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于原子的更新数据和数据的版本号，可以解决使用CAS进行原子更新时可能出现的ABA问题 要想原子地更新字段需要两步。第一步，因为原子更新字段类都是抽象类，每次使用的时候必须使用静态方法newUpdater()创建一个更新器，并且需要设置想要更新的类和属性。第二步，更新类的字段(属性)必须使用public volatile修饰符。 12345678910111213141516171819202122public class AtomicIntegerFieldUpdaterTest &#123; public static void main(String[] args) &#123; AtomicIntegerFieldUpdater&lt;User&gt; a = AtomicIntegerFieldUpdater.newUpdater(User.class, \"old\"); User user = new User(\"张三\", 18); System.out.println(a.getAndIncrement(user)); System.out.println(a.get(user)); &#125; public static class User &#123; private String name; public volatile int old; public User(String name, int old) &#123; this.name = name; this.old = old; &#125; public String getName() &#123; return name; &#125; public int getOld() &#123; return old; &#125; &#125;&#125;","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(五)Java并发容器和框架","slug":"Java并发容器和框架","date":"2019-04-03T06:20:05.000Z","updated":"2019-04-09T03:38:45.576Z","comments":true,"path":"2019/04/03/Java并发容器和框架/","link":"","permalink":"/2019/04/03/Java并发容器和框架/","excerpt":"","text":"ConcurrentHashMap的实现原理与使用ConcurrentHashMap的结构​ ConcurrentHashMap是由Segment数组结构和HashEntry数据结构组成。Segment是一种可重入锁(ReentrantLock)，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素,当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁。 ConcurrentHashMap的操作1.get操作 ​ Segment的get操作实现非常简单和高校。先经过一次再散列，然后使用这个散列值通过散列运算定位到Segment，再通过散列算法定位到元素。get操作的高效之处在于整个get过程不需要加锁，除非读到的值是空才会加锁重读。因为get方法里将要使用的共享变量都定义为volatile类型。 2.put操作 ​ 由于put方法里需要对共享变量进行写入操作，所以为了线程安全。在操作共享变量时必须加锁。put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作需要经历两个步骤，第一步判断是否需要对Segment里的HashEntry数据进行扩容，第二步定位添加元素的位置，然后将其放在HashEntry数组里。 Fork/Join框架工作窃取算法​ 工作窃取算法是指某个线程从其他队列里窃取任务来执行。 ​ 工作窃取算法的优点：充分利用线程进行并行计算，减少了线程间的竞争。 ​ 工作窃取算法的缺点：在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且该算法会消耗了更多的系统资源，比如创建多个线程和多个双端队列。 Fork/Join框架的异常处理​ ForkJoinTask在执行的时候可能会抛出异常，但是没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。 ​ getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。 Fork/Join框架的实现原理​ ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责将存放程序提交给ForkJoinPool的任务，而ForkJoinWorkerThread数组负责执行这些任务。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(四)Java中的锁","slug":"Java中的锁","date":"2019-04-01T02:00:49.000Z","updated":"2019-04-03T06:18:34.798Z","comments":true,"path":"2019/04/01/Java中的锁/","link":"","permalink":"/2019/04/01/Java中的锁/","excerpt":"","text":"Lock接口​ 锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源。 ​ Lock的使用： 1234567Lock lock = new ReentrantLock();lock.lock();tyr&#123;&#125; finally&#123;lock.unlock();&#125; ​ 在finally块中释放锁，目的是保证在获取到锁之后，最终能够被释放。 ​ 不要将获取锁的过程写在try块中，因为如果在获取锁(自定义锁的实现)时发生了异常，异常抛出的同时，也会导致锁无故释放。 ​ Lock接口提供的synchronized关键字所不具备的主要特性。 特性 描述 尝试非阻塞地获取锁 当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁 能被中断地获取锁 与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放 超时获取锁 在指定的截止时间之前获取锁，如果截止时间到了仍旧无法获取锁，则返回 ​ Lock是一个接口，它定义了锁获取和释放的基本操作。 方法名称 描述 void lock() 获取锁，调用该方法当前线程将会获取锁，当锁获得后，从该方法返回 void lockInterruptibly() throws InterruptedException 可中断地获取锁，和lock()方法的不同之处在于该方法会响应中断，即在锁的获取中可以中断当前线程 boolean tryLock() 尝试非阻塞的获取锁，调用该方法后立刻返回，如果能够获取则返回true，否则返回false boolean tryLock(long time, TimeUnit unit) throws InterruptedException 超时的获取锁，当前线程在以下3种情况下会返回：1.当前线程在超时时间内获得了锁 2.当前线程在超时时间内被中断 3.超时时间结束，返回false void unlock() 释放锁 Condition newCondition() 获取等待通知组件，该组件和当前的锁绑定，当前线程只有获得了锁，才能调用该组件的wait()方法，而调用后，当前线程将释放锁 队列同步器​ 队列同步器AbstractQueuedSynchronized，是用来构建锁或者其他同步组件的基础框架，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。 同步队列​ 同步器依赖内部的同步队列(一个FIFO双向队列)来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。 ​ 同步队列中的节点用来保存获取同步状态失败的线程引用、等待状态以及前驱和后继节点。 独占式同步状态获取与释放​ 通过调用同步器的acquire(int arg)方法可以获取同步状态，该方法对中断不敏感，也就是由于线程获取同步状态失败后进入同步队列中，后续对线程进行中断操作时，线程不会从同步队列中移出。 ​ 当前线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释放了同步状态之后，会唤醒其后继节点(进而使后继节点重新尝试获取同步状态)。 共享式同步状态获取与释放​ 通过调用同步器的acquireShared(int arg)方法可以共享式地获取同步状态。通过调用realeaseShared(int arg)方法可以释放同步状态。 重入锁​ 重入锁ReentrantLock，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。除此之外，该锁还支持获取锁时的公平和非公平选择。 读写锁​ 读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 特性 说明 公平性选择 支持非公平(默认)和公平的锁获取方式，吞吐量还是非公平优于公平 重进入 该锁支持重进入，以读写线程为例：读线程在获取了读锁之后，能够再次获取读锁。而写线程在获取了写锁之后能够再次获取读写锁，同时也可以获取读锁 锁降级 遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁 写锁的获取与释放​ 写锁是一个支持重进入的排他锁。如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取(读状态不为0)或者该线程不是已经获取写锁的线程，则当前线程进入等待状态。 ​ 写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，从而等待的读写线程能够继续访问读写锁，同时前次写线程的修改对后续读写线程可见。 读锁的获取释放​ 读锁是一个支持重进入的共享锁，它能够被多个线程同时获取，在没有其他写线程访问(或者写状态为0)时，读锁总会被成功地获取，而所做的也只是(线程安全的)增加读状态。如果当前线程已经获取了读锁，则增加读状态。如果当前线程在获取读锁时，写锁已被其他线程获取，则进入等待状态。 锁降级​ 锁降级指的是写锁降级成为读锁。锁降级是指把持住(当前拥有的)写锁，再获取到读锁，随后释放(先前拥有的)写锁的过程。 Condition接口​ Condition定义了等待/通知两种类型的方法，当前线程调用这些方法时，需要提前获取到Condition对象关联的锁。Condition对象时由Lock对象(调用Lock对象的newCondition()方法)创建出来的，Condition是依赖Lock对象的。 等待队列​ 等待队列是一个FIFO的队列，在队列的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程会释放锁、构造成节点加入等待队列并进入等待状态。 等待​ 调用Condition的await()方法，会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从await()方法返回时，当前线程一定获取了Condition相关联的锁。 通知​ 调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点(首节点)，在唤醒节点之前，会将节点移到同步队列中。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(三)Java并发编程基础","slug":"Java并发编程基础","date":"2019-03-28T13:28:20.000Z","updated":"2019-04-01T02:00:12.109Z","comments":true,"path":"2019/03/28/Java并发编程基础/","link":"","permalink":"/2019/03/28/Java并发编程基础/","excerpt":"","text":"线程简介线程优先级​ 现代操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配。线程分配到的时间片多少也就决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。 ​ 在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1～10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程。设置线程优先级时，针对频繁阻塞(休眠或者I/O操作)的线程需要设置较高优先级，而偏重计算(需要较多CPU时间或者偏运算)的线程舍之间较低的优先级，确保处理器不会被独占。在不同的JVM以及操作系统上，线程规划会存在差异，有些操作系统甚至会忽略对线程优先级的设定。 线程状态 状态名称 说明 NEW 初始状态，线程被构建，但是还没有调用start()方法 RUNNABLE 运行状态，Java线程将操作系统中的就绪和运行两种状态笼统地称作”运行中” BLOCKED 阻塞状态，表示线程阻塞于锁 WAITING 等待状态，表示线程进入等待状态，进入该状态表示当前线程需要等待其他线程做出一些特定动作(通知或中断) TIME_WATING 超时等待状态，该状态不同于WATING，它是可以在指定的时间自行返回的 TERMINATED 终止状态，表示当前线程已经执行完毕 Daemon线程​ Dameon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这意味着，当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。可以通过调用Thread.setDaemon(true)将线程设置为Daemon线程。 ​ Daemon线程需要在启动线程之前设置，不能在启动线程之后设置。 ​ Daemon线程被用作完成支持性工作，但是在Java虚拟机退出时Daemon线程中的finally块并不一定会执行。 启动和终止线程启动线程​ 线程对象在初始化完成之后，调用start()方法就可以启动这个线程。线程start()方法的含义是：当前线程(即parent线程)同步告知Java虚拟机，只要线程规划器空闲，应立即启动调用start()方法的线程。 理解中断​ 中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。其他线程通过调用该线程的interrupt()方法对其进行中断操作。 ​ 线程通过检查自身是否被中断来进行响应，线程通过方法isInterrupted()来进行判断是否被中断，也可以调用静态方法Thread.interrupted()对当前线程的中断标识位进行复位。如果该线程已经处于终结状态，即使该线程被中断过，在调用该线程对象的isInterrupted()时依旧会返回false。 安全地终止线程​ 中断操作是一种简便的线程间交互方式，而这种交互方式最适合用来取消或停止任务。除了中断以外，还可以利用一个boolean变量来控制是否需要停止任务并终止该线程。 1234567891011121314151617181920212223242526272829303132333435public class ShutDown &#123; public static void main(String[] args) throws Exception&#123; Runner one = new Runner(); Thread countThread = new Thread(one, \"CountThread\"); countThread.start(); //睡眠1秒，main线程对CountThread进行中断，使CountThread能够感知中断而结束 TimeUnit.SECONDS.sleep(1); countThread.interrupt(); Runner two = new Runner(); countThread = new Thread(two, \"CountThread\"); countThread.start(); //睡眠1秒，main线程对Runner two进行取消，使CountThread能够感知on为false而结束 TimeUnit.SECONDS.sleep(1); two.cancel(); &#125; private static class Runner implements Runnable &#123; private long i; private volatile boolean on = true; public void run() &#123; while (on &amp;&amp; !Thread.currentThread().isInterrupted()) &#123; i++; &#125; System.out.println(\"Count i = \" + i); &#125; public void cancel() &#123; on = false; &#125; &#125;&#125; 线程间通信volatile和synchronized关键字​ 关键字volatile可以用来修饰字段(成员变量)，就是告知程序任何对该变量的访问均需要从共享内存中获取，而对它的改变必须同步刷新回共享内存，它能保证所有线程对变量访问的可见性。 ​ 关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，它保证了线程对变量访问的可见性和排他性。 等待/通知机制 方法名称 描述 notify() 通知一个在对象上等待的线程，使其从wait()方法返回，而返回的前提是该线程获取到了对象的锁 notifyAll() 通知所有等待在该对象上的线程 wait() 调用该方法的线程进入WAITING状态，只有等待另外线程的通知或被中断才会返回，需要注意，调用wait()方法后，会释放对象的锁 wait(long) 超时等待一段时间，这里的参数时间是毫秒，也就是等待长达n毫秒，如果没有通知就超时返回 wait(long,int) 对于超时时间更细粒度的控制，可以达到纳秒 ​ 等待/通知机制，是指一个线程A调用了对象O的wait()方法进入等待状态， 而另一个线程B调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 等待/通知 * */public class WaitNotify &#123; static boolean flag = true; static Object lock = new Object(); public static void main(String[] args) throws Exception &#123; Thread waitThread = new Thread(new Wait(), \"WaitThread\"); waitThread.start(); TimeUnit.SECONDS.sleep(1); Thread notifyThread = new Thread(new Notify(), \"NotifyThread\"); notifyThread.start(); &#125; static class Wait implements Runnable &#123; @Override public void run() &#123; //加锁，拥有lock的monitor synchronized (lock) &#123; //当条件不满足时，继续wait，同时释放了lock的锁 while (flag) &#123; try &#123; System.out.println(Thread.currentThread() + \" flag is true. wait @ \" + LocalTime.now()); lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //条件满足时，完成工作 System.out.println(Thread.currentThread() + \" flag is false. running @ \" + LocalTime.now()); &#125; &#125; &#125; static class Notify implements Runnable &#123; @Override public void run() &#123; //加锁，拥有lock的monitor synchronized (lock) &#123; //获取lock的锁，然后进行通知，通知时不会释放lock的锁 //直到当前线程释放了lock后，waitThread才能从wait方法中返回 System.out.println(Thread.currentThread() + \" hold lock. notify @ \" + LocalTime.now()); lock.notifyAll(); flag = false; SleepUtils.second(5); &#125; //再次加锁 synchronized (lock) &#123; System.out.println(Thread.currentThread() + \" hold lock again. sleep @ \" + LocalTime.now()); SleepUtils.second(5); &#125; &#125; &#125;&#125; 管道输入/输出流​ 管道输入/输出流和普通的文件输入/输出流或者网络输入/输出流不同之处在于，它主要用于线程之间的数据传输，而传输的媒介为内存。 ​ 管道输入/输出流主要包括了4种具体实现:PipedOutputStream、PipedInputStream、PipedReader和PipedWriter，前两种面向字节，后两种面向字符。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 管道输入/输出流 * */public class Piped &#123; public static void main(String[] args) throws Exception &#123; PipedWriter out = new PipedWriter(); PipedReader in = new PipedReader(); //将输出流和输入流进行连接，否则在使用时会抛出IOException out.connect(in); Thread printThread = new Thread(new Print(in), \"PrintThread\"); printThread.start(); int receive = 0; try &#123; while ((receive = System.in.read()) != -1) &#123; out.write(receive); &#125; &#125; finally &#123; out.close(); &#125; &#125; static class Print implements Runnable &#123; private PipedReader in; public Print(PipedReader in) &#123; this.in = in; &#125; @Override public void run() &#123; int receive = 0; try &#123; while ((receive = in.read()) != -1) &#123; System.out.print((char) receive); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; ​ 对于Piped类型的流，必须先要进行绑定，也就是调用connec()方法，如果没有将输入/输出流绑定起来，对于该流的访问将会抛出异常。 Thread.join()的使用​ 如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后从thread.join()返回。线程Thread除了提供join()方法之外，还提供了join(long millis)和join(long millis,int nanos)两个具备超时特性的方法。这两个超时方法表示，如果线程thread在给定的超时时间里没有终止，那么将会从该超时方法中返回。 123456789101112131415161718192021222324252627282930313233343536/** * join方法 * */public class Join &#123; public static void main(String[] args) throws Exception &#123; Thread previous = Thread.currentThread(); for (int i = 0; i &lt; 10; i++) &#123; Thread thread = new Thread(new Domino(previous), String.valueOf(i)); thread.start(); previous = thread; &#125; TimeUnit.SECONDS.sleep(5); System.out.println(Thread.currentThread().getName() + \" terminate.\"); &#125; static class Domino implements Runnable &#123; private Thread thread; public Domino(Thread thread) &#123; this.thread = thread; &#125; @Override public void run() &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \" terminate.\"); &#125; &#125;&#125; ThreadLocal的使用​ ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。可以通过set(T)方法来设置一个值，在当前线程下再通过get()方法获取到原先的值。 123456789101112131415161718192021222324252627/** * ThreadLocal使用 * */public class Profiler &#123; //第一次get()方法调用时会进行初始化(如果set方法没有调用)，每个线程会调用一次 private static final ThreadLocal&lt;Long&gt; TIME_THREADLOCAL = new ThreadLocal&lt;&gt;() &#123; @Override protected Long initialValue() &#123; return System.currentTimeMillis(); &#125; &#125;; public static final void begin() &#123; TIME_THREADLOCAL.set(System.currentTimeMillis()); &#125; public static final long end() &#123; return System.currentTimeMillis() - TIME_THREADLOCAL.get(); &#125; public static void main(String[] args) throws Exception &#123; Profiler.begin(); TimeUnit.SECONDS.sleep(1); System.out.println(\"Cost: \" + Profiler.end() + \" mills\"); &#125;&#125; ​ Profiler可以被复用在方法调用耗时统计的功能上，在方法的入口前执行begin()方法，在方法调用后执行end()方法，好处是两个方法的调用不用在一个方法或者类中。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(二)Java内存模型","slug":"Java内存模型","date":"2019-03-22T07:11:55.000Z","updated":"2019-09-19T10:58:43.667Z","comments":true,"path":"2019/03/22/Java内存模型/","link":"","permalink":"/2019/03/22/Java内存模型/","excerpt":"","text":"Java内存模型的基础Java内存模型的抽象结构​ 在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享。局部变量，方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 ​ Java线程之间的通信由Java内存模型(JMM)控制，JMM定义了一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 从源代码到指令序列的重排序 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作上看上去可能是在乱序执行。 顺序一致性顺序一致性内存模型两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 不管程序是否同步，所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 未同步程序的执行顺序​ JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。 ​ 未同步程序在JMM的执行时，整体上是无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有如下几个差异。 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序。 JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。 volatile的内存语义volatile的特性 可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种符合操作不具有原子性。 volatile写-读的内存语义写的语义如下：当写入一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 读的语义如下：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 volatile内存语义的实现 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 锁的内存含义锁的释放和获取的内存含义 当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。 锁内存语义的实现12345678910111213141516171819202122232425public class ReentrantLockExample &#123; int a = 0; ReentrantLock lock = new ReentrantLock(); public void writer()&#123; //获取锁 lock.lock(); try &#123; a++; &#125; finally &#123; //释放锁 lock.unlock(); &#125; &#125; public void reader()&#123; //获取锁 lock.lock(); try &#123; int i = a; //方法 &#125; finally &#123; //释放锁 lock.unlock(); &#125; &#125;&#125; ​ 在ReentrantLock中，调用lock()方法获取锁；调用unlock()方法释放锁。 ​ ReentrantLock的实现依赖于Java同步器框架AbstractQueuedSynchronizer。AQS使用一个整型的volatile变量(命名为state)来维护同步状态。 公平锁和非公平锁释放时，最后都要写一个volatile变量state。 公平锁获取时，首先会去读volatile变量。 非公平锁获取时，首先会用CAS更新volatile变量，这个操作同时具有volatile读和volatile写的内存语义。 final域的内存含义final域的重排序规则 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，让这两个操作之间不能重排序。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 写final域的重排序规则​ 写final域的重排序规则禁止把final域的写重排序到构造函数之外。这个规则的实现包含下面2个方面。 JMM禁止编译器把final域的写重排序到构造函数之外。 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外。 ​ 写final域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了，而普通域不具有这个保障。 读final域的重排序规则​ 读final域的重排序规则是，在一个线程中，初次读对象引用与初次读该对象包含的final域，JMM禁止处理器重排序这两个操作(仅针对处理器)。编译器会在读final域操作的前面插入一个LoadLoad屏障。 ​ 读final域的重排序规则可以确保：在读一个对象的final域之前，一定会读包含这个final域的对象的引用。 happens-beforehappens-before的定义 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这么重排序并不非法(JMM允许这种重排序)。 happnes-before规则happens-before规则如下： 程序员顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作 监视器规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()(启动线程)，那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从Thread.join()操作成功返回。 类初始化 通过在Class对象上同步(即获取Class对象的初始化锁)，来控制类或接口的初始化。这个获取锁的线程会一直等待，直到当前线程能够获取到这个初始化锁。 线程A执行类的初始化，同时线程B在初始化锁对应的Condition上等待。 线程A设置state=initialized，然后唤醒在condition中等待的所有线程。 线程B结束类的初始化处理。 线程C执行类的初始化的处理。 JMM的内存可见性保证 单线程程序。单线程程序不会出现内存可见性问题。编译器、runtime和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。 正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性(程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同)。这时JMM关注的重点，JMM通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。 未同步/未正确同步的多线程程序。JMM为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值(0、null、false)。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(一)Java并发机制的底层实现原理","slug":"Java并发机制的底层实现原理","date":"2019-03-19T01:40:17.000Z","updated":"2019-09-19T10:58:31.778Z","comments":true,"path":"2019/03/19/Java并发机制的底层实现原理/","link":"","permalink":"/2019/03/19/Java并发机制的底层实现原理/","excerpt":"","text":"volatile定义​ Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 实现原则1）Lock前缀指令会引起处理器缓存回写到内存。 2）一个处理器的缓存回写到内存会导致其他处理器的缓存无效。 Synchronized利用Synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现在以下三种形式。 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchronized括号里配置的对象。 Java对象头​ synchronized用的锁是存在Java对象头里的。如果对象是数组类型，则虚拟机用3个自宽(Word)存储对象头，如果对象是非数组类型，则用2自宽存储。在32位虚拟机中，1字宽澄宇4字节，即32bit。 ​ Java对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位。在运行期间，Mark Word里存储的数据会随着锁标记位的变化而变化。 锁的升级与对比​ 锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。 偏向锁​ 大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当一个线程访问同步块获取锁时，会在对象头和栈桢中的锁记录里存储锁偏向的线程ID，以后该进程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。 偏向锁的撤销​ 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁状态或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 关闭偏向锁​ 默认为启用状态，在应用程序启动几秒钟后才激活，可使用JVM参数来关闭延迟:-XX:BiasedLockingStartupDelay=0。确定如果程序里所有的锁通常情况下处于竞争状态，可通过JVM参数关闭偏向锁:-XX:-UseBiasedLocking=false，那么程序默认会进入轻量级锁状态。 轻量级锁轻量级锁加锁​ 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 轻量级锁解锁​ 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前存在锁竞争，锁就会膨胀成重量级锁。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"设计模式","slug":"设计模式","date":"2019-03-03T07:20:28.000Z","updated":"2019-05-26T06:54:22.612Z","comments":true,"path":"2019/03/03/设计模式/","link":"","permalink":"/2019/03/03/设计模式/","excerpt":"","text":"观察者模式定义观察者模式定义了对象之间的的一对多依赖，这样一来，当一个对象改变状态时，它的所有依赖者都会收到通知并自动更新。 Java内置的观察者模式把对象编程观察者实现观察者接口（java.util.Observer），然后调用任何Observable对象的addObserver()方法。不想再当观察者时，调用deleteObserver()方法。 可观察者送出通知首先利用java.util.Observable接口产生“可观察者”类，然后，需要两个步骤： 先调用setChanged()方法，标记状态已经改变的事实。 然后调用两种notifyObservers()方法中的一个：notifyObservers()或notifyObservers(Object arg) 观察者接收通知实现update(Observable o, Object arg)。主题本身当作第一个变量，好让观察者知道是哪个主题通知它的。第二个参数是传入notifyObservers()的数据对象，如果没有则为空。 装饰者模式定义装饰者模式动态地将责任附加到对象上。若要扩展功能，装饰者提供了比继承更有弹性的替代方案。 装饰者和被装饰对象有相同的超类型 你可以用一个或多个装饰者包装一个对象 既然装饰者和被装饰对象有相同的超类型，所以在任何需要原始对象（被包装的）的场合，可以用装饰过的对象代替它 装饰者可以在所委托被装饰者的行为之前与/或之后，加上自己的行为，以达到特定的目的 对象可以在任何时候被装饰，所以可以在运行时动态地、不限量地用你喜欢的装饰者来装饰对象 开放-关闭原则类应该对扩展开放，对修改关闭 工厂模式定义工厂方法模式定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法让类把实例化推迟到子类。 依赖倒置原则要依赖抽象，不要依赖具体类。 抽象工厂模式抽象工厂模式提供一个接口，用于创建相关或依赖对象的家族，而不需要明确指定具体类。 要点 所有的工厂都是用来封装对象的创建。 简单工厂，虽然不是真正的设计模式，但仍不失为一个简单的方法，可以将客户程序从具体类解耦。 工厂方式使用继承：把对象的创建委托给子类，子类实现工厂方法来创建对象。 抽象工厂使用对象组合：对象的创建被实现在工厂接口所暴露出来的方法中。 所有工厂模式都通过减少应用程序和具体类之间的依赖促进松耦合。 工厂方法允许类将实例化延迟到子类进行。 抽象工厂创建相关的对象家族，而不需要依赖它们的具体类。 单例模式定义单例模式确保一个类只有一个实例，并提供一个全局访问点。 使用静态内部类实现的单例模式 12345678910111213public class Singleton &#123; private static class LazyHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton() &#123; &#125; public Singleton getSingleton() &#123; return LazyHolder.INSTANCE; &#125;&#125; 命令模式定义命令模式将”请求”封装成对象，以便使用不同的请求、队列或者日志来参数化其他对象。命令模式也支持可撤销的操作。 要点 命令模式将发出请求的对象和执行请求的对象解耦。 在被解耦的两者之间是通过命令对象进行沟通的。命令对象封装了接收者的动作被调用。 调用者可以接受命令当作参数，甚至在运行时动态地进行。 命令可以支持撤销，做法是实现一个undo()方法来回到execute()被执行前的状态。 宏命令是命令的一种简单的眼神，允许调用多个命令。宏方法也可以支持撤销。 命令也可以用来实现日志和事务系统。 适配器与外观模式定义适配器模式将一个类的接口，转换成客户期望的另一个接口。适配器让原本接口不兼容的类可以合作无间。 外观模式定义外观模式提供了一个统一的接口，用来访问子系统中的一群接口。外观定义了一个高层接口，让子系统更容易使用。 要点 当需要使用一个现有的类而其接口并不符合需要时，就使用适配器。 当需要简化并统一一个很大的接口或者一群复杂的接口时，使用外观。 适配器改变接口以符合客户的期望。 外观将客户从一个复杂的子系统中解耦。 适配器模式由两种形式：对象适配器和类适配器。类适配器需要用到多重继承。 适配器将一个对象包装起来以改变其接口；装饰者将一个对象包装起来以增加新的行为和责任；而外观将一群对象”包装”起来以简化其接口。 模版方法模式定义模版方法模式在一个方法中定义一个算法的骨架，而将一些步骤延迟到子类中。模版方式使得子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤。 要点 “模版方法”定义了算法的步骤，把这些步骤的实现延迟到子类。 模版方法模式提供了一种代码复用的重要技巧。 模版方法的抽象类可以定义具体方法、抽象方法和钩子。 抽象方法由子类实现。 钩子是一种方法，它在抽象类中不做事，或者只做默认的事情，子类可以选择要不要去覆盖它。 为了防止子类改变模版方法中的算法，可以讲模版方法声明为final。 策略模式和模版方法模式都封装算法，一个用组合，一个用继承。 工厂方法是模版方法的一个特殊版本。 迭代器与组合模式定义迭代器模式提供一种方法顺序访问一个聚合对象中的各个元素，而不暴露其内部的表示。 组合模式定义组合模式允许将对象组合成树形结构来表现”整体/部分”层次结构。组合能让客户以一致的方式处理个别对象以及对象组合。 状态模式定义状态模式允许对象在哪部状态改变时改变它的行为，对象看起来好像修改了它的类。 代理模式定义代理模式为另一个对象提供了一个替身或占位符以控制对这个对象的访问。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"/categories/设计模式/"}],"tags":[]},{"title":"Zuul","slug":"Zuul","date":"2019-02-14T13:25:44.000Z","updated":"2019-02-14T14:58:24.545Z","comments":true,"path":"2019/02/14/Zuul/","link":"","permalink":"/2019/02/14/Zuul/","excerpt":"","text":"为什么需要ZuulZuul作为路由网关组件体现在以下6个方面： Zuul、Ribbon以及Eureak相结合，可以实现智能路由和负载均衡的功能，Zuul能够将请求流量按某种策略分发到集群状态的多个服务实例。 网关将所有服务的API接口统一聚合，并统一对外暴露。 网关服务可以做用户身份认证和权限认证，防止非法请求操作API接口，对服务器起到保护作用。 网关可以实现监控功能，实时日志输出，对请求进行记录。 网关可以用来实现流量监控，在高流量的情况下，对服务进行降级。 API接口从内部服务分离出来，方便做测试。 Zuul的工作原理Zuul是通过Servlet实现的，Zuul通过自定义的ZuulServlet（类似于Spring MVC的DispatchServlet）来对请求进行控制。Zuul的核心是一系列过滤器，可以在Http请求的发起和响应返回期间执行一系列的过滤器。包括以下4种过滤器。 PRE过滤器：它是在请求路由到具体的服务之前执行的，这种类型的过滤器可以做安全验证，例如身份验证、参数验证等。 ROUTING过滤器：它用于将请求路由到具体的微服务实例。默认情况下使用Http Client进行网络请求。 POST过滤器：它是在请求已被路由到微服务后执行的。一般情况下，用作收集统计信息、指标，以及将响应传输到客户端。 ERROR过滤器：它是在其他过滤发生错误时执行的。 搭建Zuul服务引入Eureka Client的起步依赖spring-cloud-starter-eureka、Zuul的起步依赖spring-cloud-starter-zuul、Web功能的起步依赖spring-boot-starter-web以及Spring Boot测试的起步依赖spring-boot-starter-test，pom文件如下 1234567891011121314151617181920&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在程序的启动类EurekaZuulClientApplication加上@EnableEurekaClient注解，开启EurekaClient的功能；加上@EnableZuulProxy注解，开启Zuul的功能。 12345678910@EnableZuulProxy@EnableEurekaClient@SpringBootApplicationpublic class EurekaZuulClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaZuulClientApplication.class, args); &#125;&#125; 在application.yml中配置，配置注册中心eureka-client，程序端口号5000，程序名为servicce-zuul，zuul.routes.hiapi.path为“/hiapi/**”，zuul.routes.hiapi.serviceId为“eureka-client”，这两个配置就可以将以”/hiapi”开头的Url路由到eureka-client服务。 1234567891011121314151617181920eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/server: port: 5000spring: application: name: service-zuulzuul: routes: hiapi: path: /hiapi/** serviceId: eureka-client ribbonapi: path: /rubbibapi/** serviceId: eureka-ribbon-client feignapi: path: /feignapi/** serviceId: eureka-feign-client 在Zuul上配置API接口的版本号，只需在application.yml中增加 1zuul.prefix: /v1 在Zuul上配置熔断器，需实现ZuulFallbackProvider接口。实现该接口有两个方法，一个是getRoute()方法，用于指定熔断功能应用于哪些路由的服务；另一个方法fallbackResponse()为进入熔断功能时执行的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Componentpublic class MyFallbackProvider implements ZuulFallbackProvider &#123; @Override public String getRoute()&#123; return \"eureka-client\"; &#125; @Override public ClientHttpResponse fallbackResponse() &#123; return new ClientHttpResponse() &#123; @Override public HttpStatus getStatusCode() throws IOException &#123; return HttpStatus.OK; &#125; @Override public int getRawStatusCode() throws IOException &#123; return 200; &#125; @Override public String getStatusText() throws IOException &#123; return \"OK\"; &#125; @Override public void close() &#123; &#125; @Override public InputStream getBody() throws IOException &#123; return new ByteArrayInputStream(\"oooops!error, i'm the fallback.\".getBytes()); &#125; @Override public HttpHeaders getHeaders() &#123; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); return headers; &#125; &#125;; &#125;&#125; 在Zuul上使用过滤器，需要继承ZuulFilter，并实现其中的抽象方法，包括filterType()和filterOrder()以及IZuulFilter的shouldFilter()和object run()的两个方法。其中，filterType()即过滤器的类型，分别是“pre””post””routing”和”error”。filterOrder()是过滤顺序，它为一个Int类型的值，值越小，越早执行该过滤器。shouldFilter()表示该过滤器是否过滤逻辑，如果为true，则执行run()方法；false为不执行。run()方法写具体的过滤的逻辑。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class MyFilter extends ZuulFilter &#123; private static Logger log = LoggerFactory.getLogger(MyFilter.class); @Override public String filterType() &#123; return PRE_TYPE; &#125; @Override public int filterOrder() &#123; return 0; &#125; @Override public boolean shouldFilter() &#123; return true; &#125; @Override public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); Object accessToken = request.getParameter(\"token\"); if(accessToken == null)&#123; log.warn(\"token is empty\"); ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(401); try&#123; ctx.getResponse().getWriter().write(\"token is empty\"); &#125;catch (Exception e)&#123; return null; &#125; &#125; log.info(\"ok\"); return null; &#125;&#125;","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Hystrix","slug":"Hystrix","date":"2019-01-09T13:46:58.000Z","updated":"2019-01-09T15:37:18.047Z","comments":true,"path":"2019/01/09/Hystrix/","link":"","permalink":"/2019/01/09/Hystrix/","excerpt":"","text":"HystrixHystrix是Netflix公司开源的一个项目，它提供了熔断器功能，能够阻止分布式系统中出现联动故障。Hystrix是通过隔离服务的访问点阻止联动故障的，并提供了故障的解决方法，从而提高了整个分布式系统的弹性。 Hystrix的设计原则 防止单个服务的故障耗尽整个服务的Servlet容器(如Tomcat)的线程资源。 快速失败机制，如果某个服务出现了故障，则调用该服务的请求快速失败，而不是线程等待。 提供回退方案，在请求发生故障时，提供设定好的回退方案。 使用熔断机制，防止故障扩散到其他服务。 提供熔断器的监控组件Hystrix Dashboard，可以实时监控熔断器的状态。 Hystrix的工作机制当服务的某个API接口的失败次数在一定时间内小于设定的阀值时，熔断器处于关闭状态，该API接口正常提供服务。当该API接口处理请求的失败次数大于阀值时，Hystrix判断该API接口出现了故障，打开熔断器，这时请求该API接口会执行快速失败的逻辑(即fallback回退的逻辑)，不执行业务逻辑，请求的线程不会处于阻塞状态。处于打开状态的熔断器，一段时间后会处于半打开状态，并将一定数量的请求执行正常逻辑。剩余的请求会执行快速失败，若执行正常逻辑的请求失败了，则熔断器继续打开；若成功了，则将熔断器关闭。 在RestTemplate和Ribbon上使用熔断器在pom文件中引入Hystrix的起步依赖spring-cloud-starter-hystrix 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 在启动类加上@EnableHystrix注解开启Hystrix的熔断器功能 12345678910@SpringBootApplication@EnableEurekaClient@EnableHystrixpublic class EurekaRibbonClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaRibbonClientApplication.class, args); &#125;&#125; 修改RibbonService，在hi()方法上加@HystrixCommand注解。有了该注解，hi()方法就启用了Hystrix熔断器的功能。fallbackMethod为处理回退逻辑的方法。在熔断器打开的状态下，会执行fallback逻辑。fallback的逻辑最好是返回一些静态的字符串，不需要处理复杂的逻辑，也不需要远程调度其他服务，这样方便执行快速失败，释放线程资源。如果一定要在fallback逻辑中远程调度其他服务，最好在远程调度其他服务时，也加上熔断器。 1234567891011121314@Servicepublic class RibbonService &#123; @Autowired RestTemplate restTemplate; @HystrixCommand(fallbackMethod = \"hiError\") public String hi(String name)&#123; return restTemplate.getForObject(\"http://eureka-client/hi?name=\"+name, String.class); &#125; public String hiError(String name)&#123; return \"hi,\" + name + \",sorry,sorry!\"; &#125;&#125; 在Feign上使用熔断器在Feign的起步依赖已经引入了Hystrix的依赖，只需要在application.yml配置开启Hystrix的功能。 123feign: hystrix: enabled: true 修改EurekaClientFeign代码，在@FeignClient注解的fallback配置加上快速失败的处理类。 12345@FeignClient(value = \"eureka-client\", configuration = FeignConfig.class, fallback = HyStrix.class)public interface EurekaClientFeign &#123; @GetMapping(value = \"/hi\") String sayHiFromClientEureka(@RequestParam(value = \"name\") String name);&#125; Hystrix类需要实现EurekaCLientFeign接口，并写具体逻辑，以及加上@Component注解，注入IoC容器中。 1234567@Componentpublic class HyStrix implements EurekaClientFeign&#123; @Override public String sayHiFromClientEureka(String name) &#123; return \"hi,\" + name + \",sorry,sorry!\"; &#125;&#125; 使用Hystrix Dashboard监控熔断器的状态在pom文件中加上Actuator、Hystrix Dashboard和Hystrix的起步依赖。 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; 在程序的启动类加上@EnableHystrixDashboard注解。 1234567891011@SpringBootApplication@EnableEurekaClient@EnableHystrix@EnableHystrixDashboardpublic class EurekaRibbonClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaRibbonClientApplication.class, args); &#125;&#125; 使用Turbine聚合监控添加依赖 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-turbine&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 1234567891011121314spring: application.name: service-turbineserver: port: 8769security.basic.enabled: falseturbine: aggregator: clusterConfig: default appConfig: eureka-ribbon-client,eureka-feign-client clusterNameExpression: new String(&quot;default&quot;)eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ turbine.appConfig ：配置Eureka中的serviceId列表，表明监控哪些服务 turbine.aggregator.clusterConfig ：指定聚合哪些集群，多个使用”,”分割，默认为default。可使用http://.../turbine.stream?cluster={clusterConfig之一}访问 turbine.clusterNameExpression ： 1. clusterNameExpression指定集群名称，默认表达式appName；此时：turbine.aggregator.clusterConfig需要配置想要监控的应用名称；2. 当clusterNameExpression: default时，turbine.aggregator.clusterConfig可以不写，因为默认就是default；3. 当clusterNameExpression: metadata[‘cluster’]时，假设想要监控的应用配置了eureka.instance.metadata-map.cluster: ABC，则需要配置，同时turbine.aggregator.clusterConfig: ABC 启动类添加@EnableTurbine，激活对Turbine的支持 123456789@SpringBootApplication@EnableTurbinepublic class EurekaMonitorClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaMonitorClientApplication.class, args); &#125;&#125;","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Feign","slug":"Feign","date":"2019-01-08T12:42:41.000Z","updated":"2019-01-09T13:47:14.516Z","comments":true,"path":"2019/01/08/Feign/","link":"","permalink":"/2019/01/08/Feign/","excerpt":"","text":"编写Feign客户端增加Feign的起步依赖spring-cloud-starter-feig、Eureka Client的起步依赖spring-cloud-starter-eureka、Web功能的起步依赖spring-boot-starter-web，pom文件如下： 12345678910111213141516171819&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.netflix.feign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置application.yml。 123456789server: port: 8765spring: application: name: eureka-feign-clienteureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 在程序启动类EurekFeignClientApplication加上注解@EnableEurekaClient开启Eureka Client的功能，通过注解@EnableFeignClients开启Feign Client的功能。 12345678910@SpringBootApplication@EnableEurekaClient@EnableFeignClientspublic class EurekaFeignClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaFeignClientApplication.class, args); &#125;&#125; 通过以上3个步骤，该程序就具备了Feign的功能。 新建一个EurekaClientFeign的接口，在接口加上@FeignClient来声明一个Feign Client，其中value为远程调用其他服务的服务名，FeignConfig.class为Feign Client的配置类。在接口内部有一个sayHiFromClientEureka()方法，该方法通过Feign来调用eureka-client服务的”/hi”的API接口。 12345@FeignClient(value = \"eureka-client\", configuration = FeignConfig.class)public interface EurekaClientFeign &#123; @GetMapping(value = \"/hi\") String sayHiFromClientEureka(@RequestParam(value = \"name\") String name);&#125; 在FeignConfig类加上@Configuration注解，表明该类是一个配置类，并注入一个BeanName为feignRetryer的Retryer的Bean。注入该Bean之后，Feign在远程调用失败后会进行重试。 123456789import static java.util.concurrent.TimeUnit.SECONDS;@Configurationpublic class FeignConfig &#123; @Bean public Retryer feignRetryer()&#123; return new Retryer.Default(100, SECONDS.toMillis(1), 5); &#125;&#125; 在service层注入EurekaClientFeign的Bean。 12345678910@Servicepublic class HiService &#123; @Autowired EurekaClientFeign eurekaClientFeign; public String sayHi(String name)&#123; return eurekaClientFeign.sayHiFromClientEureka(name); &#125;&#125; 在HiController层注入并调用sayHi()方法。 1234567891011@RestControllerpublic class HiController &#123; @Autowired HiService hiService; @GetMapping(\"/hi\") public String sayHi(@RequestParam(defaultValue = \"zhangsan\", required = false) String name)&#123; return hiService.sayHi(name); &#125;&#125; 访问http://localhost:8765/hi 1hi zhangsan, i am from port 8762 Feign源码实现过程1.首先通过@EnableFeignClients注解开启FeignClient的功能。只有这个注解存在，才会在程序启动时开启对@FeignClient注解的包扫描。 2.根据Feign的规则实现接口，并在接口上面加上@FeignClient注解。 3.程序启动后，会进行包扫描，扫描所有的@FeignClient的注解的类，并将这些信息注入IoC容器中。 4.当接口被调用时，通过JDK的代理来生成具体的RequestTemplate模版对象。 5.根据RequestTemplate再生产Http请求的Request对象。 6.Request对象交给Client去处理，其中Client的网络请求框架可以是HttpURLConnection、HttpClient、OkHttp。 7.最后Client被封装到LoadBalanceClient类，这个类结合类Ribbon做到了负载均衡。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Ribbon","slug":"Ribbon","date":"2019-01-07T13:20:18.000Z","updated":"2019-01-08T12:41:06.141Z","comments":true,"path":"2019/01/07/Ribbon/","link":"","permalink":"/2019/01/07/Ribbon/","excerpt":"","text":"RestTemplateRestTemplate是Spring Resources中一个访问第三方RESTful API接口的网络请求框架。RestTemplate是用来消费REST服务的，所以RestTemplate的主要方法都与REST的Http协议的一些方法紧密相连，例如HEAD、GET、POST、DELETE和OPTIONS等方法，这些方法在RestTemplate类对应的方法为headForHeaders()、getForObject()、postForObject()、put()和delete()等。以下代码可以将返回的JSON字符串转换成一个User对象。 1User user = restTemplate.getForObject(\"https://www.xxx.com/\", User.class); Ribbon负载均衡是指将负载分摊到多个执行单元上，常见的负载均衡有两种方式。一种是独立进程单元，通过负责均衡策略，将请求转发到不同的执行单元上，例如Ngnix。另一种是将负载均衡逻辑以代码的形式封装到服务消费者的客户的，服务消费者客户端维护了一份服务提供者的信息列表。有了信息列表，通过负载均衡策略将请求分摊给多个服务提供者，从而达到负载均衡的目的。 目前NetFlix用于生产环境的Ribbon子模块： ribbon-loadbalancer：可以独立使用或与其他模块一起使用的负载均衡器API。 ribbon-eureka：Ribbon结合Eureka客户端的API，为负载均衡器提供动态服务注册列表。 ribbon-core：Ribbon的核心API。 使用RestTemplate和Ribbon来消费服务引入Eureka Client的起步依赖spring-cloud-starter-eureka、Ribbon的起步依赖spring-cloud-starter-ribbon，以及Web的起步依赖spring-boot-starter-web。 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 配置application.yml，指定程序名、端口号、服务注册地址。 123456789spring: application: name: eureka-ribbon-clientserver: port: 8764eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 作为Eureka Client需要在程序的入口类加上注解@EnableEurekaClient开启Eureka Client功能。 123456789@SpringBootApplication@EnableEurekaClientpublic class EurekaRibbonClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaRibbonClientApplication.class, args); &#125;&#125; 写一个RESTful API接口，在该API接口内部需要调用eureka-client的API接口”/hi”，即服务消费。这时需要将RestTemplate和Ribbon相结合，进行负载均衡。 需要在程序的IoC容器中注入一个restTemplate的Bean，并在这个Bean上加上@LoadBalanced注解，此时RestTemplate就结合了Ribbon开启了负载均衡功能。 12345678@Configurationpublic class RibbonConfig &#123; @Bean @LoadBalanced RestTemplate restTemplate()&#123; return new RestTemplate(); &#125;&#125; 写一个RibbonService类，在该类的hi()方法用restTemplate调用eureka-client的API接口，此时Uri不需要使用硬编码(如ip)，只需要写服务名即可。 123456789@Servicepublic class RibbonService &#123; @Autowired RestTemplate restTemplate; public String hi(String name)&#123; return restTemplate.getForObject(\"http://eureka-client/hi?name=\"+name, String.class); &#125;&#125; 写RibbonController，调用servcie的方法。 123456789101112@RestControllerpublic class RibbonController &#123; @Autowired RibbonService ribbonService; @GetMapping(\"/hi\") public String hi(@RequestParam(required = false, defaultValue = \"zhangsan\") String name)&#123; return ribbonService.hi(name); &#125;&#125; LoadBalanceClient负载均衡器的核心类为LoadBalanceClient，LoadBalanceClient可以获取负载均衡的服务提供者的实例信息。 获取Eureka Client的实例信息时： 123456789101112@RestControllerpublic class RibbonController &#123; @Autowired private LoadBalancerClient loadBalancerClient; @GetMapping(\"/testRibbon\") public String testRibbon()&#123; ServiceInstance instance = loadBalancerClient.choose(\"eureka-client\"); return instance.getHost() + \":\" + instance.getPort(); &#125;&#125; 不从Eureka Client获取注册列表信息时： 在启动类中增加@SpringBootApplication注解。 配置文件application.yml，配置ribbon.eureka.enable为false来禁止调用Eureka Client获取注册列表。配置程序名为stores的服务，通过stores.ribbon.listOfServers来配置这些服务实例的Url。 12345678stores: ribbon: listOfServers: example.com,google.comribbon: eureka: enabled: falseserver: port: 8769 新建RestController类。 123456789101112@RestControllerpublic class RibbonController &#123; @Autowired private LoadBalancerClient loadBalancerClient; @GetMapping(\"/testRibbon\") public String testRibbon()&#123; ServiceInstance instance = loadBalancerClient.choose(\"stores\"); return instance.getHost() + \":\" + instance.getPort(); &#125;&#125; 在浏览器上多次访问http://localhost:8769/testRibbon，浏览器会交替出现以下内容： 12example.com:80google.com:80 在Ribbon中的负载均衡客户端为LoadBalancerClient，在Spring Cloud项目中，负载均衡器Ribbon会默认从Eureka Client的服务注册列表中获取服务的信息，并缓存一份。如果禁止从Eureka获取注册列表信息，则需要自己维护一份服务注册列表信息。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Eureka","slug":"Eureka","date":"2019-01-05T13:13:26.000Z","updated":"2019-10-24T03:22:58.726Z","comments":true,"path":"2019/01/05/Eureka/","link":"","permalink":"/2019/01/05/Eureka/","excerpt":"","text":"Eureka简介什么是EurekaEureka是一个用于服务注册和发现的组件。Eureka分为Eureka Server和Eureka Client，Eureka Server为Eureka服务注册中心，Eureka Client为Eureka客户端。 Eureka的基本架构主要包括以下3种角色 Register Service:：服务注册中心，它是一个Eureka Server，提供服务注册和发现的功能 Provide Service：服务提供者，它是一个Eureka Client，提供服务 Consumer Service：服务消费者，它是一个Eureka Client，消费服务 Eureka Server在pom文件中引入Eureka Server依赖 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在application.yml中做配置。通过server.port制定Eureka Server的端口为8761。在默认情况下，Eureka Server 会向自己注册，这时需要配置eureka.client.registerWithEurkea和eureka.client.fetchRegistry为false，防止自己注册自己。 1234567891011server: port: 8761eureka: instance: hostname: localhost client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 在工程的启动类EurekaServerApplication加上注解@EnableEurekaServer，开启Eureka Server的功能。 123456789@EnableEurekaServer@SpringBootApplicationpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; Eureka Client在pom中引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 在bootstrap.yml做Eureka Client客户端的相关配置，配置了程序名为eureka-client，程序端口为8762，服务注册地址为http://localhost:8761/eureka/ 123456789server: port: 8762spring: application: name: eureka-clienteureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka 在程序的启动类加上@EnableEurekaClient开启Eureka Client功能 123456789@SpringBootApplication@EnableEurekaClientpublic class EurekaClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaClientApplication.class, args); &#125;&#125; Eureka概念 Register–服务注册 当Eureka Client向Eureka Server注册时，Eureka Client提供自身的元数据，比如IP地址、端口、运行状况指标的Url、主页地址等信息。 Renew–服务续约 Eureka Client在默认的情况下会每隔30秒发送一次心跳来进行服务续约。通过服务续约来告知Eureka Server该Eureka Client仍然可用，没有出现故障。 Fetch Registries–获取服务注册列表信息 Eureka Client从Eureka Server获取服务注册表信息，并将其缓存在本地。 Cancel–服务下线 Eureka Client在程序关闭时可以向Eureka Server发送下线请求。发送请求后，该客户端的实例信息将从Eureka Server的服务注册列表中删除。该下线请求不会自动完成，需要在程序中调用: 1DiscoveryManager.getInstance().shutdownComponent(); Eviction–服务剔除 在默认情况下，当Eureka Client连续90秒没有向Eureka Server发送服务续约(即心跳)时，Eureka Server会将该服务实例从服务注册列表中删除。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Spring Boot","slug":"Spring-Boot","date":"2018-12-17T13:44:39.000Z","updated":"2019-01-05T14:10:39.215Z","comments":true,"path":"2018/12/17/Spring-Boot/","link":"","permalink":"/2018/12/17/Spring-Boot/","excerpt":"","text":"@SpringBootApplication注解@SpringBootApplication开启了Spring的组件扫描和Spring Boot的自动配置功能呢。实际上，它将三个有用的注解组合在了一起。 @Configuration:标明该类使用Spring基于Java的配置。 @ComponentScan:启用组件扫描。 @EnableAutoConfiguration:开启自动配置。","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"/categories/Spring-Boot/"}],"tags":[]}]}