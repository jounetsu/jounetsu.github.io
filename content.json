{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"jounetsu","url":""},"pages":[],"posts":[{"title":"线程安全与锁优化","slug":"线程安全与锁优化","date":"2021-07-12T06:28:04.000Z","updated":"2021-07-13T07:05:12.347Z","comments":true,"path":"2021/07/12/线程安全与锁优化/","link":"","permalink":"/2021/07/12/线程安全与锁优化/","excerpt":"","text":"1. 线程安全​ 定义：当多个线程同时访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方法进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那就称这个对象是线程安全的。 1.1 Java语言中线程安全1.1.1 不可变​ 在Java语言里，不可变（Immutable）的对象一定是线程安全的，无论是对象的方法实现还是方法的调用者，都不需要再进行任何线程安全保障措施。“不可变”带来的安全性是最直接、 最纯粹的。 ​ Java语言中，如果多线程共享的数据是一个基本数据类型，那么只要在定义时使用final关键字修饰它就可以保证它是不可变的。如果共享数据是一个对象，由于Java语言目前暂时还没有提供值类型的支持，那就需要对象自行保证其行为不会对其状态产生任何影响才行。 ​ 保证对象行为不影响自己状态的途径有很多种，最简单的一种就是把对象里面带有状态的变量都声明为final，这样在构造函数结束之后，它就是不可变的。 1.1.2 绝对线程安全​ 绝对的线程安全能够完全满足线程安全的定义，这个定义其实是很严格的，一个类要达到“不管运行时环境如何，调用者都不需要任何额外的同步措施”可能需要付出非常高昂的， 甚至不切实际的代价。在Java API中标注自己是线程安全的类，大多数都不是绝对的线程安全。 1.1.3 相对线程安全​ 相对线程安全就是我们通常意义上所讲的线程安全，它需要保证对这个对象单次的操作是线程安全的，我们在调用的时候不需要进行额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性。 ​ 在Java语言中，大部分声称线程安全的类都属于这种类型，例如Vector、HashTable、Collections的synchronizedCollection()方法包装的集合等。 1.1.4 线程兼容​ 线程兼容是指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用。 1.1.5 线程对立​ 线程对立是指不管调用端是否采取了同步措施，都无法在多线程环境中并发使用代码。由于Java语言天生就支持多线程的特性，线程对立这种排斥多线程的代码是很少出现的，而且通常都是有害的，应当尽量避免。 1.2 线程安全的实现方法1.2.1 互斥同步​ 互斥同步（Mutual Exclusion &amp; Synchronization）是一种最常见也是最主要的并发正确性保障手段。同步是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻只被一条（或者是一些，当使用信号量的时候）线程使用。而互斥是实现同步的一种手段，临界区（Critical Section）、互斥量（Mutex）和信号量（Semaphore）都是常见的互斥实现方式。 ​ 在Java里面，最基本的互斥同步手段就是synchronized关键字，这是一种块结构(Block Structured)的同步语法。synchronized关键字经过Javac编译之后，会在同步块的前后分别形成monitorenter和monitorexit这两个字节码指令。这两个字节码指令都需要一个reference类型的参数来指明要锁定和解锁的对象。如果Java源码中的synchronized明确指定了对象参数，那就以这个对象的引用作为reference；如果没有明确指定，那将根据synchronized修饰的方法类型(如实例方法或类方法)，来决定是取代码所在的对象实例还是取类型对应的Class对象来作为线程要持有的锁。 ​ 根据《Java虚拟机规范》的要求，在执行monitorenter指令时，首先要去尝试获取对象的锁。如果这个对象没被锁定，或者当前线程已经持有了那个对象的锁，就把锁的计数器的值增加一，而在执行monitorexit指令时会将锁计数器的值减一。一旦计数器的值为零，锁随即就被释放了。如果获取对象锁失败，那当前线程就应当被阻塞等待，直到请求锁定的对象被持有它的线程释放为止。 ​ 使用synchorinzed时需要特别注意的： 被synchorinzed修饰的同步快对同一条线程来说是可重入的。这意味着同一线程反复进入同步块也不会出现自己把自己锁死的情况。 被synchronized修饰的同步块在持有锁的线程执行完毕并释放锁之前，会无条件地阻塞后面其他线程的进入。这意味着无法像处理某些数据库中的锁那样，强制已获取锁的线程释放锁；也无法强制正在等待锁的线程中断等待或超时退出。 ​ 重入锁（ReentrantLock）是Lock接口最常见的一种实现，它与synchronized一样是可重入的。在基本用法上，ReentrantLock也与synchronized很相似，只是代码写法上稍有区别。ReentrantLock比synchronized相比增加了一些高级功能，主要有以下三项：等待可中断、可实现公平锁及可以绑定多个条件。 等待可中断：是指当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。可中断特性对处理执行时间非常长的同步块很有帮助。 公平锁：是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；而非公平锁不保证这一点，在锁被释放时，任何一个等待锁的线程都有机会获得锁。synchronized中的锁是非公平的，ReentrantLock在默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁。不过一旦使用了公平锁，将会导致ReentranLock的性能急剧下降，会明显影响吞吐量。 锁绑定多个条件：是指一个ReentrantLock对象可以同时绑定多个Condition对象。在synchronized中，锁对象的wait()或者notifyAll()方法配合可以实现一个隐含的条件，如果要和多余一个的条件关联的时候，就不得不额外添加一个锁；而ReentrantLock则无须这样做，多次调用newCondition()方法即可。 ​ Lock应该确保在finally块中释放锁，否则一旦受保护的代码块中抛出异常，则有可能永远不会释放持有的锁。而使用synchronized的话则可以由Java虚拟机来确保即使出现异常，锁也能被自动释放。 1.2.2 非阻塞同步​ 互斥同步面临的主要问题是进行线程阻塞和唤醒所带来的性能开销，因此这种同步也被称为阻塞同步(Blocking Synchronization)。从解决问题的方式上看，互斥同步属于一种悲观的并发策略，其总是认为只要不去做正确的同步措施(例如加锁)，那就肯定会出现问题，无论共享的数据是否真的会出现竞争，它都会进行加锁。 ​ 基于冲突检测的乐观并发策略，通俗地说就是不管风险，先进行操作，如果没有其他线程争用共享数据，那操作就直接成功了；如果共享的数据的确被争用，产生了冲突，那再进行其他的补偿措施，最常用的补偿措施是不断地重试，直到出现没有竞争的共享数据为止。这种乐观并发策略的实现不再需要把线程阻塞挂起，因此这种同步操作被称为非阻塞同步(Non-Blocking Synchronization)，使用这种措施的代码也常被称为无锁(Lock-Free) 编程。 ​ 硬件保证某些从语义上看起来需要多次操作的行为可以只通过一 条处理器指令就能完成，这类指令常用的有： 测试并设置(Test-and-Set)； 获取并增加(Fetch-and-Increment)； 交换(Swap)； 比较并交换(Compare-and-Swap，CAS)； 加载链接/条件储存(Load-Linked/Store-Condit ional，LL/SC)。 ​ 在IA64、x86指令集中有用cmpxchg指令完成 的CAS功能，在SPARC-TSO中也有用casa指令实现的，而在ARM和PowerPC架构下，则需要使用一对Idres/strex指令来完成LL/SC的功能。 ​ CAS指令需要有三个操作数，分别是内存位置(在Java中可以简单地理解为变量的内存地址，用V 表示)、旧的预期值(用A表示)和准备设置的新值(用B表示)。CAS指令执行时，当且仅当V符合 A时，处理器才会用B更新V的值，否则它就不执行更新。但是，不管是否更新了V的值，都会返回V的旧值，上述的处理过程是一个原子操作，执行期间不会被其他线程中断。 ​ CAS存在一个逻辑漏洞:如果一个变量V初次读取的时候是A 值，并且在准备赋值的时候检查到它仍然为A值，那就能说明它的值没有被其他线程改变过了吗?这是不能的，因为如果在这段期间它的值曾经被改成B，后来又被改回为A，那CAS操作就会误认为它从来没有被改变过。这个漏洞称为CAS操作的“ABA问题”。大部分情况下ABA问题不会影响程序并发的正确性，如果需要解决ABA问题，改用传统的互斥同步可能会比原子类更为高效。 1.2.3 无同步方案​ 要保证线程安全，也并非一定要进行阻塞或非阻塞同步，同步与线程安全两者没有必然的联系。同步只是保障存在共享数据争用时正确性的手段，如果能让一个方法本来就不涉及共享数据，那它自然就不需要任何同步措施去保证其正确性，因此会有一些代码天生就是线程安全的。 ​ 可重入代码(Reentrant Code)：这种代码又称纯代码(Pure Code)，是指可以在代码执行的任何时刻中断它，转而去执行另外一段代码(包括递归调用它本身)，而在控制权返回后，原来的程序不会出现任何错误，也不会对结果有所影响。这意味着相对线程安全来说，可重入性是更为基础的特性，它可以保证代码线程安全，即所有可重入的代码都是线程安全的，但并非所有的线程安全的代码都是可重入的。 ​ 可重入代码有一些共同的特征，例如，不依赖全局变量、存储在堆上的数据和公用的系统资源，用到的状态量由参数重传入，不调用非可重入的方法等。一个简单的判断：如果一个方法的返回结果是可以预测的，只要输入了相同的数据，就都能返回相同的结果，那它就满足可重入性的要求，当然也就是线程安全的。 ​ 线程本地存储(Thread Local Storage)：如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。 ​ 符合这种特点的应用并不少见，大部分使用消费队列的架构模式(如“生产者-消费者”模式)都会将产品的消费过程限制在一个线程中消费完，其中最重要的一种应用实例就是经典Web交互模型中的“一个请求对应一个服务器线程”(Thread-per-Request)的处理方式，这种处理方式的广泛应用使得很多Web服务端应用都可以使用线程本地存储来解决线程安全问题。 ​ Java语言中，如果一个变量要被多线程访问，可以使用volatile关键字将它声明为“易变的”；如果一个变量只要被某个线程独享，可以通过java.lang.ThreadLocal类来实现线程本地存储的功能。每一个线程的Thread对象中都有一个ThreadLocalMap对象，这个对象存储了一组以ThreadLocal.threadLocalHashCode为键，以本地线程为值的K- V值对，ThreadLocal对象就是当前线程的ThreadLocalM ap的访问入口，每一个ThreadLocal对象都包含了一个独一无二的threadLocalHashCode值，使用这个值就可以在线程K -V值对中找回对应的本地线程变量。 2. 锁优化2.1 自旋锁与自适应锁​ 互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给Java虚拟机的并发性能带来了很大的压力。如果物理机器有一个以上的处理器或者处理器核心，能让两个或以上的线程同时并行执行，我们就可以让后面请求锁的那个线程“稍等一会”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只须让线程执行一个忙循环(自旋)，这项技术就是所谓的自旋锁。 ​ 自旋等待不能代替阻塞，自旋等待本身虽然避免了线程切换的开销，但它是要占用处理器时间的，所以如果锁被占用的时间很短，自旋等待的效果就会非常好，反之如果锁被占用的时间很长，那么自旋的线程只会白白消耗处理器资源，而不会做任何有价值的工作，这就会带来性能的浪费。因此自旋等待的时间必须有一定的限 度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程。自旋次数的默认值是十次，用户也可以使用参数-XX:PreBlockSpin来自行更改。 ​ 在JDK6中对自旋锁的优化，引入了自适应的自旋。自适应意味着自旋的时间不再是固定的了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而允许自旋等待持续相对更长的时间。另一方面，如果对于某个锁，自旋很少成功获得过锁，那在以后要获取这个锁时将有可能直接省略掉自旋过程，以避免浪费处理器资源。 2.2 锁消除​ 锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须再进行。 2.3 锁粗化​ 如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展(粗化)到整个操作序列的外部。 2.4 轻量级锁​ 轻量级锁是JDK 6时加入的新型锁机制，它名字中的“轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的，因此传统的锁机制就被称为“重量级”锁。 ​ 轻量级锁的工作过程：在代码即将进入同步块的时候，如果此同步对象没有被锁定(锁标志为“01”状态)，虚拟机首先在当前线程的栈帧中建立一个名为锁记录(Lock Record)的空间，用于存储锁对象目前的Mark Word的拷贝(官方为这份拷贝加了一个Displaced前缀，即Displaced Mark Word)，这时候线程堆栈与对象头的状态如图所示。 ​ 然后，虚拟机将使用CAS操作尝试把对象的Mark Word更新为指向Lock Record的指针。如果这个更新动作成功了，即代表该线程拥有了这个对象的锁，并且对象Mark Word锁标志位(Mark Word的最后两个比特)将转变为“00”，表示此对象处于轻量级锁定状态。这时候线程堆栈与对象头的状态如图所示。 ​ 如果这个更新操作失败了，那就意味着至少存在一条线程与当前线程竞争获取该对象的锁。虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是，说明当前线程已经拥有了这个对象的锁，那直接进入同步块继续执行就可以了，否则就说明这个锁对象已经被其他线程抢占了。如果出现两条以上的线程竞争用同一个锁的情况，那轻量级锁就不在再有效，必须要膨胀为重量级锁，锁标志的值变为“10”，此时Mark Word中存储的就是指向重量级锁(互斥量)的指针，后面等待锁的线程也必须进入阻塞状态。 ​ 它的解锁过程也同样是通过CAS操作来进行的，如果对象的Mark Word仍然指向线程的锁记录，那就用CAS操作把对象当前的Mark Word和线程中复制的Displaced Mark Word替换回来。假如能够成功替换，那整个同步过程就顺利完成了；如果替换失败，则说明有其他线程尝试过获取该锁，就要在释放锁的同时，唤醒被挂起的线程。 ​ 如果没有竞争，轻量级锁便通过CAS操作成功避免了使用互斥量的开销；但如果确实存在锁竞争，除了互斥量的本身开销外，还额外发生了CAS操作的开销。因此在有竞争的情况下，轻量级锁反而会比传统的重量级锁更慢。 2.5 偏向锁​ 偏向锁也是JDK6中引入的一项锁优化措施，它的目的是消除数据在无竞争情况下的同步原语， 进一步提高程序的运行性能。如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不去做了。 ​ 它的意思是这个锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁一直没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。 ​ 假设当前虚拟机启用了偏向锁(启用参数-XX:+UseBiased Locking，这是自JDK6起HotSpot虚拟机的默认值)，那么当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设置为“01”、把偏向模式设置为“1”，表示进入偏向模式。同时使用CAS操作把获取到这个锁的线程 的ID记录在对象的Mark Word之中。如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作(例如加锁、解锁及对Mark Word的更新操作等)。 ​ 一旦出现另外一个线程去尝试获取这个锁的情况，偏向模式就马上宣告结束。根据锁对象目前是否处于被锁定的状态决定是否撤销偏向(偏向模式设置为“ 0”)，撤销后标志位恢复到未锁定(标志位为“ 01”)或轻量级锁定(标志位为“00”)的状态，后续的同步操作就按照上面介绍的轻量级锁那样去执行。 ​ 作为绝大多数对象哈希码来源的Object ::hashCode()方法，返回的是对象的一致性哈希码(Identity Hash Code)，这个值是能强制保证不变的，它通过在对象头中存储计算结果来保证第一次计算之后，再次调用该方法取到的哈希码值永远不会再发生改变。因此，当一个对象已经计算过一致性哈希码后，它就再也无法进入偏向锁状态了；而当一个对象当前正处于偏向锁状态，又收到需要计算其一致性哈希码请求时，它的偏向状态会被立即撤销，并且锁会膨胀为重量级锁。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"Java内存模型与线程","slug":"Java内存模型与线程","date":"2021-07-09T08:09:53.000Z","updated":"2021-07-26T08:17:27.716Z","comments":true,"path":"2021/07/09/Java内存模型与线程/","link":"","permalink":"/2021/07/09/Java内存模型与线程/","excerpt":"","text":"1. 硬件的效率与一致性​ 由于计算机的存储设备与处理器的运算速度有着几个数量级的差距，所以现代计算机系统都不得不加入一层或多层读写速度尽可能接近处理器运算速度的高速缓存(Cache)来作为内存与处理器之间的缓冲：将运算需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。 ​ 基于高速缓存的存储交互很好地解决了处理器与内存速度之间的矛盾，但是也为计算机系统带来更高的复杂度，它引入了一个新的问题：缓存一致性(Cache Coherence)。在多路处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存(Main Memory)，这种系统称为共享内存多核系统(Shared Memory Multiprocessors System)。为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有MSI、MESI(Illinois Protocol)、MOSI、Synapse、Firefly及Dragon Protocol等。 ​ 除了增加高速缓存之外，为了使处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执行(Out-Of-Order Execution)优化，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的，但并不保证程序中各个语句计算的先后顺序与输入代码中的顺序一致，因此如果存在一个计算任务依赖另外一个计算任务的中间结果，那么其顺序性并不能靠代码的先后顺序来保证。与处理器的乱序执行优化类似，Java虚拟机的即时编译器中也有指令重排序 (Instruction Reorder)优化。 2. Java内存模型​ 《Java虚拟机规范》中曾试图定义一种“Java内存模型”(Java Memory Model，JMM)来屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。 2.1 主内存与工作内存​ Java内存模型的主要目的是定义程序中各种变量的访问规则，即关注在虚拟机中把变量值存储到内存和从内存中取出变量值这样的底层细节。此处的变量值（Variables）与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但是不包括局部变量与方法参数，因为后者是线程私有的，不会被共享，自然就不会存在竞争问题。为了获得更好的执行效能，Java内存模型并没有限制执行引擎使用处理器的特定寄存器或缓存来和主内存进行交互，也没有限制即时编译器是否要进行调整代码执行顺序这类优化措施。 ​ Java内存模型规定了所有的变量都存储在主内存（Main Memory）中。每条线程还有自己的恶工作内存（Working Memory），线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。不能的线程直接也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。 2.2 内存间交互操作​ 关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存这一类的实现细节，Java内存模型中定义了以下8种操作来完成。Java虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的。 lock（锁定）：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。 unlock（解锁）：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 stroe（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用。 write（写入）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 ​ 如果要把一个变量从主内存拷贝到工作内存，那就要按顺序执行read和load操作，如果要把变量从工作内存同步回主内存，就要按顺序执行store和write操作。Java内存模型只要求上述两个操作必须按顺序执行，但不要求是连续执行。也就是说read与load之间、store与write之间是可插入其他指令的。Java内存模型还规定了在执行上述8种基本操作时必须满足如下规则： 不允许read和load、store和write操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者工作内存发起回写了但主内存不接受的情况出现。 不允许一个线程丢弃它最近的assign操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。 一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量 ，换句话说就是对一个变量实施use、store操作之前，必须先执行assign和load操作。 一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作以初始化变量的值。 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。 2.3 对于volatile型变量的特殊规则​ 当一个变量被定义成volatile之后，它将具备两项特性：第一项是保证此变量对所有线程的可见性，这里的“可见性”是指当这一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。而普通变量并不能做到这一点，普通变量的值在线程间传递说均需要通过主内存来完成。 ​ 由于volatile变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁 （使用synchronized、java.util.concurrent中的锁或原子类）来保证原子性： 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。 变量不需要与其他的状态变量共同参与不变约束。 ​ 使用volatile变量的第二个语义是禁止指令重排序优化，普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。 因为在同一个线程的方法执行过程中无法感知到这点，这就是Java内存模型中描述的所谓“线程内表现为串行的语义”（Within-Thread As-If-Serial Semantics）。 2.4 针对long和double型变量的特殊规则​ Java内存模型要求lock、unlock、read、load、assign、use、store、write这八种操作都具有原子性，但是对于64位的数据类型（long和double），在模型中特别定义了一条宽松的规定：允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32为位的操作来进行，即允许虚拟机实现自行选择是否要保证64位数据类型的load、sotre、read和write这四个操作的原子性，这就是所谓的“long和double的非原子性协定”（Non-Atomic Treatment of double and long Variables）。 2.5 原子性、可见性与有序性2.5.1 原子性（Atomicity）​ 由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write这六个，可以认为基本数据类型的访问、读写都是具备原子性的。 ​ 如果应用场景需要一个更大范围的原子性保证，Java内存模型还提供了lock和unlock操作来满足这种需求，尽管虚拟机未把lock和unlock操作直接开放给用户使用，但是却提供了更高层次的字节码指令monitorenter和monitorexit来隐式地使用这两个操作。这两个字节码指令反映到Java代码中就是同步块——synchronized关键字，因此在synchronized块之间的操作也具备原子性。 2.5.2 可见性（Visibility）​ 可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。普通变量与volatile变量的区别是，volatile的特殊规则保证了新值 能立即同步到主内存，以及每次使用前立即从主内存刷新。volatile保证了多线程操作 时变量的可见性，而普通变量则不能保证这一点。 ​ 除了volatile之外，Java还有两个关键字能实现可见性，它们是synchronized和final。同步块的可见性是由“对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）”这条规则获得的。而final关键字的可见性是指：被final修饰的字段在构造器中一旦被初始化完成，并且构造器没有把“this”的引用传递出去，那么在其他线程中就能看见final字段的值。 2.5.3 有序性（Ordering）​ Java程序中天然的有序性可以总结为一句话：如果在本线程内观察，所有的操作都是有序的;如果在一个线程中观察另一个线程， 所有的操作都是无序的。前半句是指“线程内似表现为串行的语义”（Within-Thread As-If-SerialSemantics），后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。 ​ Java语言提供了volat ile和sy nchroniz ed两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排序的语义，而synchronized则是由“一个变量在同一个时刻只允许一条线程对其进行lock操作”这条规则获得的，这个规则决定了持有同一个锁的两个同步块只能串行地进入。 2.6 先行发生原则(Happens-Before)​ 先行发生是Java内存模型中定义的两项操作之间的偏序关系。 ​ 下面是Java内存模型下一些“天然的”先行发生关系，这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则它们就没有顺序性保障，虚拟机可以对它们随意地进行重排序。 程序次序规则（Program Order Rule）：在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作。注意，这里说的是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。 管程锁定规则（Monitor Lock Rule）：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是“同一个锁”，而“后面”是指时间上的先后。 volatile变量规则（Volatile Variable Rule）：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的“后面”同样是指时间上的先后。 线程启动规则（Thread Start Rule）：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终止规则（Thread Termination Rule）：线程中的所有操作都先行发生于对此线程的终止检测，可以通过Thread::join()方法是否结束、Thread::isAlive()的返回值等手段检测线程是否已经终止执行。 线程中断规则（Thread Interruption Rule）：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread::int errupted()方法检测到是否有中断发生。 对象终结规则（Finalizer Rule）：一个对象的初始化完成(构造函数执行结束)先行发生于它的 finalize()方法的开始。 传递性（Transitivity）：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。 3. Java与线程3.1 线程的实现​ 线程是比进程更轻量级的调度执行单位，线程的引入，可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源（内存地址、文件I/O等），又可以独立调度。目前线程是Java里面进行处理器资源调度的最基本单位。 ​ 实现线程主要有三种方式:使用内核线程实现（1:1实现），使用用户线程实现（1:N实现）， 使用用户线程加轻量级进程混合实现（N:M实现）。 3.1.1 内核线程实现​ 使用内核线程实现的方式也被称为1：1实现。内核线程（Kernel-Level Thread，KLT）就是直接由操作系统内核（Kernel）支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器（Scheduler）对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程可以视为内核的一个分身，这样操作系统就有能力同时处理多件事情，支持多线程的内核就称为多线程内核（Multi-Threads Kernel）。 ​ 程序一般不会直接使用内核线程，而是使用内核线程的一种高级接口——轻量级进程（Light Weight Process，LWP），轻量级进程就是通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。这种轻量级进程与内核线程之间1：1的关系称为一对一的线程模型。 ​ 轻量级进程也具有它的局限性：首先，由于是基于内核线程实现的，所以各种线程操作，如创建、析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态（User M ode）和内核态（Kernel Mode）中来回切换。其次，每个轻量级进程都需要有一个内核线程的支持，因此轻量级进程要消耗一定的内核资源（如内核线程的栈空间），因此一个系统支持轻量级进程的数量是有限的。 3.1.2 用户线程实现​ 使用用户线程实现的方式被称为1：N实现。广义上来讲，一个线程只要不是内核线程，都可以认为是用户线程（User Thread，UT）的一种。 ​ 而狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知到用户线程的存在及如何实现的。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。如果程序实现得当，这种线程不需要切换到内核态，因此操作可以是非常快速且低消耗的，也能够支持规模更大的线程数量，部分高性能数据库中的多线程就是由用户线程实现的。 ​ 用户线程的优势在于不需要系统内核支援，劣势也在于没有系统内核的支援，所有的线程操作都需要由用户程序自己去处理。 3.1.3 混合实现​ 线程除了依赖内核线程实现和完全由用户程序自己实现之外，还有一种将内核线程与用户线程一起使用的实现方式，被称为N：M实现。 ​ 许多UNIX系列的操作系统，如Solaris、HP-UX等都提供了M：N的线程模型实现。在这些操作系统上的应用也相对更容易应用M：N的线程模型。 3.1.4 Java线程的实现​ Java线程如何实现并不受Java虚拟机规范的约束，这是一个与具体虚拟机相关的话题。“主流”平台上的“主流”商用Java虚拟机的线程模型普遍都被替换为基于操作系统原生线程模型来实现，即采用1：1的线程模型。 3.2 Java线程调度​ 线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种，分别是协同式 （Cooperative Threads-Scheduling）线程调度和抢占式（Preemptive Threads-Scheduling）线程调度。 ​ 如果使用协同式调度的多线程系统，线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上去。协同式多线程的最大好处是实现简单，切换操作对线程自己是可知的，所以一般没有什么线程同步的问题。它的坏处也很明显：线程执行时间不可控制。 ​ 如果使用抢占式调度的多线程系统，那么每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。Java使用的线程调度方式就是抢占式调度。 3.3 状态转换​ Java语言定义了6种线程状态，在任意一个时间点中，一个线程只能有且只有其中的一种状态，并且可以通过特定的方法在不同状态之间转换。这6种状态分别是： 新建（New）：创建后尚未启动的线程处于这种状态。 运行（Runnable）：包括操作系统线程状态中的Running和Ready ，也就是处于此状态的线程有可能正在执行，也有可能正在等待着操作系统为它分配执行时间。 无限期等待（Waiting）：处于这种状态的线程不会被分配处理器执行时间，它们要等待被其他线程显式唤醒。以下方法会让线程陷入无限期的等待状态： 没有设置Timeout参数的Object::wait()方法； 没有设置Timeout参数的Thread::join()方法； LockSupport::parking()方法。 限期等待（Timed Waiting）：处于这种状态的线程也不会被分配处理器执行时间，不过无须等待被其他线程显式唤醒，在一定时间之后它们会由系统自动唤醒。以下方法会让线程陷入限期等待状态： Thread::sleep()方法； 设置了Timeout参数的Object::wait()方法； 设置了Timeout参数的Thread::join()方法； LockSupport::parkNanos()方法； LockSupport::parkUntil()方法。 阻塞（Blocked）：线程被阻塞了，“阻塞状态”与“等待状态”的区别是“阻塞状态”在等待着获取到一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生；而“等待状态”则是在等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。 结束（Terminated）：已终止线程的线程状态，线程已经结束执行。 ​ 上述6种状态在遇到特定事件发生的时候将会互相转换。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"虚拟机类加载机制","slug":"虚拟机类加载机制","date":"2021-07-01T06:08:59.000Z","updated":"2021-07-07T07:48:46.766Z","comments":true,"path":"2021/07/01/虚拟机类加载机制/","link":"","permalink":"/2021/07/01/虚拟机类加载机制/","excerpt":"","text":"1. 概述​ Java虚拟机把描述该类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这个过程被称作虚拟机的类加载机制。 2. 类加载的时机​ 一个类型从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期将会经历加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）七个阶段，其中验证、准备、解析三个部分统称为连接（Linking）。 ​ 加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的，类型的加载过程必须按照这种顺序按部就班地开始，而解析阶段不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定特性（也称为动态绑定或晚期绑定）。 3. 类加载的过程3.1 加载​ “加载”（Loading）阶段是整个“类加载”（Class Loading）过程中的一个阶段。在加载阶段，Java虚拟机要完成以下三件事情： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 ​ 加载阶段结束后，Java虚拟机外部的二进制字节流就按照虚拟机所设定的格式存储在方法区之中了，方法区的数据存储格式完全由虚拟机实现自行定义，《Java虚拟机规范》未规定此区域的具体数据结构。类型数据妥善安置在方法区之后，会在Java堆内存中实例化一个java.lang.Class类的对象，这个对象将作为程序访问方法区中的类型数据的外部接口。 ​ 加载阶段与连接阶段的部分动作（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的一部分，这两个阶段的开始时间仍然保持着固定的先后顺序。 3.2 验证​ 验证是连接阶段的第一步，目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。从整体上看，验证阶段大致上会完成下面四个阶段的校验动作：文件格式验证、元数据验证、字节码验证和符号引用验证。 3.2.1 文件格式验证​ 第一阶段要验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理。该验证阶段的主要目的是保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个Java类型信息的要求。这阶段的验证是基于二进制字节流进行的，只有通过了这个阶段的验证之后，这段字节流才被允许进入Java虚拟机内存的方法区中进行存储，所以后面的三个验证阶段全部是基于方法区的存储结构上进行的，不会再直接读取、操作字节流了。 3.2.2 元数据验证​ 第二阶段是对字节码描述的信息进行语义分析，以保证其描述的信息符合《Java语言规范》的要求。目的主要是对类的元数据信息进行语义校验，保证不存在与《Java语言规范》定义相悖的元数据信息。 3.2.3 字节码验证​ 第三阶段是整个验证过程中最复杂的一个阶段，主要目的是通过数据流分析和控制流分析，确定程序语义是合法的、符合逻辑的。在第二阶段对元数据信息中的数据类型校验完毕以后，这阶段就要对类的方法体(Class文件中的Code属性)进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的行为。 3.2.4 符号引用验证​ 最后一个阶段的校验行为发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生。符号引用验证可以看作是对类自身以外(常量池中的各种符号 引用)的各类信息进行匹配性校验，通俗来说就是，该类是否缺少或者被禁止访问它依赖的某些外部类、方法、字段等资源。 ​ 符号引用验证的主要目的是确保解析行为能正常秩序，如果无法通过符号引用验证，Java虚拟机将会跑出一个java.lang.IncompatibleClassChangeError的子类异常，典型的如：java.lang.IllegalAccessError、java.lang.NoSuchFieldError、java.lang.NoSuchMethoError等。 3.3 准备​ 准备阶段是正式为类中定义的变量（即静态变量，被static修饰的变量）分配内存并设置类变量初始值的阶段。 ​ 首先这时候进行内存分配的仅包括类变量，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中。其次是这里所说的初始值“通常情况”下是数据类型的零值。 3.4 解析​ 解析阶段是Java虚拟机将常量池内的符号引用替换为直接引用的过程。 符号引用（Symbolic References）：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量。符号引用与虚拟机布局无关，引用的目标并不一定是已经加载到虚拟机内存当中的内容。各种虚拟机实现的内存布局可以各不相同，但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在《Java虚拟机规范》的Class文件格式中。 直接引用（Direct References）：直接引用是可以直接指向目标的指针、相对偏移量或者是一个能间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局直接相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在虚拟机的内存中存在的。 ​ 类似地，对方法或者字段的访问，也会在解析阶段中对它们的可访问性(public、protected、 private、&lt; package&gt; ) 进行检查。 ​ 对同一个符号引用进行多次解析请求是很常见的事情，除invokedy namic指令以外，虚拟机实现可以对第一次解析的结果进行缓存，譬如在运行时直接引用常量池中的记录，并把常量标识为已解析状态，从而避免解析动作重复进行。 ​ 不过对于invokedy namic指令，上面的规则就不成立了。当碰到某个前面已经由invokedy namic指令触发过解析的符号引用时，并不意味着这个解析结果对于其他invokedy namic指令也同样生效。因为invokedy namic指令的目的本来就是用于动态语言支持，它对应的引用称为“动态调用点限定符 (Dynamically-Computed Call Site Specifier)”，这里“动态”的含义是指必须等到程序实际运行到这条指令时，解析动作才能进行。 ​ 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符这7类符号引用进行。 3.4.1 类或接口的解析​ 假设当前代码所处的类为D，如果要把一个从未解析过的符号引用N解析为一个类或接口C的直接引用，那虚拟机完成整个解析的过程包括以下3个步骤： 如果C不是一个数组类型，那虚拟机会把代表N的全限定名传递给D的类加载器去加载这个类C。在加载过程中，由于元数据验证、字节码验证的需要，又可能触发其他相关类的加载动作。一旦这个加载过程出现了任何异常，解析过程就将宣告失败。 如果C是一个数组类型，并且数组的元素类型为对象，也就是N的描述符会是类似“[Ljava/lang/Integer”的形式，那将会按照第一点的规则加载数组元素类型。如果N的描述符如前面所假设的形式，需要加载的元素类型就是“java.lang.Integer”，接着由虚拟机生产一个代表数组维度和元素的数组对象。 如果上面这两步没有出现任何异常，那么C在虚拟机中实际上已经成为一个有效的类或接口了，但在解析完成前还要进行符号引用验证，确认D是否具备C的访问权限。如果发现不具备访问权限，将抛出java.lang.IllegalAccessError异常。 针对上面第3点访问权限验证，在JDK 9引入了模块化以后，一个public类型也不再意味着程序任何位置都有它的访问权限，我们还必须检查模块间的访问权限。 如果我们说一个D拥有C的访问权限，那就意味着以下3条规则中至少有其中一条成立： 被访问类C是public的，并且与访问类D处于同一个模块。 被访问类C是public的，不与访问类D处于同一个模块，但是被访问类C的模块允许被访问类D的模块进行访问。 被访问类C不是public的，但是它与访问类D处于同一个包中。 3.4.2 字段解析​ 要解析一个未被解析过的字段符号引用，首先将会对字段表内class_index项中索引的CONST ANT_Class_info符号引用进行解析，也就是字段所属的类或接口的符号引用。如果在解析这个类或接口符号引用的过程中出现了任何异常，都会导致字段符号引用解析的失败。如果解析成功完成，那把这个字段所属的类或接口用C表示，《Java虚拟机规范》要求按照如下步骤对C进行后续字段的搜索： 如果C本身就包含了简单名称和字段描述符都与目标匹配的字段，则返回这个字段的直接引用，查找结束。 否则，如果在C中实现了接口，将会按照继承关系从下往上递归搜索各个接口和它的父接口，如果接口中包含了简单名称和字段描述都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。 否则，如果C不是java.lang.Object的话，将会按照继承关系从下往上递归搜索其父类，如果在父类中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。 否则，查找失败，抛出java.lang.NoSuchFieldError异常。 ​ 如果查找过程成功返回了引用，将会对这个字段进行权限验证，如果发现不具备对字段的访问权限，将抛出java.lang.IllegalAccessError异常。 3.4.3 方法解析方法解析的第一个步骤与字段解析一样，也是需要先解析出方法表的class_index项中索引的方法所属的类或接口的符号引用，如果解析成功，那么依然用C表示这个类，接下来虚拟机将会按照如下步骤进行后续的方法搜索： 由于Class文件格式中类的方法和接口的方法符号引用的常量类型定义是分开的，如果在类的方法表中发现class_index中索引的C是个接口的话，那就直接抛出java.lang.IncompatibleClassChangeError异常。 如果通过了第一步，在类C中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在类C的父类中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在类C实现的接口列表及它们的父接口之中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果存在匹配的方法，说明类C是一个抽象类 ，这时候查找结束，抛出java.lang.AbstractMethodError异常。 否则，宣告方法查找失败，抛出java.lang.NoSuchMethodError。 ​ 最后，如果查找过程成功返回了直接引用，将会对这个方法进行权限验证，如果发现不具备对此方法的访问权限，将抛出java.lang.IllegalAccessError异常。 3.4.4 接口方法解析​ 接口方法也是需要先解析出接口方法表的class_index项中索引的方法所属的类或接口的符号引用，如果解析成功，依然用C表示这个接口，接下来虚拟机将会按照如下步骤进行后续的接口方法搜索： 与类的方法解析相反，如果在接口方法中发现class_index中的索引C是个类而不是接口，那么就直接抛出java.lang.IncompatibleClassChangeError异常。 否则，在接口C中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在接口C的父接口中递归查找，直到java.lang.Object类型（接口方法的查找范围也会包括Object类中的方法）为止，看是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 对于规则3，由于Java的接口允许多重继承，如果C的不同父接口中存有多个简单名称和描述符都与目标相匹配的方法，那将会从这多个方法中返回一个并结束查找。 否则，宣告方法查找失败，抛出java.lang.NoSuchMethodError异常。 ​ 在JDK 9中增加了接口的静态私有方法，也有了模块化的访问约束，所以从JDK 9起，接口方法的访问也完全有可能因访问权限控制而出现java.lang.IllegalAccessError异常。 3.5 初始化​ 类的初始化阶段是类加载过程的最后一个步骤。直到初始化阶段，Java虚拟机才真正开始执行类中编写的Java程序代码，将主导权移交给应用程序。 ​ 进行准备阶段时，变量已经赋过一次系统要求的初始零值，而在初始化阶段，则会根据程序员通过程序编码制定的主观计划去初始化类变量和其他资源。初始化阶段就是执行类构造器()方法的过程。()并不是程序员在Java代码中直接编写的方法，它是Javac编译器的自动生成物。 ()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能放问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。 ()方法与类的构造函数（即在虚拟机视角中的实例构造器()方法）不同，它不需要显示地调用父类构造器，Java虚拟机会保证在子类的()方法执行前，父类的()已经执行完毕。因此在Java虚拟机中第一个被执行的()方法的类型肯定是java.lang.Object。 由于父类的()方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。 ()方法对于类或接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成()方法。 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成()方法。但接口与类不同的是，执行接口的()方法不需要先执行父接口的()方法，因为只有当父接口中定义的变量被使用时，父接口才会被初始化。此外，接口的实现类在初始化时也一样不会执行接口的()方法。 Java虚拟机必须保证一个类的()方法在多线程环境中被正确地加锁同步，如果多个线程同时去初始化一个类，那么只会有其中一个线程去执行这个类的()方法，其他线程都需要阻塞等待，直到活动线程执行完毕()方法。如果在一个类的()方法中有耗时很长的操作，那就可能造成多个进程阻塞。 4. 类加载器Java虚拟机设计团队有意把类加载阶段中的“通过一个类的全限定名来获取描述该类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需的类。实现这个动作的代码被称为“类加载器”（Class Loader）。 4.1 类与类加载器​ 类加载器虽然只用于实现类的加载动作，但它在Java程序中起到的作用却远超类加载阶段。对于任意一个类，都必须由加载它的类加载器和这个类本身一起共同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。 4.2 双亲委派模型​ 站在Java虚拟机的角度来看，只存在两种不同的类加载器：一种是启动类加载器（Bootstrap ClassLoader），这个类加载器使用C++实现，是虚拟机自身的一部分；另外一种是其他所有的类加载器，这些类加载器都由Java语言实现，独立存在于虚拟机外部，并且全部继承自抽象类java.lang.ClassLoader。 启动类加载器（Bootstrap Class Loader）：这个类加载器负责加载存放在&lt;JAVA_HOME&gt;\\lib目录，或者被-Xbootclasspath参数所指定的路径中存放的，而且是Java虚拟机能够识别的类库加载到虚拟机的内存中。 扩展类加载器（Extension Class Loader）：这个类加载器是在类sun.misc.Launcher&amp;ExtClassLoader中以Java代码的形式实现的。它负责加载&lt;JAVA_HOME&gt;\\lib\\ext目录中，或者被java.ext.dirs系统变量所指定的路径中所有的类库。 应用程序类加载器（Application Class Loader）：这个类加载器由sum.misc.Launcher$AppClassLoader来实现。它负责加载用户类路径（ClassPath）上所有的类库。 ​ 各种类加载器之间的层次关系被称为类加载器的“双亲委派模型（Parents Delegation Model）”。双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。不过这里类加载器之间的父子关系一般不是以继承（Inheritance）的关系来实现的，而是通常使用组合（Composition）关系来复用父加载器的代码。 ​ 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到最顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去完成加载。 5. Java模块化系统5.1 模块的兼容性​ 为了使可配置的封装隔离机制能够兼容传统的类路径查找机制，JDK 9提出了与“类路径”(ClassPath)相对应的“模块路径”(M odulePath)的概念。简单来说，就是某个类库到底是模块还 是传统的JAR包，只取决于它存放在哪种路径上。 5.2 模块化下的类加载器​ 模块化下的类加载器仍然发生了一些应该被注意到变动，主要包括以下几个方面。 扩展类加载器（Extension Class Loader）被平台类加载器（Platform Class Loader）取代。 平台类加载器和应用程序加载器都不再派生自java.net.URLClassLoader。现在启动类加载器、平台类加载器、应用程序类加载器全都继承于jdk.int ernal.loader.BuiltinClassLoader。 JDK 9中虽然仍然维持着三层类加载器和双亲委派的架构，但类加载的委派关系也发生了变动。当平台及应用程序类加载器收到类加载请求，在委派给父加载器加载前，要先判断该类是否能够归属到某一个系统模块中，如果可以找到这样的归属关系，就要优先委派给负责那个模块的加载器完成加载。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"类文件结构","slug":"类文件结构","date":"2021-06-17T08:28:56.000Z","updated":"2021-07-13T08:33:59.167Z","comments":true,"path":"2021/06/17/类文件结构/","link":"","permalink":"/2021/06/17/类文件结构/","excerpt":"","text":"1. Class类文件的结构​ 任何一个Class文件都对应着唯一的一个类或接口的定义信息，但是反过来说，类或接口并不一定都得定义在文件里(譬如类或接口也可以动态生成，直接送入类加载器中)。 ​ Class文件是一组以8个字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑地排列在文件之中，中间没有添加任何分隔符，这使得整个Class文件中存储的内容几乎全部是程序运行的必要数据，没有空隙存在。当遇到需要占用8个字节以上空间的数据项时，则会按照高位在前的方式分割成若干个8个字节进行存储。 ​ 根据《Java虚拟机规范》的规定，Class文件格式采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：“无符号数”和“表”。 无符号数属于基本的数据类型，以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。 表是由多个符号数或者其他表作为数据项构成的复合数据类型，为了便于区分，所有表的命名都习惯性地以“_info”结尾。表用于描述有层次关系的复合结构的数据，整个Class文件本质上也可以视作是一张表。 1.1 魔数与Class文件的版本​ 每个Class文件的头4个字节被称为魔数(Magic Number)，它的唯一作用是确定这个文件是否为一个能被虚拟机接受的Class文件。使用魔数而不是扩展名来进行识别主要是基于安全考虑，因为文件扩展名可以随意改动。Class文件的魔数的值为0xCAFEBABE。 ​ 紧接着魔数的4个字节存储的是Class文件的版本号：第5和第6个字节是次版本号(Minor Version)，第7个和第8个字节是主版本号(Major Version)。 1.2 常量池​ 紧接着主、次版本号之后的是常量池入口，常量池可以比喻为Class文件里的资源仓库，它是Class 文件结构中与其他项目关联最多的数据，通常也是占用Class文件空间最大的数据项目之一，另外，它还是在Class文件中第一个出现的表类型数据项目。 ​ 由于常量池中常量的数量是不固定的，所以在常量池的入口需要放置一项u2类型的数据，代表常量池容量计数值(constant_pool_count)。这个容量计数是从1而不是0开始的。 ​ 常量池中主要存放两大类常量：字面量(Literal)和符号引用(Symbolic Reference)。字面量比较接近于Java语言层面的常量概念，如文本字符串、被声明为final的常量值等。而符号引用属于编译原理方面的概念，主要包括下面几类常量： 被模块导出或者开放的包（Package） 类和接口的全限定名（Fully Qualified Name） 字段的名称和描述符（Descriptor） 方法的名称和描述符 方法句柄和方法类型（Method Handle、Method Type、Invoke Dynamic） 动态调用点和动态常量（Dynamically-Computed Call Site、Dynamically-Computed Constant） ​ Java代码在进行Javac编译的时候，是在虚拟机加载Class文件的时候进行动态连接。也就是说，在Class文件中不会保存各个方法、字段最终在内存中的布局信息，这些字段、符号引用不经过虚拟机在运行期转换的话是无法得到真正的内存入口地址，也就无法直接被虚拟机使用的。当虚拟机做类加载时，将会从常量池获得对应的符号引用，再在类创建时或运行时解析、翻译到具体的内存地址之中。 1.3 访问标志​ 在常量池结束之后，紧接着的2个字节代表访问标志（access_flags），这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final；等等。 1.4 类索引、父类索引与接口索引集合​ 类索引（this_class）和父类索引（super_class）都是一个u2类型的数据，而接口索引集合（interfaces）是一组u2类型的数据的集合，Class文件中由这三项数据来确定该类型的继承关系。类索引引用用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。由于Java语言不允许多重继承，所以父类索引只有一个，除了java.lang.Object之外，所有的Java类都有父类，因此除了 java.lang.Object外，所有Java类的父类索引都不为0。 ​ 类索引、父类索引和接口索引集合都按顺序排列在访问标志之后，类索引和父类索引用两个u2类型的索引值表示，它们各自指向一个类型为CONSTANT_Class_info的类描述符常量，通过 CONSTANT_Class_info类型的常量中的索引值可以找到定义在CONSTANT_Utf8_info类型的常量中的全限定名字符串。 ​ 对于接口索引集合，入口的第一项u2类型的数据为接口计数器(int erfaces _count )，表示索引表的容量。如果该类没有实现任何接口，则该计数器值为0，后面接口的索引表不再占用任何字节。 1.5 字段表集合​ 字段表（field_info）用于描述接口或者类中声明的变量。 ​ 字段修饰符放在access_flags项目中，它与类中的access_flags项目是非常类似的，都是一个u2的数据类型，其中可以设置的标志位和含义为下表。 ​ 由于语法规则的约束，ACC_PUBLIC、ACC_PRIVATE、ACC_PROTECTED三个标志最 多只能选择其一，ACC_FINAL、ACC_VOLATILE不能同时选择。接口之中的字段必须有 ACC_PUBLIC、ACC_STATIC、ACC_FINAL标志，这些都是由Java本身的语言规则所导致的。 ​ 跟随access_flags标志的是两项索引值:name_index和descrip tor_index。它们都是对常量池项的引用，分别代表着字段的简单名称以及字段和方法的描述符。 ​ 描述符的作用是用来描述字段 的数据类型、方法的参数列表(包括数量、类型以及顺序)和返回值。根据描述符规则，基本数据类型(byte、char、double、float、int、long、short、boolean)以及代表无返回值的void类型都用一个大写字符来表示，而对象类型则用字符L加对象的全限定名来表示。 ​ 字段表所包含的固定数据项目到descrip tor_index为止就全部结束了，不过在descrip -tor_index之后跟随着一个属性表集合，用于存储一些额外的信息，字段表可以在属性表中附加描述零至多项的额外信息。 1.6 方法表集合​ Class文件存储格式对方法的描述与对字段的描述采用了几乎完全一致的方式，方法表的结构如同字段表一样，依次包括访问标志（access_flags）、名称索引（name_index）、描述索引符（descriptor_index）、属性表集合（attributes）几项。 ​ 因为volat ile关键字和t rans ient 关键字不能修饰方法，所以方法表的访问标志中没有了 ACC_VOLATILE标志和ACC_TRANSIENT标志。与之相对，synchronized、native、strictfp和abstract 关键字可以修饰方法，方法表的访问标志中也相应地增加了ACC_SYNCHRONIZED、ACC_NATIVE、ACC_STRICTFP和ACC_ABSTRACT标志。 1.7 属性表集合​ Class文件、字段表、方法表都可以携带自己的属性表集合，以描述某些场景专有的信息。 2. 字节码指令​ Java虚拟机的指令由一个字节长度的、代表着某种特定操作含义的数字（称为操作码，Opcode）以及跟随其后的零至多个代表此操作所需的参数（称为操作数，Operand）\u0010构成。由于Java虚拟机采用面向操作数栈而不是面向寄存器的架构，所以大多数指令都不包含操作数，只有一个操作码，指令参数都存放在操作数栈中。 2.1 字节码与数据类型​ 在Java虚拟机的指令集中，大多数指令都包含其操作所对应的数据类型信息。对于大部分与数据类型相关的字节码指令，它们的操作码助记符种都有特殊的字符来表明专门为哪种数据类型服务：i代表int类型的数据操作，l代表long，s代表short，b代表byte，c代表char，f代表float，d代表double，a代表reference。 2.2 加载和存储指令​ 加载和存储指令用于将数据在栈帧中的局部变量表和操作数栈之间来回传输，这类指令包括： 将一个局部变量加载到操作数栈：load、iload_、lload、lload_、fload、fload_、dload、dload_、aload、aload_ 将一个数值从操作数栈存储到局部变量表：istore、istore_、lstore、lstore_、fstore、fstore_、dstore、dstore_、astore、astore_ 将一个常量加载到操作数栈：bipush、sipush、ldc、ldc_w、ldc2_w、aconst_null、iconst_m1、iconst_、lconst_、fconst_、dconst_ 扩充局部变量表的访问索引的指令：wide 2.3 运算指令​ 算术指令用于对两个操作数栈上的值进行某种特定运算，并把结果重新存入到操作栈项。大体上运算指令可以分为两种：对整型数据进行运算的指令与对浮点数据进行运算的指令。无论是哪种算术指令，均是使用Java虚拟机的算术类型来进行计算的，换句话说不存在直接支持byte、short、char和boolean类型的算术指令，对于上述几种数据的运算，应使用操作int类型的指令代替。所有的算术指令包括: 加法指令:iadd、ladd、fadd、dadd 减法指令:isub、lsub、fsub、dsub 乘法指令:imul、lmul、fmul、dmul 除法指令:idiv、ldiv、fdiv、ddiv 求余指令:irem、lrem、frem、drem 取反指令:ineg、lneg、fneg、dneg 位移指令:ishl、ishr、iushr、lshl、lshr、lushr 按位或指令:ior、lor 按位与指令:iand、land 按位异或指令:ixor、lxor 局部变量自增指令:iinc 比较指令:dcmpg、dcmpl、fcmpg、fcmpl、lcmp 2.4 类型转换指令​ 类型转换指令可以将两种不同的数值类型互相转换，这些转换操作一般用于实现用户代码中的显示类型转换操作。 ​ Java虚拟机直接支持（即转换时无须显式的转换指令）以下数值类型的宽化类型转换（Widening Numeric Conversion，即小范围类型向大范围类型的安全转换）： int类型到long、float或者double类型 long类型到float、double类型 float类型到double类型 ​ 与之相对的，处理窄化类型转换(Narrowing Numeric Conversion)时，就必须显式地使用转换指令来完成，这些转换指令包括i2b、i2c、i2s、l2i、f2i、f2l、d2i、d2l和d2f。窄化类型转换可能会导致转换结果产生不同的正负号、不同的数量级的情况，转换过程很可能会导致数值的精度丢失。 2.5 对象创建与访问指令​ 虽然类实例和数组都是对象，但Java虚拟机对类实例和数组的创建与操作使用了不同的字节码指令。对象创建后，就可以通过对象访问指令获取对象实例或者数组实例中的字段或者数组元素，这些指令包括： 创建类实例的指令：new 创建数组的指令：newarray、anewarray、multianewarray 访问类字段(static字段，或者称为类变量)和实例字段(非static字段，或者称为实例变量)的 指令：getfield、putfield、getstatic、putstatic 把一个数组元素加载到操作数栈的指令：baload、caload、saload、iaload、laload、faload、 daload、aaload 将一个操作数栈的值储存到数组元素中的指令：bastore、castore、sastore、iastore、fastore、 dastore、aastore 取数组长度的指令：array length 检查类实例类型的指令：instanceof、checkcast 2.6 操作数栈管理指令​ 如同操作一个普通数据结构中的堆栈那样，Java虚拟机提供了一些用于直接操作操作数栈的指令，包括: 将操作数栈的栈顶一个或两个元素出栈：pop 、pop2 复制栈顶一个或两个数值并将复制值或双份的复制值重新压入栈顶：dup 、dup 2、dup _x1、 dup2_x1、dup_x2、dup2_x2 将栈最顶端的两个数值互换：swap 2.7 控制转移指令​ 控制转移指令可以让Java虚拟机有条件或无条件地从指定位置指令(而不是控制转移指令)的下 一条指令继续执行程序，从概念模型上理解，可以认为控制指令就是在有条件或无条件地修改PC寄存器的值。控制转移指令包括： 条件分支：ifeq、iflt、ifle、ifne、ifgt、ifnull、ifnonnull、if_icmpeq、if_icmpne、if_icmplt、if_icmpgt、if_icmpge、if_acmpeq和if_acmpne 复合条件分支：tableswitch、lookupswitch 无条件分支：goto、goto_w、jsr、jsr_w、ret 2.8 方法调用和返回指令​ 以下五条指令用于方法调用： invokevirtual指令:用于调用对象的实例方法，根据对象的实际类型进行分派(虚方法分派)， 这也是Java语言中最常见的方法分派方式。 invokeinterface指令:用于调用接口方法，它会在运行时搜索一个实现了这个接口方法的对象，找出适合的方法进行调用。 invokespecial指令:用于调用一些需要特殊处理的实例方法，包括实例初始化方法、私有方法和父类方法。 invokestatic指令:用于调用类静态方法(static方法)。 invokedynamic指令:用于在运行时动态解析出调用点限定符所引用的方法。并执行该方法。前面四条调用指令的分派逻辑都固化在Java虚拟机内部，用户无法改变，而invokedynamic指令的分派逻辑是由用户所设定的引导方法决定的。 ​ 方法调用指令与数据类型无关，而方法返回指令是根据返回值的类型区分的，包括ireturn(当返回值是boolean、byte、char、short和int类型时使用)、lreturn、freturn、dreturn和areturn，另外还有一 条return指令供声明为void的方法、实例初始化方法、类和接口的类初始化方法使用。 2.9 异常处理指令​ 在Java程序中显式抛出异常的操作(throw语句)都由athrow指令来实现。而在Java虚拟机中，处理异常(catch语句)不是由字节码指令来实现的，而是采用异常表来完成。 2.10 同步指令​ Java虚拟机可以支持方法级的同步和方法内部一段指令序列的同步，这两种同步结构都是使用管程(Monitor，更常见的是直接将它称为“锁”)来实现的。 ​ 方法级的同步是隐式的，无须通过字节码指令来控制，它实现在方法调用和返回操作之中。虚拟机可以从方法常量池中的方法表结构中的ACC_SYNCHRONIZED访问标志得知一个方法是否被声明为同步方法。 ​ 同步一段指令集序列通常是由Java语言中的synchronized语句块来表示的，Java虚拟机的指令集中有monitorenter和monitorexit两条指令来支持synchronized关键字的语义。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"垃圾收集器与内存分配策略","slug":"垃圾收集器与内存分配策略","date":"2021-06-10T09:49:07.000Z","updated":"2021-06-17T08:23:28.556Z","comments":true,"path":"2021/06/10/垃圾收集器与内存分配策略/","link":"","permalink":"/2021/06/10/垃圾收集器与内存分配策略/","excerpt":"","text":"1.对象1.1 引用计数法​ 在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一;当引用失效时，计数器值就减一;任何时刻计数器为零的对象就是不可能再被使用的。 ​ 在Java 领域，至少主流的Java虚拟机里面都没有选用引用计数算法来管理内存，主要原因是，这个看似简单的算法有很多例外情况要考虑，必须要配合大量额外处理才能保证正确地工作，譬如单纯的引用计数就很难解决对象之间相互循环引用的问题。 1.2 可达性分析算法​ 这个算法的基本思路就是通过一系列成为“GC Roots”的根对象作为起始点节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”（Reference Chain），如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。 ​ 在Java技术体系里面，固定可作为GC Roots的对象包括以下几种： 在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。 在本地方法栈中JNI(即通常所说的Native方法)引用的对象。 Java虚拟机内部的引用，如基本数据类型所对应的Class对象，一些常驻的异常对象（比如NullPointException、OutOfMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 ​ 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整GC Roots集合。 1.3 再谈引用​ Java对引用的概念进行了扩充，将引用分为强引用(Strongly Re-ference)、软引用(Soft Reference)、弱引用(Weak Reference)和虚引用(Phantom Reference)4种，这4种引用强度依次逐渐减弱。 强引用是最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似“Object obj == new Object()”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用是用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK1.2之后，提供了SoftReference类来实现软引用。 弱引用也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK1.2之后，提供了WeakReference类来实现弱引用。 虚引用也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。在JDK1.2之后，提供了PhantomRefernce类来实现虚引用。\u0010 1.4 回收方法区​ 方法区的垃圾收集主要回收两部分内容：废弃的常量和不再使用的类型。回收废弃常量与回收 Java堆中的对象非常类似。 ​ 判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件： 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 ​ Java虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。 2. 垃圾收集算法2.1 分代收集理论​ 当前商业虚拟机的垃圾收集器，大多数都遵循了“分代收集”(Generational Collection)[1]的理论进行设计，分代收集名为理论，实质是一套符合大多数程序运行实际情况的经验法则，它建立在两个分代假说之上: ​ 1)弱分代假说(Weak Generational Hypothesis):绝大多数对象都是朝生夕灭的。 ​ 2)强分代假说(Strong Generational Hypothesis):熬过越多次垃圾收集过程的对象就越难以消亡。 ​ 把分代收集理论具体放到现在的商用Java虚拟机里，设计者一般至少会把Java堆划分为新生代(Young Generation)和老年代(Old Generation)两个区域。在新生代中，每次垃圾收集 时都发现有大批对象死去，而每次回收后存活的少量对象，将会逐步晋升到老年代中存放。 部分收集(Partial GC)：指目标不是完整收集整个Java堆的垃圾收集，其中又分为： 新生代收集(Minor GC/Young GC)：指目标只是新生代的垃圾收集。 老年代收集(Major GC/Old GC)：指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。 混合收集(Mixed GC)：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为。 整堆收集(Full GC)：收集整个Java堆和方法区的垃圾收集。 2.2 标记-清除算法​ 最早出现也是最基础的垃圾收集算法是“标记-清除”(Mark-Sweep)算法。算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存活的对象，统一回收所有未被标记的对象。 ​ 它的主要缺点有两个： 执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低； 内存空间的碎片化问题，标记、清除之后会产生大 量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 2.3 标记-复制算法​ 标记-复制算法常被简称为复制算法。为了解决标记-清除算法面对大量可回收对象时执行效率低的问题。它将可用 内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着 的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 ​ 现在的商用Java虚拟机大多都优先采用了这种收集算法去回收新生代。 ​ Appel式回收：把新生代分为一块较大的Eden空间和两块较小的 Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1，也即每次新生代中可用内存空间为整个新生代容量的90%(Eden的80%加上一个Survivor的10%)，只有一个Survivor空间，即10%的新生代是会被“浪费”的。 2.4 标记-整理算法​ 针对老年代对象的存亡特征，提出了另外一种有针对性的“标记-整理”(Mark-Compact)算法，其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 3. 经典垃圾收集器3.1 Serial收集器​ Serial收集器是最基础、历史最悠久的收集器，曾经(在JDK 1.3.1之前)是HotSpot虚拟机新生代收集器的唯一选择。这个收集器是一个单线程工作的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。 ​ 迄今为止，它依然是HotSpot虚拟机运行在客户端模式下的默认新生代收集器，有着优于其他收集器的地方，那就是简单而高效(与其他收集器的单线程相比)，对于内存资源受限的环境，它是所有收集器里额外内存消耗(Memory Footprint)最小的。 3.2 ParNew收集器​ ParNew收集器实质上是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外 ，其余的行为包括Serial收集器可用的所有控制参数 、收集算法、对象分配规则、回收策略等都与Serial收集器完全一致。ParNew是HotSpot虚拟机中第一款退出历史舞台的垃圾收集器。 3.3 Parallel Scavenge收集器​ Parallel Scavenge收集器也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是能够并行收集的多线程收集器。Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量(Throughput)。 ​ 虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。这种调节方式称为垃圾收集的自适应的调节策略(GC Ergonomics)。自适应调节策略也是Parallel Scavenge收集器区别于ParNew收集器的一个重要特性。 3.4 Serial Old收集器​ Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。 3.5 Parallel Old收集器​ Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。 3.6 CMS收集器​ CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。 ​ 它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为四个步骤，包括: ​ 1)初始标记(CMS initial mark) ​ 2)并发标记(CMS concurrent mark) ​ 3)重新标记(CMS remark) ​ 4)并发清除(CMS concurrent sweep) 3.7 Garbage First收集器​ Garbage First(简称G1)收集器是垃圾收集器技术发展历史上的里程碑式的成果，它开创了收集器面向局部收集的设计思路和基于Region的内存布局形式。 ​ G1可以面向堆内存任 何部分来组成回收集(Collection Set，一般简称CSet)进行回收，衡量标准不再是它属于哪个分代，而 是哪块内存中存放的垃圾数量最多，回收收益最大，这就是G1收集器的Mixed GC模式。 ​ G1开创的基于Region的堆内存布局是它能够实现这个目标的关键。虽然G1也仍是遵循分代收集理论设计的，但其堆内存的布局与其他收集器有非常明显的差异：G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域(Region)，每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的 Region采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果。 ​ Region中还有一类特殊的Humongous区域，专门用来存储大对象。G1认为只要大小超过了一个Region容量一半的对象即可判定为大对象。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"MySQL实战笔记-实践篇","slug":"MySQL实战笔记-实践篇","date":"2021-05-10T08:34:36.000Z","updated":"2021-07-29T09:03:08.069Z","comments":true,"path":"2021/05/10/MySQL实战笔记-实践篇/","link":"","permalink":"/2021/05/10/MySQL实战笔记-实践篇/","excerpt":"","text":"08.事务到底是隔离的还是不隔离的 begin/start transaction命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果想要马上启动一个事务，可以使用start transaction with consistent snapshot这个命令。 在这个例子中，事务C没有显示地使用begin/commit，表示这个update语句本身就是一个事务，语句完成的时候会自动提交。事务B在更新了行之后查询；事务A在一个只读事务中查询，并且时间顺序上是在事务B的查询之后。 事务B查到的k的值是3，而事务A查到的k的值是1。 在MySQL里，有两个“视图”的概念： 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view …，而它的查询方法与表一样。 另一个是InnoDB在实现MVCC时用到的一致性视图，即consistent read view，用于支持RC（Read Committed，读未提交）和RR（Repeatable Read，可重复读）隔离级别的实现。 “快照”在MVCC里是怎么工作的在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。这个快照是基于整库的。 InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。 也就是说，数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id。 如图所示，就是一个记录被多个事务连续更新后的状态。 实际上，图中的三个虚线箭头，就是undo log（回滚日志）；而V1、V2、V3并不是物理上真实存在的，而是每次需要的时候根据当前版本和undo log计算出来的。比如，需要V2的时候，就是通过V4依 次执行U3、U2算出来。 InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。 数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。 这个视图数组和高水位，就组成了当前事务的一致性视图(read-view)。 而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。 这个视图数组把所有的row trx_id分成了几种不同的情况。 这样，对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能: 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 若row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见； 若row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。 比如，对于图中的数据来说，如果有一个事务，它的低水位是18，那么当它访问这一行数据 时，就会从V4通过U3计算出V3，所以在它看来，这一行的值是11。 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况: 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 更新逻辑更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”(current read)。 除了update语句外，select语句如果加锁，也是当前读。 如果把事务A的查询语句select * from t where id=1修改一下，加上lock in share mode 或 for update，也都可以读到版本号是101的数据，返回的k的值是3。 下面这两个select语句，就是分别加了读锁(S锁，共享锁)和写锁(X锁，排他锁)。 123mysql&gt; select k from t where id=1 lock in share mode;mysql&gt; select k from t where id=1 for update; 再往前一步，假设事务C不是马上提交的，而是变成了下面的事务C’，会怎么样呢? 事务C’的不同是，更新后并没有马上提交，在它提交前，事务B的更新语句先发起了。前面说过了，虽然事务C’还没提交，但是(1,2)这个版本也已经生成了，并且是当前的最新版本。那么，事务B的更新语句会怎么处理呢? “两阶段锁协议”。事务C’没提交，也就是说(1,2) 这个版本上的写锁还没释放。而事务B是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务C’释放这个锁，才能继续它的当前读。 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 “start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的start transaction。 这时，事务A的查询语句的视图数组是在执行这个语句的时候创建的，时序上(1,2)、(1,3)的生成时间都在创建这个视图数组的时刻之前。但是，在这个时刻： (1,3)还没提交，属于情况1，不可见； (1,2)提交了，属于情况3，可见。 所以，这时候事务A查询语句返回的是k=2。 显然地，事务B查询结果k=3。 09.普通索引和唯一索引 查询过程假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先 是通过B+树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 这个不同带来的性能差距是微乎其微。 InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候， 并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每 个数据页的大小默认是16KB。 因为引擎是按页读写的，所以说，当找到k=5的记录的时候，它所在的数据页就都在内存里了。 那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。 当然，如果k=5这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下 一个数据页，这个操作会稍微复杂一些。 对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概 率会很低。所以，计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以忽略不计。 更新过程当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 虽然名字叫做change buffer，实际上它也是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。 将change buffer中的操作应用到原始数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。 如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。 唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。 change buffer用的是buffer pool里的内存，因此不能无限增大。change buffer的大小，可以通 过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。 将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。 change buffer的使用场景对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问 IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。 索引选择和实践普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上 是没差别的，主要考虑的是对更新性能的影响。所以尽量选择普通索引。 如果所有的更新后面，都马上伴随着对这个记录的查询，那么应该关闭change buffer。而在 其他情况下，change buffer都能提升更新性能。 在实际使用中，会发现普通索引和change buffer的配合使用，对于数据量大的表的更新优 化还是很明显的。 特别地，在使用机械硬盘时，change buffer这个机制的收效是非常显著的。 Change buffer和redo log1mysql&gt; insert into t(id,k) values(id1,k1),(id2,k2); 假设当前k索引树的状态，查找到位置后，k1所在的数据页在内存(InnoDB buffer pool)中，k2所在的数据页不在内存中。如图所示是带change buffer的更新状态图。 分析这条更新语句，会发现它涉及了四个部分:内存、redo log(ib_log_fileX)、 数据表空间 (t.ibd)、系统表空间(ibdata1)。 这条更新语句做了如下的操作(按照图中的数字顺序): Page1在内存中，直接更新内存； Page2没有在内存中，就在内存的changebuffer区域，记录下“我要往Page2插入一行”这个 信息 ； 将上述两个动作记入redolog中(图中3和4)。 做完上面这些，事务就可以完成了。所以，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘(两次操作合在一起写了一次磁盘)，而且还是顺序写的。 同时，图中的两个虚线箭头，是后台操作，不影响更新的响应时间。 在这之后读请求的处理方式，现在要执行 select * from t where k in (k1, k2)。 如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间(ibdata1)和 redo log(ib_log_fileX)无关了。 . 读Page1的时候，直接从内存返回。 要读Page2的时候，需要把Page2从磁盘读入内存中，然后应用changebuffer里面的操作 日志，生成一个正确的版本并返回结果。 redo log 主要节省的是随机写磁盘的IO消耗(转成顺序写)，而change buffer主要节省的则是随机读磁盘的IO消耗。 10.MySQL为什么有时候会选错索引优化器的逻辑而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少。 扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。 MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。 这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越 好。而一个索引上不同的值的个数，我们称之为“基数”(cardinality)。也就是说，这个基数越 大，索引的区分度越好。 MySQL选错索引，原因是没能准确地判断出扫描行数。analyze table t 命令，可以用来重新统计索引信息。 索引选择异常和处理大多数时候优化器都能找到正确的索引。原本可以执行得很快的SQL语句，执行速度却比预期的慢很多的解决办法： 采用force index强行选择一个索引。 修改语句，引导MySQL使用期望的索引。 在有些场景下，可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。 11.给字符串字段加索引MySQL是支持前缀索引的，也就是说，可以定义字符串的一部分作为索引。 123mysql&gt; alter table SUser add index index1(email);或mysql&gt; alter table SUser add index index2(email(6)); 第一个语句创建的index1索引里面，包含了每个记录的整个字符串；而第二个语句创建的index2 索引里面，对于每个记录都是只取前6个字节。 如果使用的是index1(即email整个字符串的索引结构)，执行顺序是这样的: 从index1索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得ID2的值; 到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集; 取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足 email=&#39;zhangssxyz@xxx.com’的条件了，循环结束。 如果使用的是index2(即email(6)索引结构)，执行顺序是这样的: 从index2索引树找到满足索引值是’zhangs’的记录，找到的第一个是ID1; 到主键上查到主键值是ID1的行，判断出email的值不是’zhangssxyz@xxx.com’，这行记录丢 弃; 取index2上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出ID2，再到ID索引上取 整行然后判断，这次值对了，将这行记录加入结果集; 重复上一步，直到在idxe2上取到的值不是’zhangs’时，循环结束。 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 前缀索引对覆盖索引的影响1234当前这个SQL语句select id,email from SUser where email=&apos;zhangssxyz@xxx.com&apos;;与前面例子中的SQL语句select id,name,email from SUser where email=&apos;zhangssxyz@xxx.com&apos;; 相比，这个语句只要求返回id和email字段。 所以，如果使用index1(即email整个字符串的索引结构)的话，可以利用覆盖索引，从index1查 到结果后直接就返回了，不需要回到ID索引再去查一次。而如果使用index2(即email(6)索引结 构)的话，就不得不回到ID索引再去判断email字段的值。 即使你将index2的定义修改为email(18)的前缀索引，这时候虽然index2已经包含了所有的信息， 但InnoDB还是要回到id索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。 也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是在选择是否使用前缀索引时需要考虑的一个因素。 其他方式对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时： 第一种方式是使用倒序存储。如果存储身份证号的时候把它倒过来存，每次查询的时候，你 可以这么写： 1select field_list from t where id_card=reverse(&apos;input_id_card_string&apos;); 第二种方式是使用hash字段。可以在表上再创建一个整数字段，来保存身份证的校验码， 同时在这个字段上创建索引。 1alter table t add id_card_crc int unsigned, add index(id_card_crc); 然后每次插入新记录的时候，都同时用crc32()这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过crc32()函数得到的结果可能是相同的，所以查询语句where部分要判断id_card的值是否精确相同。 1select field_list from t where id_card_crc=crc32(&apos;input_id_card_string&apos;) and id_card=&apos;i 首先，它们的相同点是，都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的 方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了。同样地，hash字段的方式也只能支持等值查询。 它们的区别，主要体现在以下三个方面: 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字 段方法需要增加一个字段。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个hash字段也差不多抵消了。 在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash 字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的话，reverse函数额外消耗的CPU资源会更小些。 从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽 然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。 12.InnoDB脏页当内存数据页跟磁盘数据页内容不一致的时候，这个内存页就是“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 不论是脏页还是干净页，都在内存中。 引发数据库flush（刷脏页）\u0010的情况： InnoDB的redo log写满了。这时候系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写。 系统内存不足，当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。如果刷脏页一定会写盘，就保证了每个数据页有两种状态： 一种是内存里存在，内存里就肯定是正确的结果，直接返回； 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。这样的效率最高。 MySQL认为系统“空闲”的时候。 MySQL正常关闭的时候，MySQL会把内存的脏页都flush到磁盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 InnoDB用缓冲池 (buffer pool)管理内存，缓冲池中的内存页有三种状态: 第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。 而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久 不使用的数据页从内存中淘汰掉；如果要淘汰的是一个干净页，就直接释放出来复用;但如果是脏页，就必须将脏页先刷到磁盘，变成干净页后才能复用。 所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的: 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长； 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。 所以，InnoDB需要有控制脏页比例的机制，来尽量避免上面的这两种情况。 InnoDB刷脏页的控制策略首先，要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时 候，可以刷多快。 这就要用到innodb_io_capacity这个参数了，它会告诉InnoDB你的磁盘能力。这个值建议设置成磁盘的IOPS。磁盘的IOPS可以通过fio这个工具来测试，下面的语句是用来测试磁盘随机读写的命令: 1fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size 参数innodb_max_dirty_pages_pct是脏页比例上限，默认值是75%。InnoDB会根据当前的脏页 比例(假设为M)，算出一个范围在0到100之间的数字。 InnoDB每次写入的日志都有一个序号，当前写入的序号跟checkpoint对应的序号之间的差值， 假设为N。InnoDB会根据这个N算出一个范围在0到100之间的数字，这个计算公式可以记为 F2(N)。F2(N)算法比较复杂，只要知道N越大，算出来的值越大就好了。 然后，根据上述算得的F1(M)和F2(N)两个值，取其中较大的值记为R，之后引擎就可以按照innodb_io_capacity定义的能力乘以R%来控制刷脏页的速度。 要合理地设置innodb_io_capacity的值，并且平时要多关注脏页比例，不要让它经常接近75%。 一旦一个查询请求需要在执行过程中先flush掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL中的一个机制，可能让你的查询会更慢:在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉;而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在InnoDB中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为1的时候会有上述 的“连坐”机制，值为0时表示不找邻居，自己刷自己的。 找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机IO。机械硬盘的随机IOPS 一般只有几百，相同的逻辑操作减少随机IO就意味着系统性能的大幅度提升。 在MySQL 8.0中，innodb_flush_neighbors参数的默认值已经是0了。 13.表数据删掉一般，表文件大小不变一个InnoDB表包含两部分，即：表结构定义和数据。在MySQL 8.0版本以前，表结构是存在以.fm为后缀的文件里。而MySQL 8.0版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间小，所以主要是表数据。 参数innodb_file_per_table表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数innodb_file_per_table控制的： 这个参数设置为OFF表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为ON表示的是，每个InnoDB表数据存储在一个以.ibd为后缀的文件中。 从MySQL 5.6.6版本开始，它的默认值就是ON了。 推荐做法为，不论使用MySQL的哪个版本，都将这个值设置为ON。因为，一个表单独存储为一个文件更容易管理，而且在不需要这个表的时候，通过drop table命令，系统会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。 在删除整个表的时候，可以使用drop table命令回收表空间。但是，遇到的更多的删除数据的场景是删除某些行，这时就遇到了问题：表中的数据被删除了，但是表空间却没有被回收。 数据删除流程InnoDB里的数据都是用B+树的结构组织的。 假设，要删掉R4这个记录，InnoDB引擎只会把R4这个记录标记为删除。如果之后要再插入 一个ID在300和600之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。 InnoDB的数据是按页存储的，那么如果删掉了一个数据页上的所有记录，整个数据页就可以被复用了。 数据页的复用跟记录的复用是不同的。 记录的复用，只限于符合范围条件的数据。 而当整个页从B+树里面摘掉以后，可以复用到任何位置。 不止是删除数据会造成空洞，插入数据也会。 经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。 而重建表，就可以达到这样的目的。 重建表如果现在有一个表A，需要做空间收缩，为了把表中存在的空洞去掉，可以新建一个与表A结构相同的表B，然后按照主键ID递增的顺序，把数据一行一行地从表A 里读出来再插入到表B中。 由于表B是新建的表，所以表A主键索引上的空洞，在表B中就都不存在了。显然地，表B的主键索引更紧凑，数据页的利用率也更高。如果把表B作为临时表，数据从表A导入表B的操作完成后，用表B替换A，从效果上看，就起到了收缩表A空间的作用。 这里，可以使用alter table A engine=InnoDB命令来重建表。在MySQL 5.5版本之前，这个命令的执行流程跟前面描述的差不多，区别只是这个临时表B不需要自己创建，MySQL会自动完成转存数据、交换表名、删除旧表的操作。 花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表A的话，就会造成数据丢失。因此，在整个DDL过程中，表A中不能有更新。也就是说，这个 DDL不是Online的。 而在MySQL 5.6版本开始引入的Online DDL，对这个操作流程做了优化。 引入了Online DDL之后，重建表的流程: 建立一个临时文件，扫描表A主键的所有数据页； 用数据页中表A的记录生成B+树，存储到临时文件中； 生成临时文件的过程中，将所有对A的操作记录在一个日志文件(rowlog)中，对应的是图 中state2的状态； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表A相同的数据文件，对应的就是图中state3的状态； 用临时文件替换表A的数据文件。 对于一个大表来说，Online DDL最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执 行期间可以接受增删改操作。所以，相对于整个DDL过程来说，锁的时间非常短。对业务来说， 就可以认为是Online的。 Online 和 inplace在图3中，表A中的数据导出来的存放位置叫作tmp_table。这是一个临时表，是在server层创建的。 在图4中，根据表A重建出来的数据是放在“tmp_file”里的，这个临时文件是InnoDB在内部创建出来的。整个DDL过程都在InnoDB内部完成。对于server层来说，没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。 重建表的这个语句alter table t engine=InnoDB，其实隐含的意思是： 1alter table t engine=innodb,ALGORITHM=inplace; 跟inplace对应的就是拷贝表的方式了，用法是： 1alter table t engine=innodb,ALGORITHM=copy; 当使用ALGORITHM=copy的时候，表示的是强制拷贝表，对应的流程就是图3的操作过程。 如果说这两个逻辑之间的关系是什么的话，可以概括为： DDL过程如果是Online的，就一定是inplace的； 反过来未必，也就是说inplace的DDL，有可能不是Online的。截止到MySQL8.0，添加全文 索引(FULLTEXT index)和空间索引(SPATIAL index)就属于这种情况。 使用optimize table、analyze table和alter table这三种方式重建表的区别： 从MySQL 5.6版本开始，alter table t engine = InnoDB(也就是recreate)默认的就是上面图4 的流程了； analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程 中加了MDL读锁； optimize table t 等于recreate+analyze。 14.count(*)count(*)的实现方式在不同的MySQL引擎中，count(*)有不同的实现方式。 MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高； 而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 如果加了where 条件的话，MyISAM表也是不能返回得这么快的。 InnoDB不跟MyISAM一样，把数字存起来的原因：这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。 MyISAM表虽然count(*)很快，但是不支持事务； show table status命令虽然返回很快，但是不准确； InnoDB表直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。 不同的count用法count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。 所以，count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；而count(字 段)，则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数。 对于count(主键id)来说，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。 对于count(1)来说，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。 对于count(字段)来说： 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。 但是count(*)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。 所以结论是：按照效率排序的话，count(字段)&lt;count(主键id)&lt;count(1)≈count(*)。 15.order by1select city,name,age from t where city=&apos;杭州&apos; order by name limit 1000 ; 全字段排序在city字段上创建索引之后，用explain命令来看看这个语句的执行情况。 Extra这个字段中的“Using filesort”表示的就是需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。 从图中可以看到，满足city=’杭州’条件的行，是从ID_X到ID_(X+N)的这些记录。 通常情况下，这个语句执行流程如下所示 : 初始化sort_buffer，确定放入name、city、age这三个字段; 从索引city找到第一个满足city=’杭州’条件的主键id，也就是图中的ID_X; 到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中; 从索引city取下一个记录的主键id; 重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y; 对sort_buffer中的数据按照字段name做快速排序; 按照排序结果取前1000行返回给客户端。 这个排序过程，称为全字段排序，执行流程的示意图如下所示。 图中“按name排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数sort_buffer_size。 sort_buffer_size，就是MySQL为排序开辟的内存(sort_buffer)的大小。如果要排序的数据量 小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不 利用磁盘临时文件辅助排序。 rowid排序在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件 中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么sort_buffer里面 要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。 来修改一个参数，让MySQL采用另外一种算法。 1SET max_length_for_sort_data = 16; max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL就认为单行太大，要换一个算法。 新的算法放入sort_buffer的字段，只有要排序的列(即name字段)和主键id。 但这时，排序的结果就因为少了city和age字段的值，不能直接返回了，整个执行流程就变成如下所示的样子: 初始化sort_buffer，确定放入两个字段，即name和id; 从索引city找到第一个满足city=’杭州’条件的主键id，也就是图中的ID_X; 到主键id索引取出整行，取name、id这两个字段，存入sort_buffer中; 从索引city取下一个记录的主键id; 重复步骤3、4直到不满足city=’杭州’条件为止，也就是图中的ID_Y; 对sort_buffer中的数据按照字段name进行排序; 遍历排序结果，取前1000行，并按照id的值回到原表中取出city、name和age三个字段返回 给客户端。 这个执行流程的示意图如下，称为rowid排序。 对比图3的全字段排序流程图会发现，rowid排序多访问了一次表t的主键索引，就是步骤7。 需要说明的是，最后的“结果集”是一个逻辑概念，实际上MySQL服务端从排序后的sort_buffer中依次取出id，然后到原表查到city、name和age这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。 全字段排序 VS rowid排序如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。 如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这 样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 这也就体现了MySQL的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。 如果能够保证从city这个索引上取出来的行，天然就是按照name递增排序的话， 就可以不用再排序了。增加联合索引。 1alter table t add index city_user(city, name); 16.如何正确地显示随机消息内存临时表1mysql&gt; select word from words order by rand() limit 3; 语句的意思很直白，随机排序取前3个。虽然这个SQL语句写法很简单，但执行流程却有点 复杂的。 Extra字段显示Using temporary，表示的是需要使用临时表;Using filesort，表示的是需要执行 排序操作。 因此这个Extra的意思就是，需要临时表，并且需要在临时表上排序。 order by rand()使用了内存临时表，内存临时表排序的时候使 用了rowid排序方法。 磁盘临时表tmp_table_size这个配置限制了内存临时表的大小，默认值是16M。如果临时表大小超过了tmp_table_size，那么内存临时表就会转成磁盘临时表。 磁盘临时表使用的引擎默认是InnoDB，是由参数internal_tmp_disk_storage_engine控制的。 随机排序方法使用随机函数 17.幻读幻读是什么 幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。 上面sessionB的修改结果，被sessionA之后的select语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。 幻读有什么问题首先是语义上的。session A在T1时刻就声明了，“我要把所有d=5的行锁住，不准别的事务进行读写操作”。而实际上，这个语义被破坏了。 其次，是数据一致性的问题。 锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。 即使把所有的记录都加上锁，还是阻止不了新插入的记录。 如何解决幻读产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB只好引入新的锁，也就是间隙锁(Gap Lock)。 间隙锁，锁的就是两个值之间的空隙。比如文章开头的表t，初始化插入了6个记录， 这就产生了7个间隙。 这样，当执行 select * from t where d=5 for update的时候，就不止是给数据库中已有的6个记录加上了行锁，还同时加了7个间隙锁。这样就确保了无法再插入新的记录。 数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟之前碰到过的锁都不太一样。 比如行锁，分成读锁和写锁。下图就是这两种类型行锁的冲突关系。 也就是说，跟行锁有冲突关系的是“另外一个行锁”。 但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操 作。间隙锁之间都不存在冲突关系。 间隙锁和行锁合称next-key lock，每个next-key lock是前开后闭区间。也就是说，表t初始化以后，如果用select * from t for update要把整个表所有记录锁起来，就形成了7个next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。 间隙锁和next-key lock的引入，帮我们解决了幻读的问题，但同时也带来了一些“困扰”。 间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。 间隙锁是在可重复读隔离级别下才会生效的。所以，如果把隔离级别设置为读提交的话， 就没有间隙锁了。 18.加锁规则 原则1：加锁的基本单位是next-key lock。next-key lock是前开后闭区间。 原则2：查找过程中访问到的对象才会加锁。 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 19.MySQL提高性能的方法(短期、临时)短链接风暴正常的短连接模式就是连接到数据库后，执行很少的SQL语句就断开，下次需要的时候再重连。 如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。 MySQL建立连接的过程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。 在数据库压力比较小的时候，这些额外的成本并不明显。 但是，短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨max_connections参数，用来控制一个MySQL实例同时存在的连接数的上限，超过这个值，系统 就会拒绝接下来的连接请求，并报错提示“Too many connections”。对于被拒绝连接的请求来 说，从业务角度看就是数据库不可用。 碰到这种情况时，一个比较自然的想法，就是调高max_connections的值。但这样做是有风险 的。因为设计max_connections这个参数的目的是想保护MySQL，如果把它改得太大，让更 多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑 上，结果可能是适得其反，已经连接的线程拿不到CPU资源去执行业务的SQL请求。 其他的方法： 先处理掉那些占着连接都是不工作的线程。从服务端断开连接使用的是kill connection + id的命令， 一个客户端处于sleep状态时，它的连接被服务端主动断开后，这个客户端并不会马上知道。直到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。 减少连接过程的消耗。 慢查询性能问题在MySQL中，会引发性能问题的慢查询，大体有以下三种可能: 索引没有设计好; SQL语句没写好; MySQL选错了索引。 导致慢查询的第一种可能是，索引没有设计好。 这种场景一般就是通过紧急创建索引来解决。MySQL 5.6版本以后，创建索引都支持Online DDL 了，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行alter table 语句。 导致慢查询的第三种可能是，MySQL选错了索引。 这时候，应急方案就是给这个语句加上force index。 QPS突增问题有时候由于业务突然出现高峰，或者应用程序bug，导致某个语句的QPS突然暴涨，也可能导致 MySQL压力过大，影响服务。 20.MySQL是怎么保证数据不丢的binlog的写入机制binlog的写入逻辑比较简单：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。 一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了binlog cache的保存问题。 系统给binlog cache分配了一片内存，每个线程一个，参数binlog_cache_size用于控制单个线程内binlog cache所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 事务提交的时候，执行器把binlog cache里的完整事务写入到binlog中，并清空binlog cache。 每个线程有自己binlog cache，但是共用同一份binlog文件。 图中的write，指的就是把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。 图中的fsync，才是将数据持久化到磁盘的操作。一般情况下，认为fsync才占磁盘的IOPS。 write和fsync的时机，是由参数sync_binlog控制的： sync_binlog=0的时候，表示每次提交事务都只write，不fsync； sync_ binlog=1的时候，表示每次提交事务都会执行fsync； sync_binlog=N(N&gt;1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。 因此，在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。在实际 的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成0，比较常见的是将其 设置为100~1000中的某个数值。 但是，将sync_binlog设置为N，对应的风险是:如果主机发生异常重启，会丢失最近N个事务的 binlog日志。 redo log的写入机制事务在执行过程中，生成的redo log是要先写到redo log buffer的。 redo log buffer里面的内容，不需要每次生成后都要直接持久化到磁盘。 事务还没提交的时候，redo log buffer中的部分日志有可能被持久化到磁盘。 这三种状态分别是： 存在redo log buffer中，物理上是在MySQL进程内存中，就是图中的红色部分； 写到磁盘(write)，但是没有持久化(fsync)，物理上是在文件系统的page cache里面，也就是图中的黄色部分； 持久化到磁盘，对应的是hard disk，也就是图中的绿色部分。 日志写到redo log buffer是很快的，wirte到page cache也差不多，但是持久化到磁盘的速度就慢多了。 为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数，它有三种可能取值: 设置为0的时候，表示每次事务提交时都只是把redolog留在redo log buffer中； 设置为1的时候，表示每次事务提交时都将redolog直接持久化到磁盘； 设置为2的时候，表示每次事务提交时都只是把redolog写到pagecache。 InnoDB有一个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的 page cache，然后调用fsync持久化到磁盘。 事务执行中间过程的redo log也是直接写在redo log buffer中的，这些redo log也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的redo log，也是可能已经持久化到磁盘的。 实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中。 一种是，redo log buffer占用的空间即将达到 innodb_log_buffer_size一半的时候， 后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是write，而没有调用fsync，也就是只留在了文件系统的page cache。 另一种是，并行的事务提交的时候，顺带将这个事务的redo log buffer持久化到磁盘。假设一个事务A执行到一半，已经写了一些redo log到buffer中，这时候有另外一个线程的事务B提交，如果innodb_flush_log_at_trx_commit设置的是1，那么按照这个参数的逻辑，事务B要把redo log buffer里的日志全部持久化到磁盘。这时候，就会带上事务A在redo log buffer里的日志一起持久化到磁盘。 通常说MySQL的“双1”配置，指的就是sync_binlog和innodb_flush_log_at_trx_commit都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是redo log(prepare阶段)，一次是binlog。 日志逻辑序列号(log sequence number，LSN)。LSN是单调 递增的，用来对应redo log的一个个写入点。每次写入长度为length的redo log， LSN的值就会加 上length。 LSN也会写到InnoDB的数据页中，来确保数据页不会被多次执行重复的redo log。 WAL机制主要得益于两个方面: redo log 和 binlog都是顺序写，磁盘的顺序写比随机写速度要快; 组提交机制，可以大幅度降低磁盘的IOPS消耗。 21.MySQL是如何保持主备一致的MySQL主备的基本原理 在状态1中，客户端的读写都直接访问节点A，而节点B是A的备库，只是将A的更新都同步过来，到本地执行。这样可以保持节点B和A的数据是相同的。 当需要切换的时候，就切成状态2。这时候客户端读写访问的都是节点B，而节点A是B的备库。 在状态1中，虽然节点B没有被直接访问，但是依然建议把节点B(也就是备库)设置成只读 (readonly)模式。这样做，有以下几个考虑: 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作; 防止切换逻辑有bug，比如切换过程中出现双写，造成主备不一致; 可以用readonly状态，来判断节点的角色。 备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的: 在备库B上通过changemaster命令，设置主库A的IP、端口、用户名、密码，以及要从哪个 位置开始请求binlog，这个位置包含文件名和日志偏移量。 在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和 sql_thread。其中io_thread负责与主库建立连接。 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。 备库B拿到binlog后，写到本地文件，称为中转日志(relaylog)。 sql_thread读取中转日志，解析出日志里的命令，并执行。 binlog的三种格式对比binlog有两种格式，一种是statement，一种是row。第三种格式，叫作mixed，其实它就是前两种格式的混合。 由于statement格式下，记录到binlog里的是语句原文，因此可能会出现这样一种情况:在主库 执行这条SQL语句的时候，用的是索引a;而在备库执行这条SQL语句的时候，却使用了索引 t_modified。因此，MySQL认为这样写是有风险的。 当binlog_format使用row格式的时候，binlog里面记录了真实删除行的主键id，这样binlog传到备库去的时候，就肯定会删除对应的行，不会有主备删除不同行的问题。 为什么会有mixed格式的binlog?为什么会有mixed这种binlog格式的存在场景?推论过程是这样的: 因为有些statement格式的binlog可能会导致主备不一致，所以要使用row格式。 但row格式的缺点是，很占空间。比如用一个delete语句删掉10万行数据，用statement的 话就是一个SQL语句被记录到binlog中，占用几十个字节的空间。但如果用row格式的binlog， 就要把这10万条记录都写到binlog中。这样做，不仅会占用更大的空间，同时写binlog也要耗 费IO资源，影响执行速度。 所以，MySQL就取了个折中方案，也就是有了mixed格式的binlog。mixed格式的意思 是，MySQL自己会判断这条SQL语句是否可能引起主备不一致，如果有可能，就用row格式， 否则就用statement格式。 也就是说，mixed格式可以利用statment格式的优点，同时又避免了数据不一致的风险。 现在越来越多的场景要求把MySQL的binlog格式设置成row。这么做的理由有很多，一个可以直接看出来的好处：恢复数据。 循环复制问题 实际生产上使用比较多的是双M结构。 双M结构和M-S结构，其实区别只是多了一条线，即:节点A和B之间总是互为主备关系。这样在切换的时候就不用再修改主备关系。 如果节点A同时是节点B的备库，相当于又把节点B新生成的binlog拿过来执行了一次，然 后节点A和B间，会不断地循环执行这个更新语句，也就是循环复制了。这个要怎么解决? MySQL在binlog中记录了这个命令第一次执行时所在实例的server id。因此，可以用下面的逻辑，来解决两个节点间的循环复制的问题: 规定两个库的serverid必须不同，如果相同，则它们之间不能设定为主备关系; 一个备库接到binlog并在重放的过程中，生成与原binlog的serverid相同的新的binlog; 每个库在收到从自己的主库发过来的日志后，先判断serverid，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。 按照这个逻辑，如果设置了双M结构，日志的执行流就会变成这样: 从节点A更新的事务，binlog里面记的都是A的serverid; 传到节点B执行一次以后，节点B生成的binlog的serverid也是A的serverid; 再传回给节点A，A判断到这个serverid与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了。 22.MySQL是怎么保证高可用的正常情况下，只要主库执行更新生成的所有binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。 主备延迟主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值。 在备库上执行show slave status命令，它的返回结果里面会显示seconds_behind_master，用于表示当前备库延迟了多少秒。 网络正常情况下，主备延迟的主要来源是备库接收完binlog和执行完这个事务之间的时间差。 主备延迟的来源首先，有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。 当然，这种部署现在比较少了。因为主备可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器，并且做对称部署，是现在比较常见的情况。 第二种常见的可能，即备库的压力大。 这种情况，一般可以这么处理: 一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。 通过binlog输出到外部系统，比如Hadoop这类系统，让外部系统提供统计类查询的能力。 第三种可能，即大事务。另一种典型的大事务场景，就是大表DDL。 造成主备延迟还有一个大方向的原因，就是备库的并行复制能力。 可靠性优先策略双M结构下，从状态1到状态2切换的详细过程是这样的: 判断备库B现在的seconds_behind_master，如果小于某个值(比如5秒)继续下一步，否则 持续重试这一步; 把主库A改成只读状态，即把readonly设置为true; 判断备库B的seconds_behind_master的值，直到这个值变成0为止; 把备库B改成可读写状态，也就是把readonly设置为false; 把业务请求切到备库B。 这个切换流程，一般是由专门的HA系统来完成的，称之为可靠性优先流程。 可用性优先策略如果强行把步骤4、5调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库 B，并且让备库B可以读写，那么系统几乎就没有不可用时间了。 把这个切换流程，称作可用性优先流程。这个切换流程的代价，就是可能出现数据不一 致的情况。 使用row格式的binlog时，数据不一致的问题更容易被发现。而使用mixed或者statement格式的binlog时，数据很可能悄悄地就不一致了。 主备切换的可用性优先策略会导致数据不一致。因此，大多数情况下，都建议使用可靠 性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。 23.全表扫描全表扫描对server层的影响对InnoDB表db.t，执行一个全表扫描。要把扫描结果保存在客户端，会使用类似这样的命令： 1mysql -h$host -P$port -u$user -p$pwd -e &quot;select * from db1.t&quot; &gt; $target_file InnoDB的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表t的主键索引。这条查询语句由于没有其他的判断条件，所以查到的每一行都可以直接放到结果集里面，然后返回给客户端。 服务端并不需要保存一个完整的结果集。取数据和发数据的流程是这样的： 获取一行，写到net_buffer中。这块内存的大小是由参数net_buffer_length定义的，默认是16k。 重复获取行，直到net_buffer写满，调用网络接口发出去。 如果发送成功，就清空net_buffer，然后获取下一行，并写入net_buffer。 如果发送函数返回EAGAIN或WSAEWOULDBLOCK，就表示本地网络栈（socket and buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。 MySQL是“边读边发的”。这就意味着，如果客户端接收得慢，会导致MySQL服务端由于结果发不出去，这个事务的执行时间变长。 查询的结果是分段发给客户端的，因此扫描全表，查询返回大量的数据，并不会把内存打爆。 全表扫描对InnoDB的影响内存的数据页是在Buffer Pool (BP)中管理的，在WAL里Buffer Pool 起到了加速更新的作用。而 实际上，Buffer Pool 还有一个更重要的作用，就是加速查询。 而Buffer Pool对查询的加速效果，依赖于一个重要的指标，即：内存命中率。 执行show engine innodb status ，可以看到“Buffer pool hit rate”字样，显示的就是当前的命中 率。下图这个命中率，就是99.0%。 InnoDB内存管理用的是最近最少使用 (Least Recently Used, LRU)算法，这个算法的核心就是淘汰最久未使用的数据。实际上，InnoDB对LRU算法做了改进。 24.join假设t2插入了1000行数据，在表t1里插入的是100行数据。这两个表都有一个主键索引id和一个索引a，字段b上无索引。 Index Nested-Loop Join1select * from t1 straight_join t2 on (t1.a=t2.a); 如果直接使用join语句，MySQL优化器可能会选择表t1或t2作为驱动表，这样会影响我们分析 SQL语句的执行过程。所以，为了便于分析执行过程中的性能问题，改用straight_join让 MySQL使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去join。在这个语句里，t1 是驱动表，t2是被驱动表。 因此这个语句的执行流程是这样的: 从表t1中读入一行数据 R; 从数据行R中，取出a字段到表t2里去查找; 取出表t2中满足条件的行，跟R组成一行，作为结果集的一部分; 重复执行步骤1到3，直到表t1的末尾循环结束。 这个过程是先遍历表t1，然后根据从表t1中取出的每行数据中的a值，去表t2中查找满足条件的 记录。在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称NLJ。 在这个流程里: 对驱动表t1做了全表扫描，这个过程需要扫描100行; 而对于每一行R，根据a字段去表t2查找，走的是树搜索过程。由于我们构造的数据都是一一对应的，因此每次的搜索过程都只扫描一行，也是总共扫描100行; 所以，整个执行流程，总扫描行数是200。 所以可以得出结论： 使用join语句，性能比强行拆成多个单表执行SQL语句的性能要好; 如果使用join语句的话，需要让小表做驱动表。 这个结论的前提是“可以使用被驱动表的索引”。 Simple Nested-Loop Join1select * from t1 straight_join t2 on (t1.a=t2.b); 由于表t2的字段b上没有索引，因此再用图2的执行流程时，每次到t2去匹配的时候，就要做一次 全表扫描。这个算法是正确的，而且这个算法也有一个名字，叫做“Simple Nested-Loop Join”。 MySQL也没有使用这个Simple Nested-Loop Join算法，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称BNL。 Block Nested-Loop Join这时候，被驱动表上没有可用的索引，算法的流程是这样的: 把表t1的数据读入线程内存join_buffer中，由于我们这个语句中写的是select *，因此是把整个表t1放入了内存; 扫描表t2，把表t2中的每一行取出来，跟join_buffer中的数据做对比，满足join条件的，作为结果集的一部分返回。 如果使用Simple Nested-Loop Join算法进行查询，扫描行数也是10万行。因此，从时间复杂度上来说，这两个算法是一样的。但是，Block Nested-Loop Join算法的这10万次判断是内存操作，速度上会快很多，性能也更好。 在join_buffer_size足够大的时候，是一样的; 在join_buffer_size不够大的时候(这种情况更常见)，应该选择小表做驱动表。 在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。 25.临时表 内存表，指的是使用Memory引擎的表，建表语法是create table … engine=memory。这种表的数据都保存在内存里，系统启动的时候会被清空。但是表结构还在。 临时表，可以使用各种引擎类型。如果是使用InnoDB引擎或者MyISAM引擎的临时表，写 数据的时候是写到磁盘上的。当然，临时表也可以使用Memory引擎。 临时表的特性 建表语法是create temporay table…。 一个临时表只能被创建它的session访问，对其他线程不可见。 临时表可以与普通表同名。 session A内有同名的临时表和普通表的时候，show create语句，以及增删改查语句访问的是临时表。 show tables命令不显示临时表。 临时表就特别适合join优化这种场景的原因： 不同session的临时表是可以重名的，如果有多个session同时执行join优化，不需要担心表名重复导致建表失败的问题。 不需要担心数据删除问题。如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。而临时表由于会自动回收，所以不需要这个额外的操作。 为什么临时表可以重名1create temporary table temp_t(id int primary key)engine=innodb; 执行这个语句的时候，MySQL要给这个InnoDB创建一个frm文件保存结构定义，还要有地方保存表数据。 这个frm文件放在临时文件目录下，文件名的后缀是.frm，前缀是“sql{进程id}_ {线程id}_序列号”。可以使用select @@tmpdir命令，来显示实例的临时文件目录。 而关于表中数据的存放方式，在不同的MySQL版本中有着不同的处理方式： 在5.6以及之前的版本里，MySQL会在临时文件目录下创建一个相同前缀、以.ibd为后缀的文件，用来存放数据文件； 而从5.7版本开始，MySQL引入了一个临时文件表空间，专门用来存放临时文件的数据。因此，就不需要再创建ibd文件了。 MySQL维护数据表，除了物理上要有文件外，内存里面也有一套机制区别不同的表，每个表都对应一个table_def_key。 一个普通表的table_def_key的值是由“库名+表名”得到的，所以如果你要在同一个库下创建两个同名的普通表，创建第二个表的过程中就会发现table_def_key已经存在了。 而对于临时表，table_def_key在“库名+表名”基础上，又加入了“server_id+thread_id”。 在实现上，每个线程都维护了自己的临时表链表。这样每次session内操作表的时候，先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表，如果没有再操作普通表；在 session结束的时候，对链表里的每个临时表，执行 “DROP TEMPORARY TABLE +表名”操作。 这时候会发现，binlog中也记录了DROP TEMPORARY TABLE这条命令。 临时表和主备复制如果当前的binlog_format=row，那么跟临时表有关的语句，就不会记录到binlog里。也就是说，只在binlog_format=statment/mixed 的时候，binlog中才会记录临时表的操作。 MySQL在记录binlog的时候，会把主库执行这个语句的线程id写到binlog中。这样，在备库的应用线程就能够知道执行每个语句的主库线程id，并利用这个线程id来构造临时表的 table_def_key： session A的临时表t1，在备库的table_def_key就是：库名+t1+“M的serverid”+“session A的 thread_id”; session B的临时表t1，在备库的table_def_key就是：库名+t1+“M的serverid”+“session B的thread_id”。 由于table_def_key不同，所以这两个表在备库的应用线程里面是不会冲突的。 26.内部临时表union执行流程1(select 1000 as f) union (select id from t1 order by id desc limit 2); 这条语句用到了union，它的语义是，取这两个子查询结果的并集。并集的意思就是这两个集合加起来，重复的行只保留一行。 第二行的key=PRIMARY，说明第二个子句用到了索引id。 第三行的Extra字段，表示在对子查询的结果集做union的时候，使用了临时表(Using temporary)。 如果改为union all，则不会使用临时表。 group by执行流程1select id%10 as m, count(*) as c from t1 group by m; 在Extra字段里面，可以看到三个信息： Using index，表示这个语句使用了覆盖索引，选择了索引a，不需要回表； Using temporary，表示使用了临时表； Using filesort，表示需要排序。 group by优化方法 –索引在MySQL 5.7版本支持了generated column机制，用来实现列数据的关联更新。可以用下面的 方法创建一个列z，然后在z列上创建一个索引(如果是MySQL 5.6及之前的版本，也可以创建 普通列和索引，来解决这个问题)。 1alter table t1 add column z int generated always as(id % 100), add index(z); 上面的group by语句就可以改成： 1select z, count(*) as c from t1 group by z; group by优化方法 –直接排序在group by语句中加入SQL_BIG_RESULT这个提示(hint)，就可以告诉优化器:这个语句涉 及的数据量很大，请直接用磁盘临时表。 1select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m; MySQL什么时候会使用内部临时表? 如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果； join_buffer是无序数组，sort_buffer是有序数组，临时表是二维表结构； 如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。","categories":[{"name":"MySQL实战笔记","slug":"MySQL实战笔记","permalink":"/categories/MySQL实战笔记/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"/tags/MySQL/"}]},{"title":"","slug":"MySQL实战笔记-基础篇","date":"2021-04-23T06:35:24.000Z","updated":"2021-07-27T15:47:24.463Z","comments":true,"path":"2021/04/23/MySQL实战笔记-基础篇/","link":"","permalink":"/2021/04/23/MySQL实战笔记-基础篇/","excerpt":"","text":"01.基础架构：一条SQL查询语句是如何执行的 MySQL可以分为Server层和存储引擎层两部分。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数字和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的是存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。 1.连接器连接器服务器跟客户端建立连接、获取权限、维持和管理连接。连接命令： 1mysql -h$ip -P$port -u$user -p 数据库里，长链接是指连接成功之后，如果客户端持续有请求，则一直使用同一个连接。短链接是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 2.查询缓存连接建立完成之后，就可以执行select语句了。执行逻辑就会来到第二部：查询缓存。 MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过 的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是 查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客 户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存 中。 大多数情况下建议不要使用查询缓存，因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。 好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样: 1mysql&gt; select SQL_CACHE * from T where ID=10; 需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有 这个功能了。 3.分析器如果没有命中查询缓存，就要开始真正执行语句了。 分析器先会做“词法分析”。输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。 4.优化器经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引;或者在一个语句有多表关联(join) 的时候，决定各个表的连接顺序。 5.执行器MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶 段，开始执行语句。 开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有 权限的错误，如下所示。 12mysql&gt; select * from T where ID=10;ERROR 1142 (42000): SELECT command denied to user &apos;b&apos;@&apos;localhost&apos; for table &apos;T&apos; 02.日志系统：一条SQL更新语句是如何执行的与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和binlog（归档日志）。 Redo logWAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。 当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。 InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写。 write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos和checkpoint之间的是还空着的部分，可以用来记录更新的操作。如果write pos追上checkpoint，就表示log满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 binlogMySQL整体来看，其实就只有两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。InnoDB引擎特有的日志，而Server层也有自己的日志，成为binlog（归档日志）。 有两份日志的原因：因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crush-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。 这两种日志有以下三点不同。 redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 1mysql&gt; update T set c=c+1 where ID=2; 执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一 行所在的数据页本来就在内存中，就直接返回给执行器;否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redolog里面，此时redolog处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redolog改成提交(commit)状态，更 新完成。 两阶段提交redo log的写入拆成了两个步骤:prepare和commit，这就是”两阶段提交”。 为了让两份日志之间的逻辑一致，所以有“两阶段提交”。 redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 redo log用于保证crash-safe能力。innodb_flush_log_at_rtx_commit这个参数设置成1的时候，标识每次事务的redo log都直接持久化道磁盘。 sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。 两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案。 03.事务隔离事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代的重要原因之一。 隔离性与隔离级别当数据库上有多个事务同时执行的时候，就可能出现脏读（ditry read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 SQL标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable）。 读未提交：一个事务还没提交时，它做的变更就能被别的事务看到。 读提交：一个事务提交之后，它做的变更才会被其他事务看到。 可重复读：一个事务执行过程中看到的数据，总是跟这个事务再启动时看到的数据时一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化：对同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而”串行化“隔离级别下直接用加锁的方式来避免并行访问。 在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”。 事务隔离的实现在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存多个版本，就是数据库的多版本并发控制（MVCC）。对于read-view A，要得到1，就必须将当前值一次执行图中所有的回滚操作得到。 即使现在有另外一个事务正在将4改成5，这个事务跟read-view A、B、C对应的事务是不会冲突的。 回滚日志在不需要（当系统里没有比这个回滚日志更早的read-view的时候）的时候才删除，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 事务的启动方式MySQL的事务启动方式有以下几种: 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。 可以在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查 找持续时间超过60s的事务。 1select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 04.索引（上）索引的出现其实就是为了提高数据查询的效率。 索引的常见模型哈希表是一种以键-值(key-value)存储数据的结构，我们只要输入待查找的值即key，就可以找 到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。 不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。 假设，现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示： 图中，User2和User4根据身份证号算出来的值都是N，但没关系，后面还跟了一个链表。假设，这时候你要查ID_card_n2对应的名字是什么，处理步骤就是:首先，将ID_card_n2通过哈希函数算出N；然后，按顺序遍历，找到User2。 需要注意的是，图中四个ID_card_n的值并不是递增的，这样做的好处是增加新的User时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很的。 哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。 有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示： 这里假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果要查ID_card_n2对应的名字，用二分法就可以快速得到，这个时间复杂度是O(log(N))。 如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。 有序数组索引只适用于静态存储引擎。 如果我们用二叉搜索树来实现的话，示意图如下所示： 二叉搜索树的特点是:每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果要查 ID_card_n2的话，按照图中的搜索顺序就是按照UserA -&gt; UserC -&gt; UserF -&gt; User2这个路径得 到。这个时间复杂度是O(log(N))。 为了维持O(log(N))的查询复杂度，就需要保持这棵树是平衡二叉树。为了做这个保证，更 新的时间复杂度也是O(log(N))。 树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。 以InnoDB的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就可以存 1200的3次方个值，这已经17亿了。考虑到树根的数据块总是在内存中的，一个10亿行的表上一 个整数字段的索引，查找一个值最多只需要访问3次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。 N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中 了。 在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。 InnoDB的索引模型在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。 又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。 每一个索引在InnoDB里面对应一棵B+树。 12345mysql&gt; create table T(id int primary key,k int not null,name varchar(16),index (k))engine=InnoDB; 表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意图如下。 索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引(clustered index)。 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引 (secondary index)。 基于主键索引和普通索引的查询的区别： 如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树; 如果语句是select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 索引维护B+树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插 入新的行ID值为700，则只需要在R5的记录后面插入一个新记录。如果新插入的ID值为400，就 相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的 数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中， 整体空间利用率降低大约50%。 当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 05.索引（下）在下面这个表T中，如果执行 select * from T where k between 3 and 5，需要执行几次树的搜 索操作，会扫描多少行? 12345678mysql&gt; create table T (ID int primary key,k int NOT NULL DEFAULT 0,s varchar(16) NOT NULL DEFAULT &apos;&apos;,index k(k))engine=InnoDB;insert into T values(100,1, &apos;aa&apos;),(200,2,&apos;bb&apos;),(300,3,&apos;cc&apos;),(500,5,&apos;ee&apos;),(600,6,&apos;ff&apos;),(700,7,&apos;gg); 这条SQL查询语句的执行流程: 在k索引树上找到k=3的记录，取得 ID = 300; 再到ID索引树查到ID=300对应的R3; 在k索引树取下一个值k=5，取得ID=500; 再回到ID索引树查到ID=500对应的R4; 在k索引树取下一个值k=6，不满足条件，循环结束。 在这个过程中，回到主键索引树搜索的过程，称为回表。这个查询过程读了k索引树的3条记录(步骤1、3和5)，回表了两次(步骤2和4)。 覆盖索引如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值 已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面， 索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 需要注意的是，在引擎内部使用覆盖索引在索引k上其实读了三个记录，R3~R5(对应的索引k 上的记录项)，但是对于MySQL的Server层来说，它就是找引擎拿到了两条记录，因此MySQL 认为扫描行数是2。 最左前缀原则B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。 不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。 这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 那么，如果既有联合查询，又有基于a、b各自的查询呢?查询条件里面只有b的语句，是无法使 用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护(a,b)、 (b) 这两个索引。 索引下推在MySQL 5.6之前，只能从查询到的数据开始一个个回表。到主键索引上找出数据行，再对比字段值。 而MySQL 5.6 引入的索引下推优化(index condition pushdown)， 可以在索引遍历过程中，对索 引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 06.全局锁和表锁根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类。 全局锁全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是Flush tables with read lokc(FTWRL)。当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。 官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。 一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是 只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。 所以，single-transaction方法只适用于所有的表使用事务引擎的库。如果有的表使用了不 支持事务的引擎，那么备份就只能通过FTWRL方法。 表级锁MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL）。 表锁的语法是lock tables ... read/write。与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。 另一类表级的锁是MDL（metadata lock）。MDL不需要显示引用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。 在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁;当要对表做结构变更操作的时候，加MDL写锁。 读锁直接不互斥，因此可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 07.行锁MySQL的行锁是在引擎层由各个引擎自己实现的。但并不所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。 行锁就是针对数据表中行记录的锁。比如事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。 两阶段锁 事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。 在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。 这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略: 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。 在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要超过50s才会超时推出，然后其他线程才有可能继续执行。 但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢?所以，超时时间设置太短的话，会出现很多误伤。 所以，正常情况下还是要采用第二种策略，即：主动死锁检测，而且innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。 另一个思路是控制并发度。 因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现;如果你的 团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，对于相同行的更新， 在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。 如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设上优化这个问题呢? 你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多 条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账 户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。","categories":[{"name":"MySQL实战笔记","slug":"MySQL实战笔记","permalink":"/categories/MySQL实战笔记/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"/tags/MySQL/"}]},{"title":"一-Spring基础","slug":"一-Spring基础","date":"2021-04-17T04:49:55.000Z","updated":"2021-04-17T04:52:05.489Z","comments":true,"path":"2021/04/17/一-Spring基础/","link":"","permalink":"/2021/04/17/一-Spring基础/","excerpt":"","text":"1.什么是Spring","categories":[{"name":"Spring实战笔记","slug":"Spring实战笔记","permalink":"/categories/Spring实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"十一.原子变量与非阻塞同步机制","slug":"十一-原子变量与非阻塞同步机制","date":"2020-06-23T12:43:13.000Z","updated":"2020-06-29T09:30:02.923Z","comments":true,"path":"2020/06/23/十一-原子变量与非阻塞同步机制/","link":"","permalink":"/2020/06/23/十一-原子变量与非阻塞同步机制/","excerpt":"","text":"1. 锁的劣势​ 如果有多个线程同时请求锁，那么JVM就需要借助操作系统的功能。如果出现了这种情况，那么一些线程将被挂起并且在稍后恢复运行。当线程恢复执行时，必须等待其他线程执行完它们的时间片以后，才能被调度执行。在挂起和恢复线程等过程中存在着很大的开销，并且通常存在着较长时间的中断。如果在基于锁的类中包含有细粒度的操作（例如同步容器类，在其大多数方法中只包含了少量操作），那么当在锁上存在着激烈的竞争时，调度开销与工作开销的比值会非常高。 ​ 与锁相比，volatile变量时一种更轻量级的同步机制，因为在使用这些变量时不会发生上下文切换或线程调度等操作。然而，volatile变量同样存在一些局限：虽然它们提供了相似的可见性保证，但不能用于构建原子的复合操作。因此，当一个变量依赖其他的变量时，或者当变量的新值依赖于旧值时，就不能使用volatile变量。这些都限制了volatile变量的使用，因此它们不能用来实现一些常见的工具，例如计数器或互斥体。 ​ 锁还存在其他一些缺点。当一个线程正在等待锁时，它不能做任何其他事情。如果一个线程在持有锁的情况下被延迟执行，那么所有需要这个锁的线程都无法执行下去。如果被阻塞线程的优先级较高，而持有锁的线程优先级较低，那么这将是一个严重的问题——也被称为优先级反转（Priority Inversion）。即使高优先级的线程可以抢先执行，但仍然需要等待锁被释放，从而导致它的优先级降低至低优先级线程的级别。如果持有锁的线程将永久地阻塞，所有等待这个锁的线程就永远无法执行下去。 2. 硬件对并发的支持​ 独占锁是一项悲观技术——它假设最坏的情况，并且只有在确保其他线程不会造成干扰（通过获取正确的锁）的情况下才能执行下去。 ​ 对于细粒度的操作，还有另外一种更高效的方法，也是一种乐观的方法，通过这种方法可以在不发生干扰的情况下完成更新操作。这种方法需要借助冲突检查机制来判断在更新过程中是否存在来自其他线程的干扰，如果存在，这个操作将失败，并且可以重试（也可以不重试）。 2.1 比较并交换​ 在大多数处理器架构中采购的方法是实现一个比较并交换（CAS）指令。CAS包含了3个操作数——需要读写的内存位置V、进行比较的位置A和拟写入的新值B。当且仅当V的值等于A时，CAS才会通过原子方式用新值B来更新V的值，否则不会执行任何擦欧总。无论位置V的值是否等于A，都将返回V原有的值。（这种变化形式被称为比较并设置，无论操作是否成功都会返回。）CAS是一项乐观的技术，它希望能成功地执行更新操作，并且如果有另一个线程在最近一次检查后更新了该变量，那么CAS能检测到这个错误。 ​ 当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其他线程都将失败。然而，失败的线程并不会被挂起，而是被告知在这次竞争中失败，并可以再次尝试。 ​ 模拟CAS操作 1234567891011121314151617public class SimulateCAS &#123; @GuardedBy(\"this\") private int value; public synchronized int get() &#123; return value;&#125; public synchronized int compareAndSwap(int expectedValue, int newValue) &#123; int oldValue = value; if (oldValue == expectedValue) &#123; value = newValue; &#125; return oldValue; &#125; public synchronized boolean compareAndSwap(int expectedValue, int newValue) &#123; return (expectedValue == compareAndSwap(expectedValue, newValue)); &#125;&#125; 2.2 非阻塞的计数器12345678910111213141516public class CasCounter &#123; private SimulateCAS value; public int getValue() &#123; return value.get(); &#125; public int increment() &#123; int v; do &#123; v = value.get(); &#125; while (v != value.compareAndSwap(v, v + 1)); return v + 1; &#125;&#125; ​ CasCounter不会阻塞，但如果其他线程同时更新计数器，那么会多次执行重试操作。（在实际情况中，如果仅需要一个计数器或序列生成器，那么可以直接使用AtomicInteger或AtomicLong，它们能提供原子的递增方法以及其他算术方法。） 3. 原子变量类​ 原子变量比锁的粒度更细，量级更轻。原子变量将发生竞争的范围缩小到单个变量上，这是获得的粒度最细的情况。 ​ 原子变量相当于一个泛华的volatile变量，能够支持原子的和有条件的读—改—写操作。AtomicInteger表示一个int类型的值，并提供了get和set方法，这些Volatile类型的int变量在读取和写入上有着相同的内存语义。 4. 非阻塞算法​ 如果在某种算法中，一个线程的失败或挂起不会导致其他线程也失败或挂起，那么这种算法就被称为非阻塞算法。如果在算法的每个步骤中都存在某个线程能够执行下去，那么这种算法也被称为无锁算法。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"十.显式锁","slug":"十-显式锁","date":"2020-06-17T02:00:16.000Z","updated":"2020-06-22T05:57:13.014Z","comments":true,"path":"2020/06/17/十-显式锁/","link":"","permalink":"/2020/06/17/十-显式锁/","excerpt":"","text":"1. Lock与ReentrantLock​ 与内置加锁机制不同的是，Lock提供了一种无条件的、可轮询的、定时的以及可中断的锁获取操作，所有加锁和解锁的方法都是显式的。在Lock的实现中必须提供与内部锁相同的内存可见性语义，但在加锁语义、调度算法、顺序保证以及性能特性等方面可以有所不同。 12345678910public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition();&#125; ​ ReentrantLock实现了Lock接口，并提供了与synchronized相同的互斥性和内存可见性。在获取ReentrantLock时，有着与进入同步代码块相同的内存语义，在释放ReentrantLock时，同样有着与腿处同步代码块相同的内存语义。此外，与synchronized一样，ReentrantLock还提供了可重入的加锁语义。ReentrantLock支持在Lock接口中定义的所有获取锁模式，并且与synchronized相比，它还为处理锁的不可用性问题提供了更高的灵活性。 ​ 使用Lock接口，必须在finally块中释放锁。否则，如果在被保护的代码中抛出了异常，那么这个锁永远无法释放。 123456789Lock lock = new ReentrantLock();...lock.lock();try &#123; //更新对象状态 // 捕获异常，并在必要时恢复不变性条件&#125; finally &#123; lock.unlock();&#125; 1.1 轮询锁与定时锁​ 可定时的与可轮询的锁获取锁模式是由tryLock方法实现的，与无条件的锁获取模式相比，它具有更完善的错误恢复机制。在内置锁中，死锁是一个严重的问题，恢复程序的唯一方法是重新启动程序，而防止死锁的唯一方法就是在构造程序时避免出现不一致的顺序。可定时与可轮询的锁提供了另一种选择：避免死锁的发生。 1.2 可中断的锁获取操作​ 正如定时的锁获取操作能在带有时间限制的操作中使用独占锁，可中断的锁获取操作同样能在可取消的操作中使用加锁。lockInterruptibly方法能够在获得锁的同时保持对中断的响应，并且由于它包含在Lock中，因此无须创建其他类型的不可中断阻塞机制。 ​ 可中断的锁获取操作的标准结构比普通的锁获取操作略微复杂一些，因为需要两个try块。（如果在可中断的锁获取操作中抛出了InterruptedException，那么可以使用标准的finally加锁模式。） 1234567891011public boolean sendOnSharedLine(String message) throws InterruptedException &#123; lock.lockInterruptibly(); try &#123; return cancellableSendOnSharedLind(message); &#125; finally &#123; lock.unlock(); &#125;&#125;private boolean cancellableSendOnSharedList(String message) throws InterruptedException &#123;...&#125; 2. 公平性​ 在ReentrantLock的构造函数中提供了两种公平性选择：创建一个非公平的锁（默认）或者一个公平的锁。在公平的锁上，线程将按照它们发出请求的顺序来获得锁，但在非公平的锁上，则允许“插队”：当一个线程请求非公平的锁时，如果在发出请求的同时该锁的状态变为可用，那么这个线程将跳过队列中所有的等待线程并获得这个锁。 3. 读-写锁​ ReentrantLock实现了一种标准的互斥锁：每次最多只有一个线程能持有ReentrantLock。互斥是一种保守的加锁策略，虽然可以避免“写/写”冲突和“写/读”冲突，但同样也避免了“读/读”冲突。读/写锁：一个资源可以被多个读操作访问，或者被一个写操作访问，但两者不能同时进行。 ​ ReentrantReadWriteLock为这两种锁都提供了可重入的加锁语义。与ReentrantLock类似，ReentrantReadWriteLock在构造时也可以选择是一个非公平的锁（默认）还是一个公平的锁。在公平的锁中，等待时间最长的线程将优先获得锁。如果这个锁由读线程持有，而另一个线程请求写入锁，那么其他读线程都不能获得读取锁，直到写线程使用完并且释放了写入锁。在非公平的锁中，线程获得访问许可的顺序是不确定的。写线程降级为读线程是可以的，但从读线程升级为写线程是不可以的（这样会导致死锁）。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"九.性能与可伸缩性","slug":"九-性能与可伸缩性","date":"2020-06-08T06:50:59.000Z","updated":"2020-06-16T08:47:04.952Z","comments":true,"path":"2020/06/08/九-性能与可伸缩性/","link":"","permalink":"/2020/06/08/九-性能与可伸缩性/","excerpt":"","text":"1. 对性能的思考​ 提升性能意味着更少的资源做更多的事情。“资源”的含义很广。对于一个给定的操作，通常会缺乏某种特定的资源，例如CPU时钟周期、内存、网络带宽、I/O带宽、数据库请求、磁盘空间以及其他资源。 ​ 尽管使用多个线程的目标是提升整体性能，但与单线程的方法相比，使用多个线程总会引起一些额外的性能开销。造成这些开销的操作包括：线程之间的协调（例如加锁、触发信号以及内存同步等），增加的上下文切换，线程的创建和销毁，以及线程的调度等。 1.1 性能与可伸缩性​ 应用程序的性能可以采用多个指标来衡量，例如服务时间、延迟时间、吞吐率、效率、可伸缩性以及容量等。其中一些指标（服务时间、等待时间）用于衡量程序的“运行速度”，即某个指定的任务单元需要“多快”才能处理完成。另一些指标（生产量、吞吐量）用于程序的“处理能力”，即在计算资源一定的情况下，能完成“多少”工作。 ​ 可伸缩性指的是：当增加计算资源时（例如CPU、内存、存储容量或I/O带宽），程序的吞吐量或者处理能力相应地增加。 2. Amdahl定律​ Amdahl定律描述的是：在增加计算资源的情况下，程序在理论上能够实现最高加速比，这个值取决于程序中可并行组件与串行组件的鼻子。假定F是必须被串行执行的部分，那么根据Amdahl定律，在包含N个处理器的机器中，最高的加速比为：$$Speedup \\leq \\frac{1}{F + \\frac{(1- F)}{N}}$$​ 当N趋近无穷大时，最大的加速比趋近于1/F。因此，如果程序有50%的计算需要串行执行，那么最高的加速比只能是2（不管有多少线程可用）。 3. 线程引入的开销3.1 上下文切换​ 如果主线程是唯一的线程，那么它基本上不会被调度出去。另一方面，如果可运行的线程数大于CPU的数量，那么操作系统最终会将某个正在运行的线程调度出来，从而使其他线程能够使用CPU。这将导致一次上下文切换，在这个过程中将保存当前运行线程的上下文，并将新调度进来的线程的执行上下文设置为当前上下文。 ​ 切换上下文需要一定的开销，而在线程调度过程中需要访问由操作系统和JVM共享的数据结构。应用程序、操作系统以及JVM都使用一组相同的CPU。在JVM和操作系统的代码中消耗越多的CPU时钟周期，应用程序的可用CPU时钟周期就越少。 ​ 当线程由于等待某个发生竞争的锁而被阻塞时，JVM通常会将这个线程挂起，并允许它被交换出去。如果线程频繁地发生阻塞，那么它们将无法使用完整的调度时间片。在程序中发生越多的阻塞（包括阻塞I/O，等待获取发生竞争的锁，或者在条件变量上等待），与CPU密集型的程序就好发生越多的上下文切换，从而增加调度开销，并因此而降低吞吐量。 3.2 内存同步​ 同步操作的性能开销包括多个方面。在synchronized和volatile提供的可见性保证中可能会使用一些特殊指令，即内存栅栏（Memory Barrier）。内存栅栏可以刷新缓存，使缓存无效，刷新硬件的写缓冲，以及停止执行管道。内存栅栏可能同样会对性能带来间接的影响，因为它们将抑制一些编译器优化操作。在内存栅栏中，大多数操作都是不能被重排序的。 ​ 编译器可以执行锁粒度粗化（Lock Coarsening）操作，即将临近的同步代码块用同一个锁合并起来。 3.3 阻塞​ 非竞争的同步可以完全在JVM中进行处理，而竞争的同步可能需要操作系统的介入，从而增加开销。当在锁上发生竞争时，竞争失败的线程肯定会阻塞。JVM在实现阻塞行为时，可以采用自旋等待或者通过操作系统挂起被阻塞的线程。 4. 减少锁的竞争​ 串行操作会降低可伸缩性，并且上下文切换也会降低性能。在锁上发生竞争时将同时导致这两种问题，因此减少锁的竞争能够提高性能和可伸缩性。 ​ 在并发程序中，对可伸缩性的最主要威胁就是独占方式的资源锁。 ​ 有两个因素将影响在锁上发生竞争的可能性：锁的请求频率，以及每次持有该锁的时间。如果二者的乘积很小，那么大多数获取锁的操作都不会发生竞争，因此在该锁上的竞争不会对可伸缩性造成严重影响。然而，如果在锁上的请求量很高，那么需要获取该锁的线程将被阻塞并等待。在极端情况下，即使仍有大量工作等待完成，处理器也会被闲置。 ​ 有3种方式可以降低锁的竞争程度： 减少锁的持有时间。 降低锁的请求频率。 使用带有协调机制的独占锁，这些机制允许更高的并发性。 4.1 缩小锁的范围（“快进快出”）​ 降低发生竞争可能性的一种有效方式就是尽可能缩短锁的持有时间。例如，可以将一些与锁无关的代码移出同步代码块，尤其是些开销较大的操作，以及可能被阻塞的操作，例如I/O操作。 ​ 尽管缩小同步代码块能提高可伸缩性，但同步代码块也不能过小——一些需要采用原子方式执行的操作必须包含在一个同步块中。此外，同步需要一定的开销，当把一个同步代码块分解为多个同步代码块时，反而会对性能提升产生负面影响。 4.2 减小锁的粒度​ 另一种减小锁的持有时间的方式是降低线程请求锁的频率（从而减小发生竞争的可能性）。这可以通过锁分解和锁分段等技术来实现，在这些技术中将采用多个相互独立的锁来保护独立的状态变量，从而改变这些变量在之前由单个锁来保护的情况。这些技术能减小锁操作的粒度，并能实现更高的可伸缩性，然而，使用的锁越多，那么发生死锁的风险也就越高。 ​ 如果一个锁需要保护多个相互独立的状态变量，那么可以将这个锁分解为多个锁，并且每个锁只保护一个变量，从而提高可伸缩性，并最终降低每个锁被请求的频率。 4.3 锁分段​ 在某些情况下，可以将锁分解技术进一步扩展为对一组独立对象上的锁进行分解，这种情况被称为锁分段。 ​ 锁分段的一个劣势在于：与采用单个锁来实现独占访问相比，要获取多个锁来实现独占访问将更加困难并且开销更高。通常，在执行一个操作时最多只需获取一个锁，但在某些情况下需要加锁整个容器。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"八.避免活跃性危险","slug":"八-避免活跃性危险","date":"2020-06-04T10:44:47.000Z","updated":"2020-06-08T06:48:58.475Z","comments":true,"path":"2020/06/04/八-避免活跃性危险/","link":"","permalink":"/2020/06/04/八-避免活跃性危险/","excerpt":"","text":"1. 死锁​ 当一个线程永远地持有一个锁，并且其他线程都尝试获得这个锁时，那么它们将永远被阻塞。在线程A持有锁L想获得锁M的同时，线程B持有锁M并尝试获得锁L，那么这两个线程将永远地等待下去。这种情况就是最简单的死锁形式（或者成为“抱死”），其中多个线程由于存在环路的锁依赖关系而永远地等待下去。 1.1 锁顺序死锁​ 两个线程试图以不同的顺序来获得相同的锁。如果按照相同的顺序来请求锁，那么就不会出现循环的加锁依赖性，因此也就不会产生死锁。 ​ 如果所有线程都以固定的顺序来获得锁，那么在程序中就不会出现顺序死锁问题。 1.2 动态的锁顺序死锁​ 如果执行时序不当，线程的顺序将会被打乱，造成死锁。 1.3 在协作对象之间发生的死锁​ 如果在持有锁时调用某个外部方法，那么将出现活跃性问题。在这个外部方法中可能会获取其他锁（这可能会产生死锁），或者阻塞时间过长，导致其他线程无法及时获得当前被持有的锁。 1.4 开放调\u0010用​ 如果在调用某个方法时不需要持有锁，那么这种调用被称为开放调用。依赖于开放调用的类通常能表现出更好的行为，并且与那么在调用方法时需要持有锁的类相比，也更易于编写。通过尽可能地使用开放调用，将更易于找出那些需要获取多个锁的代码路径，因此也就更容易采用一致的顺序来获得锁。 1.5 资源死锁​ 正如当多个线程相互持有彼此正在等待的锁而又不释放自己已持有的锁时会发生死锁，当它们在相同的资源集合上等待时，也会发生死锁。 2. 其他活跃性危险2.1 饥饿​ 由于线程无法访问它所需要的资源而不能继续进行时，就发生了“饥饿”。引发饥饿的最常见资源就是CPU时钟周期。如果在JAVA应用程序中对线程的优先级使用不当，或者在持有锁时执行了一些无法结束的结构（例如无限循环，或者无限制地等待某个资源），那么也有可能导致饥饿，因为其他需要这个锁的序线程将无法得到它。 ​ 要避免使用线程优先级，因为这会增加平台依赖性，并可能导致活跃性问题。在大多数并发应用程序中，都可以使用默认的线程优先级。 2.2 活锁​ 活锁是另一种形式的活跃性问题，该问题尽管不会阻塞线程，但也不能继续执行，因为线程将不断重复执行相同的操作，而且总会失败。活锁通常发生在处理事务消息的应用程序中：如果不能成功地处理某个消息，那么消息处理机制将回滚整个事务，并将它重新放到队列的开头。 ​ ​","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"七.线程池的使用","slug":"七-线程池的使用","date":"2020-05-27T07:35:51.000Z","updated":"2020-06-04T10:42:56.509Z","comments":true,"path":"2020/05/27/七-线程池的使用/","link":"","permalink":"/2020/05/27/七-线程池的使用/","excerpt":"","text":"1. 在任务与执行策略之间的隐性耦合​ 有些类型的任务需要明确地制定执行策略，包括： ​ 依赖性任务。大多数行为正确的任务都是独立的：它们不依赖于其他任务的执行时序、执行结果或其他效果。当在线程中执行独立的任务时，可以随意地改变线程池的大小和配置，这些修改只会对执行性能产生影响。 ​ 使用线程封闭机制的任务。对象可以封闭在任务线程中，使得在该线程中执行的任务在访问该对象时不需要同步，即使这些资源不是线程安全的也没有问题。 ​ 对响应时间敏感的任务。如果将一个运行时间较长的任务提交到单线程的Executor中，或者将多个运行时间较长的任务提交到一个只包含少量线程的线程池中，那么将降低由该Executor管理的服务的响应性。 ​ 使用ThreadLocal的任务。ThreadLocal使每个线程都可以拥有某个变量的一个私有“版本”。然而，只要条件允许，Executor可以自由地重用这些线程。只有当线程本地值的生命周期受限于任务的生命周期时，在线程池的线程中使用ThreadLocal才有意义，而在线程池的线程中不应该使用ThreadLocal在任务之间传递值。 1.1 线程饥饿死锁​ 如果所有正在执行任务的线程都由于等待其他仍处于工作队列中的任务而阻塞，这种现象被称为线程饥饿死锁。 1.2 运行时间较长的任务​ 如果任务阻塞的时间过长，那么即使不会出现死锁，线程池的响应性也会变得糟糕。执行时间较长的任务不仅会造成线程池阻塞，甚至还会增加执行时间较短任务的服务时间。 2. 设置线程池的大小​ 对于计算密集型的任务，在拥有$N_{CPU}$个处理器的系统上，当线程池的大小为$N_{CPU}$ + 1时，通常能实现最优的利用率。（即使当计算密集型的线程偶尔由于页缺失故障或者其他原因暂停时，这个“额外”的线程也能确保CPU的时钟周期不会被浪费。） ​ 对于包含I/O操作或者其他阻塞操作的任务，由于线程并不会一直执行，因此线程池的规模应该更大。$$最佳线程数 = CPU核心数(1/CPU利用率) = CPU核心数（1 + (I/O耗时/CPU耗时)）$$​ 可以通过Runtime来获得CPU数目: 1int N_CPUS = Runtime.getRuntime().availableProcessors(); 3. 配置ThreadPoolExecutor​ ThreadPoolExecutor为一些Executor提供了基本的实现，这些Executor是由Executors中的newCachedThreadPool、newFixedThreadPool和newScheduledThreadExecutor等工厂方法返回的。ThreadPoolExecutor是一个灵活的、稳定的线程池，允许进行各种定制。 1234567public ThreadPoolExecutor(int corePoolSize, int maximumsPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Ruunable&gt; workQueue, ThreadFactory threadFactory, RejectedExecuptionHandler handler) &#123;...&#125; 3.1 线程的创建与销毁​ 线程池的基本大小（Core Pool Size）、最大大小（Maximum Pool Size）以及存活时间等因素共同负责线程的创建与销毁。基本大小也就是线程池的目标大小，即在没有任务执行时线程池的大小，并且只有在工作队列满了的情况下才会创建超过这个数量的线程。线程池的最大大小表示可同时活动的线程数量的上限。如果某个线程的空闲时间超过了存活时间，那么将被标记为可回收的，并且当线程池的当前大小超过了基本大小时，这个线程就被终止。 ​ newFixedThreadPool工厂方法将线程池的额基本大小和最大大小设置为参数中指定的值，而且创建的线程不会超时。newCachedThreadPool工厂方法将线程池的最大大小设置为Integer.MAX_VALUE，而将基本大小设置为零，并将超时设置为1分钟，这种方法创建出来的线程池可以被无限扩展，并且当需求降低时会自动收缩。其他形式的线程池可以通过显示的ThreadPoolExecutor构造函数来构造。 3.2 管理队列任务​ ThreadPoolExecutor允许提供一个BlockingQueue来保存等待执行的任务。基本的任务排队方法有3种：无界队列、有界队列和同步移交（Synchronous Handoff）。队列的选择与其他的配置参数有关，例如线程池的大小等。 ​ newFixedThreadPool和newSingleThreadExecutor在默认情况下将使用一个无界的LinkedBlockingQueue。如果所有工作者线程都处于忙碌状态，那么任务将在队列中等候。如果任务持续快速地到达，并且超过了线程池处理它们的速度，那么队列将无限制地增加。 ​ 一种更稳妥的资源管理策略是使用有界队列，例如ArrayBlockingQueue、有界的LinkedBlockingQueue、PriorityBlockingQueue。有界队列有助于避免资源耗尽的情况发生。在使用有界队列时，队列的大小与线程池的大小必须一起调节。如果线程池小而队列较大，那么有助于减少内存使用量，降低CPU的使用率，同时还可以减少上下文切换，但付出的代价是可能会限制吞吐量。 ​ 对于非常大的或者无界的线程池，可以通过SynchronousQueue来避免任务排队，以及直接将任务从生产者移交给工作者线程。SynchronousQueue不是一个真正的队列，而是一种在线程之间进行移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接受这个元素。如果没有线程正在等待，并且线程池的当前大小小于最大值，那么ThreadPoolExecutor将会创建一个新的线程，否则根据饱和策略，这个任务将被拒绝。只有当线程池是无界的活着可以拒绝任务时，SynchronousQueue才有实际价值。在newCachedThreadPool工厂方法中就使用了SynchronousQueue。 ​ 只有当任务相互独立时，为线程池或工作队列设置界限才是合理的。如果任务之间存在依赖性，那么有界的线程池队列就可能导致线程“饥饿”死锁问题。此时应该使用无界的线程池，例如newCachedThreadPool。 3.3 饱和策略​ 当有界队列被填满后，饱和策略开始发挥作用。ThreadPoolExecutor的饱和策略可以通过调用setRejectedExecutionHandler来修改。（如果某个任务被提交到一个已被关闭的Executor时，也会用到饱和策略。）JDK提供了几种不同的RejectedExecutionHandler实现，每种实现都包含有不同的饱和策略：AbortPolicy、CallerRunPolicy、DiscardPolicy和DiscardOldsetPolicy。 ​ “中止（Abort）”策略是默认的饱和策略，该策略将抛出未检查的RejectedExecuption。调用者可以捕获这个异常，然后根据需求编写自己的处理代码。当新提交的任务无法保存到队列中等待执行时，“抛弃（Discard）”策略会悄悄抛弃改任务。“抛弃最旧的（Discard-Oldset）”策略则会抛弃下一个将被执行的任务，然后尝试重新提交新的任务。 ​ “调用者运行（Caller-Runs）”策略实现了一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。它不会在线程池的某个线程中执行新提交的任务，而是在一个调用了execute的线程中执行该任务。 123456ThreadPoolExecutor executor = new ThreadPoolExecutor(N_THREADS, N_THREADS, 0L, TimeUnit.MILLSECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(CAPACITY));executor.setRejectedExecutionHandler( new ThreadPoolExecutor.CallerRunsPolicy()); 3.4 线程工厂​ 每当线程池需要创建一个线程时，都是通过线程工厂方法来完成的。默认的线程工厂方法将创建一个新的、非守护的线程，并且不包含特殊的配置信息。通过指定一个线程工厂方法，可以定制线程池的配置信息。在ThreadFactory中只定义了一个方法newThread，每当线程池需要创建一个新线程时都会调用这个方法。 3.5 在调用构造函数后再定制ThreadPoolExecutor​ 在调用完ThreadPoolExecutor的构造函数后，仍然可以通过设置函数（Setter）来修改大多数传递给它的构造函数的值（例如线程池的基本大小、最大大小、存活时间、线程工厂以及拒绝执行处理器（Rejected Execution Handler））。如果Executor是通过Executors中的某个（newSingleThreadExecutor除外）工厂方法创建的，那么可以将结果的类型转换为ThreadPoolExecutor以访问设置器。 123456ExecutorService exec = Executors.newCachedThreadPool();if (exec instanceof ThreadPoolExecutor) &#123; ((ThreadPoolExecutor) exec).setCorePoolSize(10);&#125; else &#123; throw new AssertionError(\"Oops, bad assumption\");&#125; ​ 在Executors中包含一个unconfigurableExecutorsService工厂方法，该方法对一个现有的ExecutorService进行包装，使其只暴露出ExecutorService的方法，因此不能对它进行配置。newSingleThreadExecutor返回按这种格式封装的ExecutorService，而不是最初的ThreadPoolExecutor。 4. 扩展ThreadPoolExecutor​ ThreadPoolExecutor是可扩展的，它提供了几个可以在子类中改写的方法：beforeExecute、afterExecute和terminated，这些方法可以用于扩展ThreadPoolExecutor的行为。 ​ 无论任务是从run中正常返回，还是抛出一个异常而返回，afterExecute都会被调用。（如果任务在完成后带有一个Error，那么就不会调用afterExecute。）如果beforeExecute抛出一个RuntimeExeption，那么任务将不会被执行，并且afterExecute也不会被调用。 ​ 在线程完成关闭操作时调用terminated，也就是在其所有任务都已经完成并且所有工作者线程也已经关闭后。terminated可以用来释放Executor在其生命周期里分配的各种资源，此外还可以执行发送通知、记录日期或者收集finalize统计信息等操作。 5. 递归算法的并行化​ 如果循环中的迭代操作都是独立的，并且不需要等待所有的迭代操作都完成再继续，那么就可以使用Executor将串行循环转化为并行循环。 12345678910111213void processSequentially(List&lt;Element&gt; elements) &#123; for (Element e : elements) &#123; process(e); &#125;&#125;void processInParallel(Executor exec, List&lt;Elements&gt; elemtns) &#123; for (final Element e : elements) &#123; exec.execute(new Runnable() &#123; public void run() &#123; process(e); &#125; &#125;); &#125;&#125; ​ 当串行循环中的各个迭代操作之间彼此独立，并且每个迭代操作执行的工作量比管理一个新任务时带来的开销更多，那么这个串行循环就适合并行化。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"六.取消与关闭","slug":"六-取消与关闭","date":"2020-05-20T11:03:56.000Z","updated":"2020-05-27T07:33:36.470Z","comments":true,"path":"2020/05/20/六-取消与关闭/","link":"","permalink":"/2020/05/20/六-取消与关闭/","excerpt":"","text":"1.任务取消​ 如果外部代码能在某个操作正常完成之前将其置入“完成”状态，那么这个操作就可以称为可取消的。 1.1 中断​ 线程中断是一种协作机制，线程可以通过这种机制来通知另一个线程，告诉它在合适的或者可能的情况下停止当前工作，并转而执行其他的工作。 ​ 每个线程都有一个boolean类型的中断状态。当中断线程时，这个线程的中断状态就被设置为true。在Thread中包含了中断线程以及查询线程中断状态的方法。interrupt方法能中断目标线程，而isInterrupted方法能返回目标线程的中断状态。静态的interrupted方法将清除当前线程的中断状态，并返回它之前的值，这也是清除中断状态的唯一方法。 12345public class Thread &#123; public void interrupt() &#123;...&#125; public boolean isInterrupted() &#123;...&#125; public static boolean interrupted() &#123;...&#125;&#125; ​ 阻塞库方法，例如Thread.sleep和Object.wait等，都会检查线程何时中断，并且在发现中断时提前返回。它们在响应中断时执行的操作包括：清除中断状态，抛出InterruptedException，表示阻塞操作由于中断而提前结束。 ​ 调用interrupt并不意味着立即停止目标线程正在进行的工作，而只是传递了请求中断的消息。 ​ 对中断操作的正确理解是：它并不会真正地中断一个正在运行的线程，而只是发出中断请求，然后由线程在下一个合适的时刻中断自己。 ​ 通常，中断是实现取消的 最合理方式。 1234567891011121314151617181920public class PrimeProducer extends Thread &#123; private final BlockingQueue&lt;BigInteger&gt; queue; PrimeProducer(BlockingQueue&lt;BigInteger&gt; queue) &#123; this.queue = queue; &#125; public void run() &#123; try &#123; BigInteger p = BigInteger.ONE; while (!Thread.currentThread().isInterrupted()) &#123; queue.put(p = p.nextProbablePrime()); &#125; &#125; catch (InterruptedException consumed) &#123; /* 允许线程退出 */ &#125; &#125; public void cancel() &#123; interrupt(); &#125;&#125; 1.2 中断策略​ 中断策略规定线程如何解释某个中断请求——当发现中断请求时，应该做哪些工作，哪些工作单元对于中断来说是原子操作，以及以多快的速度来响应中断。 ​ 最合理的中断策略是某种形式的线程级（Thread-Level）取消操作或者服务级（Service-Level）取消操作：尽快退出，在必要时进行清理，通知某个所有者该线程已经退出。 1.3 响应中断​ 在调用可中断的阻断函数时，例如Thread.sleep或BlockingQueue.put等，有两种实用策略可以用于处理InterruptedException： 传递异常（可能在执行某个特定于任务的清除操作之后），从而使方法也可成为可中断的阻塞方法。 恢复中断状态，从而使调用栈中的上层代码能够对其进行处理。 如果不想或无法传递InterruptedException（或许通过Runnable来定义任务），那么需要寻找另一种方法来保存中断请求。一种标准的方法就是通过再次调用interrupt来恢复中断状态。 ​ 对于一些不支持取消但仍可以调用可中断阻塞方法的操作，它们必须在循环中调用这些方法，并在发现中断后重新尝试。在这种情况下，它们应该在本地保存中断状态，并在返回前恢复状态而不是在捕获InterruptedException时修复状态。 1234567891011121314151617public Task getNextTask(BlockingQueue&lt;Target&gt; queue) &#123; boolean interrupted = false; try &#123; while (true) &#123; try &#123; return queue.take(); &#125; catch (InterruptedException e) &#123; interrupted = true; // 重新尝试 &#125; &#125; &#125; finally &#123; if (interrupted) &#123; Thread.currentThread().interrupt(); &#125; &#125;&#125; 1.4 通过Future来实现取消​ ExecutorService.submit将返回一个Future来描述任务。Future拥有一个cancel方法，该方法带有一个boolean类型的参数mayInterruptedIfRunning，表示取消操作是否成功。（这只是表示任务是否能够接受中断，而不是表示任务是否能检测并处理中断。）如果mayInterruptedIfRunning为true并且任务当前正在某个线程中运行，那么这个线程能被中断。如果这个参数为false，那么意味着“若任务还没有启动，就不要运行它”，这种方式应该用于那鞋不处理中断的任务中。 2. 处理非正常的线程终止​ 导致线程提前死亡的最主要原因就是RuntimeException。由于这些异常表示出现了某种变成错误或者其他不可修复的错误，因此它们通常不会被捕获。它们不会在调用栈中逐层传递，而是默认地在控制台中输出堆栈信息，并终止线程。 ​ 在try-catch代码块中调用这些任务，捕获未检查的异常；实现Thread API中的UncaughtExceptionHandler，它能检测出某个线程由于未捕获的异常而终结的情况。这两种方法好似互补的，通过将二者结合在一起，就能有效地防止线程泄漏问题。 3. JVM关闭​ JVM既可以正常关闭，也可以强行关闭。正常关闭的触发方式有多种，包括：当最后一个“正常（非守护）”线程结束时，或者当调用了System.exit时，或者通过其他特定于平台的方法关闭。 3.1 关闭钩子​ 在正常关闭中，JVM首先调用所有已注册的关闭钩子（Shutdown Hook）。关闭钩子时指通过Runtime.addShutdownHook注册的但尚未开始的线程。JVM并不难保证关闭钩子的调用顺序。在关闭应用程序线程时，如果有（守护或非守护）线程仍然在运行，那么这些线程接下来将与关闭进程并发执行。当所有的关闭钩子都执行结束时，如果runFinalizersOnExit为true， 那么JVM将运行终结器，然后再停止。JVM并不会停止或中断任何在关闭时仍然运行的应用程序线程。当JVM最终结束时，这些线程将被强行结束。如果关闭钩子或终结器没有执行完成，那么正常关闭进程“挂起”并且JVM必须被强行关闭。当被强行关闭时，只是关闭JVM，而不会运行关闭钩子。 ​ 关闭钩子可以用于实现服务或应用程序的清理工作，例如删除临时文件，或者清除无法由操作系统自动清除的资源。 3.2 守护线程​ 线程分为两种：普通线程和守护线程。在JVM启动时创建的所有线程中，除了主线程外，其他的线程都是守护线程（例如垃圾回收器以及其他执行辅助工作的线程）。当创建一个新线程时，新线程将继承创建它的线程的守护状态，因此在默认情况下，主线程创建的所有线程都是普通线程。 ​ 普通线程与守护线程之间的差异仅在于当线程退出时发生的操作。当一个线程退出时，JVM会检查其他正在运行的线程，如果这些线程都是守护线程，那JVM会正常退出操作。当JVM停止时，所有仍然存在的守护线程都将被抛弃——既不会执行finally代码块，也不会执行回卷栈，而JVM只是直接退出。 ​ 守护线程通常不能用来替代应用程序管理程序中各个服务的生命周期。 3.3 终结器​ 在大多数情况下，通过使用finally代码块和显式的close方法，能够比使用终结器更好地管理资源。所以应避免使用终结器。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"五.任务执行","slug":"五-任务执行","date":"2020-05-14T06:14:33.000Z","updated":"2020-05-20T11:03:33.881Z","comments":true,"path":"2020/05/14/五-任务执行/","link":"","permalink":"/2020/05/14/五-任务执行/","excerpt":"","text":"1.Executor123public interface Executor &#123; void execute (Runnable command);&#125; 1.1基于Executor的Web服务器123456789101112131415161718class TaskExecutionWebServer &#123; private static final int NTHREADS = 100; private static final Executor exec = Executors.newFiexdThreadPool(NTHRADS); public void main(String[] args) throws IOException &#123; ServerSocket socket = new ServerSocket(80); while (true) &#123; final Socket connection = socket.accept(); Runnable task = new Runnable() &#123; public void run() &#123; handleRequest(connection); &#125; &#125;; exec.execute(task); &#125; &#125;&#125; 1.2线程池​ 类库提供了一个灵活的线程池以及一些有用的默认配置。可以通过调用Executors中的静态工厂方法之一来创建一个线程池： ​ newFiexedThreadPool。newFiexedThradPool将创建一个固定长度的线程池，每当提交一个任务时就创建一个线程，直到到达线程池的最大数量，这时线程的规模将不再变化（如果某个线程由于发生了未预期的Exception而结束，那么线程会补充一个新的线程）。 ​ newCachedThreadPool。newCachedThradPool将创建一个可缓存的线程池，如果线程池的当前规模超过了处理需求时，那么将回收空闲的线程，而当需求增加时，则可以添加新的线程，线程池的规模不存在任何限制。 ​ newSingleThreadExecutor。newSingleThreadExecutor是一个单线程的Executor，它创建单个工作者线程来执行任务，如果这个线程异常结束，会创建另一个线程来替代。newSingleThreadExecutor能确保任务在队列中的顺序来串行执行。 ​ newScheduleThreadPool。newScheduleThreadPool创建了一个固定长度的线程池，而且以延迟或定时的方式来执行任务。 ​ newFixedThreadPool和newCachedThreadPool这两个工厂方法返回通用的ThreadPoolExecutor实例，这些实例可以直接用来构造专门用途的executor。 1.3Executor的生命周期​ Executor的生命周期有3种状态：运行、关闭和已终止。ExecutorService在初始创建时处于运行状态。shutdown方法将执行平缓的关闭过程：不再接受新的任务，同时等待已经提交的任务执行完成——包括那些还未开始执行的任务。shutdownNow方法将执行粗暴的关闭过程：它将尝试取消所有运行中的人物，并且不再启动队列中尚未开始执行的任务。 ​ 在ExecutorService关闭后提交的任务将由“拒绝执行处理器（Rejected Execution Handler）”来处理，它会抛弃任务，或者使得execute方法抛出一个未检查的RejectedExecutionException。等所有任务完成后，ExecutorService将转入终止状态。可以调用awaitTermination来等待ExecutorService到达终止状态，或者通过isTerminated来轮询ExecutorService是否已终止。通常在调用awaitTermination之后会立即调用shutdown，从而产生同步地关闭ExecutorService的效果。 1.4延迟任务与周期任务​ 使用DelayQueue构建自己的调度服务，它实现了BlockingQueue，并为ScheduledThreadPoolExecutor提供调度功能。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"四.基础构建模块","slug":"四-基础构建模块","date":"2020-05-06T03:17:40.000Z","updated":"2020-05-14T03:32:39.854Z","comments":true,"path":"2020/05/06/四-基础构建模块/","link":"","permalink":"/2020/05/06/四-基础构建模块/","excerpt":"","text":"1.同步容器类​ 同步容器类包括Vector和Hashtable等，这些同步的封装器类是由Collections.synchronizedXxx等工厂方法创建的。这些类实现线程安全的方式是：将它们的状态封装起来，并对每个公有方法都进行同步，使得每次只有一个线程能访问容器的状态。 1.1同步容器类的问题​ 同步容器类都是线程安全的，但在某些情况下可能需要额外的客户端加锁来保护复合操作。容器上常见的复合操作包括：迭代（反复访问元素，直到遍历完容器中所有的元素）、跳转（根据制定顺序找到当前元素的下一个元素）以及条件运算。在同步容器类中，这些复合操作在没有客户端加锁的情况下仍然是线程安全的，但当其他线程并发地修改容器时，它们可能会表现出意料之外的行为。 2.并发容器​ 通过并发容器来代替同步容器，可以极大地提高伸缩性并降低风险。 2.1 ConcurrentHashMap​ 与HashMap一样，ConcurrentHashMap也是一个基于散列的Map，但它使用了一种完全不同的加锁策略来提供更高的并发性和伸缩性。ConcurrentHashMap并不是将每个方法都在同一个锁上同步并使得每次只能有一个线程访问容器，而是使用一种粒度更细的加锁机制来实现更大程序的共享，这种机制称为分段锁（Lock Striping）。 ​ ConcurrentHashMap与其他并发容器一起增强了同步容器类：它们提供的迭代器不会抛出ConcurrentModificationException，因此不需要在迭代过程中对容器加锁。ConcurrentHashMap返回的迭代器具有弱一致性（Weakly Consistent），而并非“及时失败”。弱一致性的迭代器可以容忍并发的修改，当创建迭代器时会遍历已有的元素，并可以（但是不保证）在迭代器被构造后将修改操作反映给容器。 ​ 在ConcurrentHashMap中没有实现对Map加锁以提供独占访问。在Hashtable和synchronizedMap中，获得Map的锁能防止其他线程访问这个Map。 2.2 CopyOnWriteArrayList​ CopyOnWriteArrayList用于替代同步List，在某些情况下它提供了更好的并发性能，并且在迭代期间不需要对容器进行加锁或复制。（类似地，CopyOnWriteArraySet的作用是替代同步Set） ​ “写入时复制（Copy-On-Write）”容器的线程安全性在于，只要正确地发布一个事实不可变的对象，那么在访问该对象时就不再需要进一步的同步。在每次修改时，都会创建并重新发布一个新的容器副本，从而实现可变性。“写入时复制”容器的迭代器保留一个指向底层基础数组的引用，这个数组当前位于迭代器的起始位置，由于它不会被修改，因此在对其进行同步时只需确保数组内容的可见性。因此，多个线程可以同时对这个容器进行迭代，而不会彼此干扰或者与修改容器的线程相互干扰。“写入时复制”容器返回的迭代器不会抛出ConcurrentModificationExecption，并且返回的元素与迭代器创建时的元素完全一致，而不必考虑之后修改操作所带来的影响。 3.阻塞队列和生产者-消费者模式​ 阻塞队列提供了可阻塞的put和take方法，以及支持定时的offer和poll方法。如果队列已经满了，那么put方法将阻塞直到有空间可用；如果队列为空，那么take方法将会阻塞直到有元素可用。队列可以是有界的也可以是无界的，无界队列永远都不会充满，所以无界队列的put方法永远也不会阻塞。 ​ 在基于阻塞队列构建的生产者-消费者设计中，当数据生成时，生产者把数据放入队列，而当消费者准备处理数据时，将从队列中获取数据。生产者不需要知道消费者的标识或数量，或者它们是否是唯一的生产者，而只需将数据放入队列即可。同样，消费者也不需要知道生产者是谁，或者工作来自何处。一种常见的生产者-消费者设计模式就是线程池与工作队列的组合，在Executor任务执行框架中就体现了这种模式。 ​ 阻塞队列提供了一个offer方法，如果数据项不能被添加到队列中，那么将返回一个失败状态。 ​ 在构建高可靠的应用程序时，有界队列是一种强大的资源管理工具：它们能抑制并防止产生过的工作项，使应用程序在负荷过载的情况下变得更加健壮。 ​ 在类库中包含了BlockingQueue的多种实现，其中，LinkedBlockingQueue和ArrayBlockingQueue是FIFO队列，二者分别与LinkedList和ArrayList类似，但比同步List拥有更好的并发性能。PriorityBlockingQueue是一个按优先级排序的队列。 ​ 最后一个BlockingQueue实现是SynchronousQueue，实际上它不是一个真正的队列，因为它不会为队列中元素维护存储空间。与其他队列不同的是，它维护一组线程，这些线程在等待把元素加入或移出队列。 3.1串行线程关闭​ 对于可变对象，生产者-消费者这种设计与阻塞队列一起，促进了串行线程封闭，从而将对象所有权从生产者交付给消费者。线程封闭对象只能由单个线程拥有，但可以通过安全发布对象来“转移”所有权。在转移所有权之后，也只有另一个线程能获得这个对象的访问限制，并且发布对象的线程不会再访问它。 3.2双端队列与工作密取​ 双端队列适用于另一种相关模式，即工作密取。在生产者-消费者设计中，所有消费者有一个共享的工作队列，而在工作密取设计中，每个消费者都有自己的双端队列。如果一个消费者完成了自己双端队列中的全部工作，那么它可以从其他消费者双端队列末尾秘密地获取工作。 4.阻塞方法与中断方法​ 线程可能会阻塞或暂停执行，原因有多种：等待I/O操作结束，等待获得一个锁，等待从Thread.sleep方法中醒来，或是等待另一个线程的计算结果。当线程阻塞时，它通常被挂起，并处于某种阻塞状态（BLOCKED、WAITING或TIMED_WAITING）。阻塞线程与执行时间很长的普通操作的差别在于，被阻塞的队列必须等待某个不受它控制的事件发生后才能继续执行。当某个外部事件发生时，线程被置回RUNNABLE状态，并可以再次被调度执行。 ​ Thread提供了interrupt方法，用于中断线程或者查询这个线程是否已经被中断。每个线程都有一个布尔类型的属性，表示线程的中断状态，当中断线程时讲设置这个状态。 ​ 中断是一种协作机制。一个线程不能强制其他线程停止正在执行的操作而去执行求他的操作。 ​ 挡在代码中调用了一个将抛出InterruptedExcepiton异常的方法时，自己的方法也就变成了一个阻塞方法，并且必须要处理对应中断的响应。对于库代码来说，有两种基本选择： ​ 传递InterruptedException。只需把InterruptedException传递给方法的调用者。传递InterruptedException的方法包括，根本不捕获该异常，或者捕获该异常，然后在执行某种简单的清理工作后再次抛出该异常。 ​ 恢复中断。有时候不能抛出InterruptedException，例如当代码是Runnable的一部分时。在这些情况下，必须捕获InterruptedException，并通过调用当前线程上的interrupt方法恢复中断状态，这样在调用栈中更高层的代码将看到引发了一个中断。 123456789101112public class TaskRunnable implements Runnable &#123; BlockingQueue&lt;Task&gt; queue; ... public void run() &#123; try &#123; processTask(queue.take()); &#125; catch (InterruptedException e) &#123; //恢复中断的状态 Thread.currentThread().interrupt(); &#125; &#125;&#125; 5.同步工具类5.1闭锁​ 闭锁是一种同步工具类，可以延迟线程的进度直到其到达终止状态。当闭锁到达结束状态之后，将不会再改变状态。闭锁可以确保某些活动直到其他活动都完成后才继续执行。 ​ CountDownLatch是一种灵活的闭锁实现，它可以使一个或多个线程等待一组事件发生。闭锁状态包括一个计数器，该计数器被初始化为一个正数，表示需要等待的事件数量。countDown方法递减计数器，表示有一个事件已经发生了，而await方法等待计数器达到零，这表示所有需要等待的事件都已经发生。如果计数器的值非零，那么await会一直阻塞到计数器为零，或者等待中的线程中断，或者等待超时。 123456789101112131415161718192021222324public long timeTasks(int nThreads, Runnable task) throws InterruptedException&#123; final CountDownLatch startGate = new CountDownLatch(1); final CountDownLatch endGate = new CountDownLatch(nThreads); for (int i = 0; i &lt; nThreads; i++) &#123; Thread t = new Thread(() -&gt; &#123; try &#123; startGate.await(); try&#123; task.run(); &#125; finally &#123; endGate.countDown(); &#125; &#125; catch (InterruptedException ignored) &#123;&#125; &#125;); t.start(); &#125; long start = System.nanoTime(); startGate.countDown(); endGate.await(); long end = System.nanoTime(); return end - start;&#125; #####5.2FutureTask ​ FutureTask也可以用做闭锁。FutureTask表示的计算是通过Callable来实现的，相当于一种可生成结果的Runnable，并且可以处于以下3种状态：等待运行（Waiting to run）,正在运行（Running）和运行完成（Completed）。 ​ Future.get()的行为取决于任务的状态。如果任务已经完成，那么get会立即返回结果，否则get将阻塞直到任务进入完成状态，然后返回结果或者抛出异常。 5.3信号量​ 计数信号量（Counting Semaphore）用来控制同时访问某个特定资源的操作数量，或者同时执行某个执行操作的数量。计数信号量还可以用来实现某种资源池，或者对容器施加边界。 ​ Semaphore中管理着一组虚拟的许可（permit），许可的初始数量可通过构造函数来指定。在执行操作时可以首先获得许可（只要还有剩余的许可），并在使用以后释放许可。如果没有许可，那么acquire将阻塞直到所有许可（或许直到被中断或操作超时）。release方法将返回一个许可给信号量。 1234567891011121314151617181920212223242526272829303132public class BoundedHashSet&lt;T&gt; &#123; private final Set&lt;T&gt; set; private final Semaphore sem; public BoundedHashSet(int bound) &#123; this.set = Collections.synchronizedSet(new HashSet&lt;T&gt;()); this.sem = new Semaphore(bound); &#125; public boolean add(T o) throws InterruptedException &#123; sem.acquire(); boolean wasAdded = false; try &#123; wasAdded = set.add(o); return wasAdded; &#125; finally &#123; if (!wasAdded) &#123; sem.release(); &#125; &#125; &#125; public boolean remove(T o) &#123; boolean wasRemove = set.remove(o); if (wasRemove) &#123; sem.release(); &#125; return wasRemove; &#125;&#125; 5.4栅栏​ 栅栏（Barrier）类似于闭锁，它能阻塞一组线程直到某个事件发生。栅栏与闭锁的关键区别在于，所有线程必须同时到达栅栏位置，才能继续执行。闭锁用于等待事件，而栅栏用于等待其他线程。 ​ CyclicBarrier可以使一定数量的参与方式反复地在栅栏位置汇集，它在并行迭代算法中非常有用：这种算法通常将一个问题拆分成一系列相互独立的子问题。当线程到达栅栏位置时将调用await方法，这个方法将阻塞直到所有线程到达栅栏位置。如果所有线程到达了栅栏位置，那么栅栏将打开，此时所有线程都被释放，而栅栏将重置以便下次使用。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"三.对象的组合","slug":"三-对象的组合","date":"2020-04-27T08:30:38.000Z","updated":"2020-05-06T03:16:24.195Z","comments":true,"path":"2020/04/27/三-对象的组合/","link":"","permalink":"/2020/04/27/三-对象的组合/","excerpt":"","text":"1.设计线程安全的类​ 在设计线程安全类的过程中，需要包含以下三个基本要素： 找出构成对象状态的所有变量。 找出约束状态变量的不变性条件。 建立对象状态的并发访问管理策略。 1.1收集同步需求​ 要确保类的线程安全性，就需要确保它的不变性条件不会在并发访问的情况下被破坏，这就需要对其状态进行推断。对象与变量都有一个状态空间，即所有可能的取值。状态空间越小，就越容易判断线程的状态。final类型的域使用得越多，就越能简化对象可能状态的分析过程。（在极端的情况中，不可变对象只有唯一的状态。） 1.2依赖状态的操作​ 类的不变性条件与后验条件约束了在对象上有哪些状态和状态转换是有效的。在某些对象的方法中还包含一些基于状态的先验条件。例如，不能从空队列中移除一个元素，在删除元素之前，队列必须处于非空的状态。如果在某个操作中包含有基于状态的先验条件，那么这个操作就被称为依赖状态的操作。 ​ 在单线程程序中，如果某个操作无法满足先验条件，那么就只能失败。但在并发程序中，先验条件可能会由于其他线程执行的操作而变成真。在并发程序中要一直等到先验条件为真，然后再执行该操作。 1.3状态的所有权​ 许多情况下，所有权与封装性总是相互关联的：对象封装它拥有的状态，反之也成立，即对它封装的状态拥有所有权。状态变量的所有者将决定采用何种加锁协议来维持变量状态的完整性。所有权意味着控制权。 2.实例封闭​ 封装简化了线程安全类的实现过程，它提供了一种实例封闭机制，通常也简称为“封闭”。当一个对象被封装到另一个对象中时，能够访问被封装对象的所有代码路径都是已知的。 ​ 将数据封装在对象内部，可以将数据的访问限制在对象的方法上，从而更容易确保线程在访问数据时总能持有正确的锁。 ​ 实例封闭式构建线程安全类的一种最简单方式，它还使得在锁策略的选择上拥有了更多的灵活性。只要自始至终都使用同一个锁，就可以保护状态。实例封闭还使得不同的状态变量可以由不同的锁来保护。 2.1Java监视器模式​ Java监视器模式仅仅是一种编写代码的约定，对于任何一种锁对象，只要自始至终都使用该锁对象，都可以用来保护对象的状态。 3.线程安全性的委托​ 如果一个变量是线程安全的，并且没有任何不变性条件来约束它的值，在变量的操作上也不存在任何不允许的状态转换，那么就可以安全地发布这个变量。 4.在现有的线程安全类中添加功能4.1客户端加锁机制123456789101112131415@hreadSafepublic class ListHelper&lt;E&gt; &#123; public List&lt;E&gt; list = Collections.synchronizedList(new ArrayList&lt;E&gt;()); ... public boolean pufIfAbsent(E x) &#123; synchronized (list) &#123; boolean absent = !list.contains(x); if (absent) &#123; list.add(x); &#125; return absent; &#125; &#125;&#125; 4.2组合12345678910111213141516@ThreadSafepublic class ImprovedList&lt;T&gt; implements List&lt;T&gt; &#123; private final List&lt;T&gt; list; public ImprovedList(List&lt;T&gt; list) &#123; this.list = list; &#125; public synchronized boolean putIfAbsent(T x) &#123; boolean contains = list.contains(x); if(contains) &#123; list.add(x); &#125; return !contains; &#125;&#125;","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"二.对象的共享","slug":"二-对象的共享","date":"2020-04-25T02:22:46.000Z","updated":"2020-04-28T02:24:33.705Z","comments":true,"path":"2020/04/25/二-对象的共享/","link":"","permalink":"/2020/04/25/二-对象的共享/","excerpt":"","text":"1.可见性1.1失效数据​ 在缺乏同步的程序中可能产生错误的一种情况：失效数据。当读线程查看某个变量时，可能会得到一个已经失效的值。更糟糕的是，失效值可能不会同时出现：一个线程可能获得某个变量的最新值，而获得另一个变量的失效值。 1.2非原子的64位操作​ 当线程在没有同步的情况下读取变量时，可能会得到一个失效值，但至少这个值是由之前某个线程设置的值，而不是一个随机值。这种安全性保证也被称为最低安全性。 ​ 最低安全性适用于绝大多数变量，但是存在一个例外：非volatile类型的64位数值变量(double和long)。Java内存模型要求，变量的读取操作和写入操作都必须是原子操作，但对于非volatile类型的long和double变量，JVM允许将64位的读操作或血操作分解为两个32位的操作。因此，即使不考虑失效数据问题，在多线程程序中使用共享且可变的long和double等类型的变量也是不安全的，除非用关键字volatile来生命它们，或者用锁保护起来。 1.3加锁与可见性​ 内置锁可以用于确保某个线程以一种可预测的方式来查看另一个线程的执行结果。 ​ 加锁的含义不仅仅局限于互斥行为，也包括内存可见性。为了确保所有线程都能看到共享变量的最新值，所有执行读操作或写操作的线程都必须在同一个锁上同步。 1.4Volatile变量​ Java语言提供了一种稍弱的同步机制，即volatile变量，用来确保将变量的更新操作通知到其他线程。当把变量声明为volatile类型后，编译器与处理器都会注意到这个变量是共享的，因此不会将该变量上的操作与其他内存一起重排序。volatile变量不会被缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。 ​ 仅当volatile变量能简化代码的实现以及对同步策略的验证时，才应该使用它们。如果在验证正确性时需要对可见性进行复杂的判断，那么就不要使用volatile变量。volatile变量的正确使用方式包括：确保它们自身状态的可见性，确保它们所引用对象的状态的可见性，以及标识一些重要的程序生命周期事件的发生（例如，初始化或关闭）。 ​ 加锁机制既可以确保可见性又可以确保原子性，而volatile变量只能确保可见性。 ​ 当且仅当满足以下所有条件时，才应该使用volatile变量： 对变量的写入操作不依赖变量的当前值，或者能确保只有单个线程更新变量的值 该变量不会与其他状态变量一起纳入不变性条件中。 在访问变量时不需要加锁。 2.发布与逸出​ “发布（Publish）一个对象的意思是指，使对象能够在当前作用域之外的代码中使用。当某个不应该发布的对象被发布时，这种情况就被称为逸出（Escape）。 3.线程封闭​ 当访问共享的可变数据时，通常需要使用同步。一种避免使用同步的方式就是不共享数据。如果仅在单线程内访问数据，就不需要同步。这种技术被称为线程封闭（Thread Confinement），它是实现线程安全性的最简单方式之一。当某个对象封闭在一个线程中时，这种用法将自动实现线程安全性，即使被封闭的对象本身不是线程安全的。 3.1Ad-hoc线程封闭​ Ad-hoc线程封闭是指，维护线程封闭性职责完全由程序实现来承担。 3.2栈封闭​ 栈封闭是线程封闭的一种特例，在栈封闭中，只能通过局部变量才能访问对象。 3.3ThreadLocal类​ 维持线程封闭性的一种更规范方法是使用ThreadLocal，这个类能使线程中的某个值与保持值的对象关联起来。ThreadLocal提供了get与set等访问接口或方法，这些方法为每个使用该变量的线程都存有一份独立的副本，因此get总是返回由当前执行线程在调用set时设置的最新值。 4.不变性​ 满足同步需求的另一种方法是使用不可变对象。如果某个对象在被创建后其状态就不能被修改，那么这个对象就被称为不可变对象。线程安全性是不可变对象的固有属性之一，不可变对象一定是线程安全的。 ​ 当满足以下条件时，对象才是不可变的： 对象创建以后其状态就不能修改。 对象的所有域都是final类型。 对象是正确创建的（在对象的创建期间，this引用没有逸出）。 4.1Final域​ final类型的域是不能修改的，在Java内存模型中，final域还有着特殊的语义。final域能确保初始化过程的安全性，从而可以不受限制地访问不可变对象，并在共享这些对象时无须同步。 5.安全发布5.1不可变对象与初始化安全性​ 任何线程都可以在不需要额外同步的情况下安全地访问不可变对象，即使在发布这些对象时没有使用同步。这种保证还将延伸到被正确创建对象中所有final类型的域。在没有额外同步的情况下，也可以安全地访问final类型的域。然而，如果final类型的域所指向的是可变对象，那么在访问这些域所指向的对象的状态时仍然需要同步。 5.2安全发布的常用模式​ 要安全地发布一个对象，对象的引用以及对象的状态必须同时对其他线程可见。一个正确构造的对象可以通过以下方式来安全地发布： 在静态初始化函数中初始化一个对象引用。 将对象的引用保存到volatile类型的域或者AtomicReferance对象中。 将对象的引用保存到某个正确构造对象的final类型域中。 将对象的引用保存到一个由锁保护的域中。 线程安全库中的容器类提供了以下的安全发布保证： 通过将一个键或值放入HashTable、synchronizedMap或者ConcurrentMap中，可以安全地将它发布给任何从这些容器中访问它的线程（无论是直接访问还是通过迭代器访问）。 通过将某个元素放入Vector、CopyOnWriteArrayList、CopyOnWriteArraySet、synchronizedList或者synchronizedSet中，可以将该元素安全地发布到任何从这些容器中访问该元素的线程。 通过将某个元素放入BlockingQueue或者ConcurrentLinkedQueue中，可以将该元素安全地发布到任何从这些队列中访问该元素的线程。 通常，要发布一个静态构造的对象，最简单和最安全的方式是使用静态的初始化器： 1public static Holder holder = new Holder(42); 5.3事实不可变对象​ 所有的安全发布机制都能确保，当对象的引用对所有访问该对象的线程可见时，对象发布时的状态对于所有线程也将是可见的，并且如果对象状态不会再改变，那么就足以确保任何访问都是安全的。 ​ 如果对象从技术上来看是可变的，但其状态再发布后不会再改变，那么把这种对象称为“事实不可变对象（Effectively Immutable Object）”。 ​ 在没有额外的同步的情况下，任何线程都可以安全地使用被安全发布的事实不可变对象。 5.4可变对象​ 如果对象在构造后可以修改，那么安全发布智能确保“发布当时”状态的可见性。对于可变对象，不仅在发布对象时需要使用同步，而且在每次对象访问时同样需要使用同步来确保后续修改操作的可见性。要安全地共享可变对象，这些对象就必须被安全地发布，并且必须是线程安全的或者由某个锁保护起来。 ​ 对象的发布需求取决于它的可变性： 不可变对象可以通过任意机制来发布。 事实不可变对象必须通过安全方式来发布。 可变对象必须通过安全方式来发布，并且必须是线程安全的或者由某个锁保护起来。 5.5安全地共享对象​ 在并发程序中使用和共享对象时，可以使用一些实用的策略，包括： ​ 线程封闭。线程封闭的对象只能由一个线程拥有，对象被封闭在该线程中，并且只能由这个线程修改。 ​ 只读共享。在没有额外同步的情况下，共享的只读对象可以由多个线程并发访问，但任何线程都不能修改它。共享的只读对象包括不可变对象和事实不可变对象。 ​ 线程安全共享。线程安全的对象在其内部实现同步，因此多个线程可以通过对象的公有接口来进行访问而不需要进一步的同步。 ​ 保护对象。被保护的对象只能通过持有特定的锁来访问。保护对象包括封装在其他线程安全对象中的对象，以及已发布的并且由某个特定锁保护的对象。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"一.线程安全性","slug":"一-线程安全性","date":"2020-04-20T06:12:46.000Z","updated":"2020-04-22T11:15:23.960Z","comments":true,"path":"2020/04/20/一-线程安全性/","link":"","permalink":"/2020/04/20/一-线程安全性/","excerpt":"","text":"1.定义​ 正确性的含义：某个类的行为与其规范完全一致。在良好的规范中通常会定义各种不变性条件来约束对象的状态，以及定义各种后验条件来描述对象操作的结果。 ​ 线程安全性：当多个线程访问某个类时，不管运行环境采用何种调度方式或者这些线程将如何交替执行，并且在调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么就称这个类是线程安全的。 ​ 无状态对象一定是线程安全的。 2.原子性2.1竞态条件​ 当某个计算的正确性取决于多个线程的交替执行时序时，那么就会发生竞态条件。最常见的竞态条件类型就是”先检查后执行（Check-Then-Act）“操作，即通过一个可能失效的观测结果来决定下一步的动作。 2.2复合操作​ 假定有两个操作A和B，如果以A的线程来看，当另一个线程执行B时，要么将B全部执行完，要么完全不执行B，那么A和B对彼此来说是原子的。原子操作是指，对于访问一个状态的所有操作（包括该操作本身）来说，这个操作是一个以原子方式执行的操作。 ​ 复合操作：包含了一组必须以原子方式执行的操作以及确保线程安全性。 3.加锁机制3.1内置锁​ Java提供了一种内置的锁机制来支持原子性：同步代码块。同步代码块包括两部分：一个作为锁的对象引用，一个作为由这个锁保护的代码。以关键字synchronized来修饰的方法就是一种横跨整个方法体的同步代码块，其中该同步代码块的锁就是方法调用所在的对象。 123synchronized(lock)&#123;//访问或修改由锁保护的共享状态&#125; ​ 每个Java对象都可以用做一个实现同步的锁，这些锁被称为内置锁或监视器锁。线程在进入同步代码块之前会自动获得锁，并且在退出同步代码块时自动释放锁，而无论是通过正常的控制路径退出，还是通过从代码中抛出异常退出。获得内置锁的唯一途径就是进入由这个锁保护的同步代码块或方法。 ​ Java的内置锁是互斥锁，意味着只有一个线程能持有这种锁。 3.2重入​ 当某个线程请求一个由其他线程持有的锁时，发出请求的线程就会阻塞。然而，由于内置锁是可重入的，因此如果某个线程试图获得一个已经由它自己持有的锁，那么这个请求就会成功。 ​ 重入的一种实现方法是，为每个锁关联一个获取计数值和一个所有者线程。当计数值为0时，这个锁就被认为是没有被任何线程持有。当线程请求一个未被持有的锁时，JVM将记下锁的持有者，并且将获取计数器置为1。如果同一个线程再次获取这个锁，计数值将递增，而当线程退出同步代码块时，计数器将递减。计数器值为0时，这个锁将被释放。 4.用锁来保护状态​ 由于锁能使其保护的代码路径以串行形式来访问，因此可以通过锁来构造一些协议以实现对共享状态的独占访问。只要始终遵循这些协议，就能确保状态的一致性。 ​ 对于可能被多个线程同时访问的可变状态变量，在访问它时都需要持有同一个锁，在这种情况下，我们称状态变量是由这个锁保护的。 ​ 对于每个包含多个变量的不变性条件，其中涉及的所有变量都需要由同一个锁来保护。 5.活跃与性能​ 当执行时间较长的计算或者可能无法快速完成的操作时（例如，网络I/O或者控制台I/O），一定不要持有锁。","categories":[{"name":"java并发编程实战笔记","slug":"java并发编程实战笔记","permalink":"/categories/java并发编程实战笔记/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"/tags/并发编程/"}]},{"title":"redis设计与实现-单机数据库的实现","slug":"redis设计与实现-单机数据库的实现","date":"2019-12-29T08:44:46.000Z","updated":"2020-01-15T07:07:17.231Z","comments":true,"path":"2019/12/29/redis设计与实现-单机数据库的实现/","link":"","permalink":"/2019/12/29/redis设计与实现-单机数据库的实现/","excerpt":"","text":"1. 数据库1.1 服务器中的数据库​ Redis服务器将所有数据库都保存在服务器状态redis.h/redisServer结构的db数组中，db数组的每个项都是一个redis.h/redisDb结构，每个redisDb结构代表一个数据库： 123456struct redisServer&#123; //... //一个数组，保存着服务器中的所有数据库 redisDb *db; //...&#125;; ​ 在初始化服务器时，程序会根据服务器状态的dbnum属性来决定应该创建多少个数据库: 123456struct redisServer&#123; //... //服务器的数据库数量 int dbnum; //...&#125;; ​ dbnum属性的值由服务器配置的database选项决定，默认情况下，该选项的值为16，所以Redis服务器默认会创建16个数据库。 1.2 切换数据库​ 每个Redis客户端都有自己的目标数据库，每当客户端执行数据库写命令或者数据库读命令的时候，目标数据库就会成为这些命令的操作对象。 ​ 默认情况下，Redis客户端的目标数据库为0号数据库，但客户端可以通过SELECT命令来切换目标数据库。 1.3 数据库键空间​ Redis是一个键值对数据库服务器，服务器中的每个数据库都由一个redis.h/redisDb结构表示，其中，redisDb结构的dict字典保存了数据库中的所有键值对，我们将这个字典称为键空间： 123456typedef struct redisDb&#123; //... //数据库键空间，保存着数据中的所有键值对 dict *dict; //...&#125;redisDb; ​ 键空间和用户所见的数据库是直接对应的： 键空间的键也就是数据库的键，每个键都是一个字符串对象。 键空间的值也就是数据库的值，每个值可以是字符串对象、列表对象、哈希表对象、集合对象和有序集合对象中的任意一种Redis对象。 1.4 读写键空间时的维护操作​ 当使用Redis命令对数据库进行读写时，服务器不仅会对键空间执行指定的读写操作，还会执行一些额外的维护操作，其中包括： 在读取一个键之后（读操作和写操作都要对键进行读取），服务器会根据键是否存在来更新服务器的键空间命中（hit）次数或键空间不命中（miss）次数，这两个值可以在INFO stats命令的keyspace_hits属性和keyspace_misses数量中查看。 在读取一个键之后，服务器会更新键的LRU（最后一次使用）时间，这个值可以用于计算键的闲置时间，是呀OBJECT idletime命令可以查看键key的闲置时间。 如果服务器在读取一个键时发现该键已经过期，那么服务器会先删除这个过期键，然后才执行与下的其他操作。 如果有客户端使用WATCH命令监视了某个键，那么服务器在对被监视的键进行修改之后，会将这个键标记为脏（dirty），从而让事务程序注意到这个键已经被修改过。 服务器每次修改一个键之后，都会对脏键计数器的值增1，这个计数器会触发服务器的持久化以及复制操作。 如果服务器开启了数据库通知功能，那么在对键进行修改之后，服务器将按配置发送相应的数据库通知。 1.5 设置键的生存时间或过期时间​ 通过EXPIRE命令或者PEXPIRE命令，客户端可以以秒或者毫秒精度为数据库中的某个值设置生存时间（Time To Live，TTL），在经过指定的秒数或者毫秒数之后，服务器就会自动删除生存时间为0的键。 ​ 与EXPIRE命令和PEXPIRE命令类似，客户端可以通过EXPIREAT命令或PEXPIREAT命令，以秒或者毫秒精度给数据中的某个键设置过期时间。 ​ 过期时间是一个UNIX时间戳，当键的过期时间来临时，服务器就会自动从数据库中删除这个键。 1.5.1 设置过期时间​ Redis有四个不同的命令可以用来设置键的生存时间（键可以存在多久）或过期时间（键什么时候会被删除）： EXPIRE命令用于将键key的生存时间设置为ttl秒。 PEXPIRE命令用于将键key的生存时间设置为ttl毫秒。 EXPIREAT命令用于将键key的过期时间设置为timestamp所指定的秒数时间戳。 PEXPIREAT命令用于将键key的过期时间设置为tempstamp所制定的毫秒数时间戳。 ​ EXPIRE、PEXPIRE、EXPIREAT三个命令都是使用PEXPIREAT命令来实现的：无论客户端执行的是以上四个命令中的哪一个，经过转换之后，最终的执行效果都和执行PEXPIREAT命令一样。 1.5.2 保存过期时间​ redisDb结构的expires字典保存了数据库中所有键的过期时间，这个字典被称为过期字典： 过期字典的键是一个指针，这个指针指向键空间中的某个键对象（也即是某个数据库键）。 过期字典的值是一个long long类型的整数，这个整数保存了键所指向的数据库键的过期时间——一个毫秒精度的UNIX时间戳。 123456typedef struct redisDb&#123; //... //过期字典，保存着键的过期时间 dict *expires; //...&#125;redisDb; 1.5.3 移除过期时间​ PERSIST命令可以移除一个键的过期时间。PERSIST命令就是PEXPIREAT命令的反操作：PERSIST命令可以在过期字典中查找给定的键，并解除键和值（过期时间）在过期字典中的关联。 1.5.4 计算并返回剩余生存时间​ TTL命令以秒为单位返回键的剩余生存时间，而PTTL命令则以毫秒为单位返回键的剩余生存时间。 1.5.5 过期键的判断​ 通过过期字典，程序可以用以下步骤检查一个给定键是否过期： 检查给定键是否存在于过期字典：如果存在，那么取得键的过期时间。 检查当前UNIX时间戳是否大于键的过期时间：如果是的话，那么键已经过期；否则的话，键未过期。 1.6 Redis的过期键删除策略​ Redis服务器使用的是惰性删除和定期删除两种策略。 1.6.1 惰性删除策略的实现​ 过期键的惰性删除策略由db.c/expirelfNeeded函数实现，所有读写数据库的Redis命令在执行之前都会调用expirelfNeeded函数对输入键进行检查： 如果输入键已经过期，那么expirelfNeeded函数将输入键从数据中删除。 如果输入键未过期，那么expirelfNeeded函数不做动作。 1.6.2 定期删除策略的实现​ 过期键的定期删除策略由redis.c/activeExpireCycle函数实现，每当Redis的服务器周期性操作redis.c/serverCron函数执行时，activeExpireCycle函数就会被调用，它在规定的时间内，分多次遍历服务器中的各个数据库，从数据库的expires字典中随机检查一部分键的过期时间，并删除其中的过期键。 1.7 AOF、RDB和复制功能对过期键的处理1.7.1 生成RDB文件​ 在执行SAVE命令或者BGSAVE命令创建一个新的RDB文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的RDB文件中。 1.7.2 载入RDB文件​ 在启动Redis服务器时，如果服务器开启了RDB功能，那么服务器将对RDB文件进行载入： 如果服务器以主服务器模式运行，那么在载入RDB文件时，程序会对文件中保存的键进行检查，未过期的键会被载入到数据库中，而过期键则会被忽略，所以过期键对载入RDB文件的主服务器不会造成影响。 如果服务器以服务器模式运行，那么在载入RDB文件时，文件中保存的所有键，不论是否过期，都会被载入到数据中。不过，因为主从服务器在进行数据同步的时候，从服务器的数据库就会被清空，所以一般来讲，过期键对载入RD B文件的从服务器也不会造成影响。 1.7.3 AOF文件写入​ 当服务器以AOF持久化模式运行时，如果数据库中的某个键已经过期，但它还没有被惰性删除或者定期删除，那么AOF文件不会因为这个过期键而产生任何影响。 ​ 当过期键被惰性删除或者定期删除之后，程序会向AOF文件追加（append）一条DEL命令，来显示地记录该键已被删除。 1.7.4 AOF重写​ 和生成RDB文件时类似，在执行AOF重写的过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中。使用BGREWRITEAOF命令。 1.7.5 复制​ 当服务器运行在复制模式下时，从服务器的过期键删除动作由主服务器控制： 主服务器在删除一个过期键之后，会显示地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键。 从服务器在执行客户端发送的读命令时，即是碰到过期键也不会将过期键删除，而是继续像处理未过期的键一样来处理过期键。 从服务器只有在接到主服务器发来的DEL命令之后，才会删除过期键。 1.8 数据库通知​ 数据库通知是Redis 2.8版本新增加的功能，这个功能可以让客户端通过订阅给定的频道或者模式，来获知数据库中键的变化，以及数据库中命令的执行情况。 2. RDB持久化​ RDB持久化功能所生成的RDB文件是一个经过压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态。 ​ 因为RDB文件是保存在硬盘里面的，所以即使Redis服务器进程退出，甚至运行Redis服务器的计算机停机，但只要RDB文件仍然存在，Redis服务器就可以用它来还原数据库状态。 2.1 RDB文件的创建与载入​ 有两个Redis命令可以用于生成RDB文件，一个是SAVE，另一个是BGSAVE。 ​ SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求。 ​ 和SAVE命令直接阻塞服务器进程的做法不同，BSAVE命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程继续处理命令请求。 ​ 和使用SAVE命令或者BGSAVE命令创建RDB文件不同，RDB文件的载入工作是在服务器启动时自动执行的，所以Redis并没有专门用于载入RDB文件的命令，只要Redis服务器在启动时检测到RDB文件存在，它就会自动载入RDB文件。 ​ 因为AOF文件的更新频率通常比RDB文件的更新频率高，所以： 如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态。 只有在AOF持久化功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态。 2.1.1 SAVE命令执行时的服务器状态​ 当SAVE命令执行时，Redis服务器会被阻塞，所以当SAVE命令正在执行时，客户端发送的所有命令请求都会被拒绝。 ​ 只有在服务器执行完SAVE命令、重新开始接受命令请求之后，客户端发送的命令才会被处理。 2.1.2 BGSAVE命令执行时的服务器状态​ 因为BGSAVE命令的保持工作是由子进程执行的，所以在子进程创建RDB文件的过程中，Redis服务器仍然可以继续处理客户端的命令请求，但是，在BGSAVE命令执行期间，服务器处理SAVE、BGSAVE、BGREWRITEAOF三个命令的方式和平时有所不同。 ​ 首先，在BGSAVE命令执行期间，客户端发送的SAVE命令会被拒绝，服务器禁止SAVE命令和BGSAVE命令同时执行是为了避免父进程和子进程同时执行两个rdbSave调用，防止产生竞争条件。 ​ 其次，在BGSAVE命令执行期间，客户端发送的BGSAVE命令会被服务器拒绝，u因为同时执行两个BGSAVE命令也会产生竞争条件。 ​ 最后，BGREWRITEAOF和BGSAVE两个命令不能同时执行： 如果BGSAVE命令正在执行，那么客户端发送的BGREWRITEAOF命令会被延迟到BGSAVE命令执行完毕之后执行。 如果BGREWRITEAOF命令正在执行，那么客户端发送的BGSAVE命令会被服务器拒绝。 2.1.3 RDB文件载入时的服务器状态​ 服务器在载入RDB文件期间，会一直处于阻塞状态，直到载入工作完成为止。 2.2 自动间隔性保存​ 因为BGSAVE命令可以在不阻塞服务器进程的情况下执行，所以Redis允许用户通过设置服务器配置的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令。 ​ 用户可以通过save选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行BGSAVE命令。 2.2.1 设置保存条件​ 当Redis服务器启动时，用户可以通过指定配置文件或传入启动参数的方式设置save选项，如果用户没有主动设置save选项，那么服务器会为save选项设置默认条件： 123save 900 1save 300 10save 60 10000 ​ 接着，服务器程序会根据save选项设置的保存条件，设置服务器状态redisServer结构的saveparams属性： 123456struct redisServer&#123; //... //记录了保存条件的数组 struct saveparam *saveparams; //...&#125;; ​ saveparams属性是一个数组，数组中的每一个元素都是一个saveparam结构，每个saveparam结构都保存了一个save选项设置的保存条件： 123456struct saveparam&#123; //秒数 time_t seconds; //修改数 int changes;&#125;; 2.2.2 dirty计数器和lastsave属性​ 除了saveparams数组之外，服务器状态还维持着一个ditry计数器，以及一个lastsave属性： dirty计数器记录距离上一次成功执行SAVE命令或者BGSAVE命令之后，服务器对数据库状态（服务器中的所有数据库）进行了多少次修改（包括写入、删除、更新等操作）。 lastsave属性是一个UNIX时间戳，记录了服务器上一次成功执行SAVE命令或者BGSAVE命令的时间。 12345678struct redisServer&#123; //... //修改计数器 long long ditry; //上一次执行保存的时间 time_t lastsave; //...&#125;; 2.2.3 检查保存条件是否满足​ Redis的服务器周期性操作函数serverCron默认每隔100毫秒就会执行一次，该函数用于对正在运行的服务器进行维护，它的其中一项工作就是检查save选项所设置的保存条件是否已经满足，如果满足的话，就执行BGSAVE命令。 ​ 程序会遍历并检查saveparams数组中的所有保存条件，只要有任意一个条件被满足，那么服务器就会执行BGSAVE命令。 2.3 RDB文件结构1REDIS|db_version|databases|EOF|check_sum ​ RDB文件的最开头时REDIS部分，这个部分的长度为5字节，保存“REDIS”五个字符。通过这五个字符，程序可以在载入文件时，快速检查所载入的文件是否是RDB文件。 ​ db_version长度为4字节，它的值是一个字符串表示的整数，这个整数记录了RDB的版本号。 ​ databases部分包含着零个或任意多个数据库，以及各个数据库中的键值对数据： 如果服务器的数据库状态为空（所有数据库都是空的），那么这个部分也为空，长度为0字节。 如果服务器的数据库状态为非空（有至少一个数据库非空），那么这个部分也为非空，根据数据库所保持键值对的数量、类型和内容不同，这个部分的长度也会有所不同。 ​ EOF常量的长度为1字节，这个常量标志着RDB文件正文内容的结束，当读入程序遇到这个值的时候，它知道所有数据库的所有键值对都已经载入完毕了。 ​ check_sum是一个8字节长的无符号整数，保存着一个校验和，这个校验和是程序通过对REDIS、db_version、databases、EOF四个部分的内容进行计算得出的。服务器在载入RDB时，会将载入数据所计算出的校验和与check_sum所记录的校验和进行对比，以此来检查RDB文件是否有出错或者损坏的情况出现。 2.3.1 databases部分​ 一个RDB文件的databases部分可以保存任意多个非空数据库。 ​ 例如，如果服务器的0号数据库和3号数据库非空，那么服务器将创建如下图所示的RDB文件，图中的database 0代表0号数据库中的所有键值对数据，而database 3代表3号数据库中的所有键值对数据。 1REDIS|db_version|database 0|database 3|EOF|check_sum ​ 每个非空数据库在RDB文件中都可以保存为SELECTDB、db_number、key_value_pairs三个部分。 1SELECTDB|db_number|key_value_pairs ​ SELECTDB常量的长度为1字节，当读入程序遇到这个值的时候，它知道接下来要读入的将是一个数据库号码。 ​ db_number保存着一个数据库号码，根据号码的大小不同，这个部分的长度可以是1字节、2字节或者5字节。当程序读儒db_number部分之后，服务器会调用SELECT命令，根据读入的数据库号码进行数据库切换，使得之后读入的键值对可以载入到正确的数据库中。 ​ key_value_pairs部分保存了数据库中的所有键值对数据，如果键值对带有过期时间，那么过期时间也会和键值对保存在一起。 2.3.2 key_value_pairs部分​ RDB文件中的每个key_value_pairs部分都保存了一个或以上数量的键值对，如果键值对带有过期时间的话，那么键值对的过期时间也会被保存在内。 ​ 不带过期时间的键值对在RDB文件中由TYPE、key、value三部分组成。 1TYPE|key|value ​ 带有过期时间的键值对在RDB文件中的结构： 1EXPIRETIME_MS|ms|TYPE|key|value 3. AOF持久化​ 与RDB持久化通过保存数据库中的键值对来记录数据库状态不同，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态。 3.1 AOF持久化的实现​ AOF持久化功能的实现可以分为命令追加、文件写入、文件同步三个步骤。 3.1.1 命令追加​ 当AOF持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾。 3.1.2 AOF文件的写入与同步​ 因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到aof_buf缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用flushAppendOnlyFile函数，考虑是否需要将aof_buf缓冲区中的内容写入和保存到AOF文件里面。 appendfsync选项的值 flushAppendOnlyFile函数的行为 always 将aof_buf缓冲区中的所有内容写入并同步到AOF文件 everysec 将aof_buf缓冲区中的所有内容写入并同步到AOF文件，如果上次同步AOF文件的时间距离现在超过一秒钟，那么在此对AOF文件进行同步，并且这个同步操作是由一个线程专门负责执行 no 将aof_buf缓冲区中的所有内容写入到AOF文件，但并不对AOF文件进行同步，何时同步由操作系统来决定 ​ 系统提供了fsync和datasync两个同步函数，它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里吗，从而确保写入数据的安全性。 3.2 AOF文件的载入与数据还原","categories":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}],"tags":[]},{"title":"redis设计与实现-数据结构与对象","slug":"redis设计与实现-数据结构与对象","date":"2019-12-01T05:15:24.000Z","updated":"2019-12-29T08:41:36.836Z","comments":true,"path":"2019/12/01/redis设计与实现-数据结构与对象/","link":"","permalink":"/2019/12/01/redis设计与实现-数据结构与对象/","excerpt":"","text":"1. 简单动态字符串​ Redis没有直接使用C语言传统的字符串表示，而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型，并将SDS用作Redis的默认字符串表示。 ​ 除了用来保存数据库中的字符串值之外，SDS还被用作缓冲区：AOF模块中的AOF缓冲区，以及客户端状态中的输入缓冲区，都是由SDS实现的。 1.1 SDS的定义123456789struct sdshdr&#123; //记录buf数组中已使用字节的数量 //等于SDS所保存字符串的长度 int len; //记录buf数组中未使用字节的数量 int free; //字节数组，用于保存字符串 char buf[];&#125;; 1.2 SDS与C字符串的区别通过未使用空间(free)，SDS实现了空间预分配和惰性空间释放两种优化策略。 空间预分配 空间预分配用于优化SDS的字符串增长操作：当SDS的API对一个SDS进行修改，并且需要对SDS进行空间扩展的时候，程序不仅会为SDS分配修改所必须要的空间，还会为SDS分配额外的未使用空间。 其中，额外分配的未使用空间数量由以下公式决定： 如果对SDS进行修改之后，SDS的长度（也就是len属性的值）将小于1MB，那么程序分配和len属性同样大小的未使用空间，这是SDS len属性的值和free形同。 如果对SDS进行修改之后，SDS的长度将大于等于1MB，那么程序会分配1MB的未使用空间。 惰性空间释放 惰性空间释放用于优化SDS的字符串缩短操作：当SDS的API需要缩短SDS保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用。 C字符串 SDS 获取字符串长度的复杂度为O(N) 获取字符串长度的复杂度为O(1) API是不安全的，可能会造成缓冲区溢出 API是安全的，不会造成缓冲区溢出 修改字符串长度N次必然需要执行N次内存重分配 修改字符串长度N次最多需要执行N次内存重分配 只能保存文本数据 可以保存文本或者二进制数据 可以使用所有&lt;string.h&gt;库中的函数 可以使用一部分&lt;string.h&gt;库中的函数 1.3 SDS API 函数 作用 时间复杂度 sdsnew 创建一个包含给定C字符串的SDS O(N),N为给定C字符串的长度 sdsempty 创建一个不包含任何内容的空SDS O(1) sdsfree 释放给定的SDS O(N),N为被释放SDS的长度 sdslen 返回SDS的已使用空间节数 O(1),使用SDS的len属性获得 sdsvail 返回SDS的未使用空间节数 O(1),使用SDS的free属性获得 sdsdup 创建一个给定SDS的副本(copy) O(N),N为给定SDS的长度 sdsclear 清空SDS保存的字符串内容 因为惰性空间释放策略,O(1) sdscat 将给定C字符串拼接到SDS字符串的末尾 O(N),N为被拼接C字符串的长度 sdscatsds 将给定SDS字符串拼接到另一个SDS字符串的末尾 O(N),N为被拼接SDS字符串的长度 sdscpy 将给定的C字符串复制到SDS里面，覆盖SDS原有的字符串 O(N),N为被复制C字符串的长度 sdsgrowzero 用空字符将SDS扩展至给定长度 O(N),N为扩展新增的字节数 sdsrange 保留SDS给定区间内的数据，不在区间内的数据会被覆盖或清除 O(N),N为被保留数据的字节数 sdstrim 接受一个SDS和一个C字符串作为参数，从SDS中移除所有在C字符串中出现过的字符串 O(N^2),N为给定C字符串的长度 sdscmp 对比两个SDS字符串是否相同 O(N),N为两个SDS中较短的那个SDS的长度 2. 链表​ 每个链表节点使用一个listNode结构来表示： 12345678typedes struct listNode&#123; //前置节点 struct listNode *prev; //后置节点 struct listNode *next; //节点的值 void*value;&#125;listNode; ​ 多个listNode可以通过prev和next指针组成双端链表。 ​ 使用list来持有链表，操作会更方便： 1234567891011121314typedef struct list&#123; //表头节点 listNode *head; //表尾节点 listNode *tail; //链表所包含的节点数量 unsigned long len; //节点值复制函数 void *(*dup)(void *ptr); //节点值释放函数 void (*free)(void *ptr); //节点值对比函数 int (*match)(void *ptr,void *key);&#125;list; ​ Redis的链表实现的特性如下： 双端：链表节点带有prev和next指针，获取某个节点的前置节点和后置节点的复杂度都是O(1)。 无环：表头节点的prev指针和表尾节点的next指针都指向NULL，对链表的访问以NULL为终点。 带表头指针和表尾指针：通过list结构的head指针和tail指针，程序获取链表的表头节点和表尾节点的复杂度为O(1)。 带链表长度计数器：程序使用list结构和len属性来对list持有的链表节点进行计数，程序获取链表中节点数量的复杂度为O(1)。 多态：链表节点使用void*指针来保存节点值，并且可以通过list结构的dup、f ree、match三个属性为节点值设置类型特点函数，所以链表可以用于保存各种不同类型的值。 3. 字典​ 字典，又称为符号表(Symbol table)、关联数组(associative array)或映射(map)，是一种用于保存键值对的抽象数据结构。 3.1 字典的实现​ Redis的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。 哈希表 1234567891011typedef struct dictht&#123; //哈希表数组 dictEntry **table; //哈希表大小 unsigned long size; //哈希表大小掩码，用于计算索引值 //总是等于size-1 unsigned long sizemask; //该哈希表已有节点的数量 unsigned long used;&#125;dictht; table属性是一个数组，数组中的每个元素都是一个指向dictEntry结构的指针，每个dictEntry结构都保存着一个键值对。size属性记录了哈希表的大小，也即是table数组的大小，而used属性则纪录了哈希表目前已有节点（键值对）的数量。sizemask属性的值总是等于size-1，这个属性和哈希值一起决定一个键应该被放到table数组的哪个索引上面。 哈希表节点 哈希表节点使用dictEntry结构表示，每个dictEntry结构都保存着一个键值对： 123456789101112typedef struct dictEntry&#123; //键 void *key; //值 union&#123; void *val; uint64_tu64; int64_ts64; &#125;v; //指向下个哈希表节点，形成链表 struct dictEntry *next；&#125;dictEntry; key属性保存着键值对中的键，而v属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个uint64_t整数，又或者是一个int64_t整数。 next属性是指向另一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对连接在一起，以此来解决键冲突的问题。 字典 1234567891011typedef struct dict&#123; //类型特定函数 dictType *type; //私有数据 void *privdata; //哈希表 dictht ht[2]; //rehash索引 //当rehash不在进行时，值为-1 int trehashidx;/* rehashing not in progress if rehashidx == -1*/&#125;dict; ​ type属性时一个指向dictTYpe结构的指针，每个dictType结构保存了一簇用于操作特定类型键值对的函数，Redis会为用途不同的字典设置不同的类型特定参数。 ​ 而privdata属性则保存了需要传给那些类型特定函数的可选参数。 ​ ht属性是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表，一般情况下，字典只使用ht[0]哈希表，ht[1]哈希表只会在对ht[0]哈希表进行rehash时使用。 3.2 哈希算法​ 当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，就包含新键值对的哈希表节点放到哈希表数组的指定索引上面。 ​ Redis计算哈希值和索引值的方法如下： 1234hash = dict-&gt;type-&gt;hashFunction(key);#使用哈希表的sizemask属性和哈希值，计算出索引值#根据情况不同，ht[x]可以是ht[0]或者ht[1]index = hash &amp; dict-&gt;ht[x].sizemask; 3.3 解决键冲突​ 当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时，称这些键发生了冲突（collision）。 ​ Redis的哈希表使用链地址法(separate chaining)来解决键冲突，每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。 ​ 因为dictEntry节点组成的链表没有指向链尾表尾的指针，所以为了速度考虑，程序总是将新节点添加到链表的表头位置（复杂度为O(1)），排在其他已有节点的前面。 3.4 rehash​ 随着操作的不断执行，哈希表保存的键值对会逐渐地增多或者减少，为了让哈希表的负载因子（load factor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或者收缩。 ​ 扩展和收缩哈希表的工作可以通过rehash操作来完成，Redis对字典的哈希表执行rehash的步骤如下： 1)为字典的ht[1]哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及ht[0]当前包含的键值对数量（也即是ht[0].used属性的值）： 如果执行的是扩展操作，那么ht[1]的大小为第一个大于等于ht[0].used*2的2^n； 如果执行的是收缩操作，那么ht[1]的大小为第一个大于等于ht[0].used的2^n。 2)将保存在ht[0]中的所有键值对rehash到ht[1]上面：rehash指的是重新计算键的哈希值和索引值，然后将键值对放置到ht[1]哈希表的指定位置上。 3)当ht[0]包含的所有键值对都迁移到了ht[1]之后（ht[0]变为空表），释放ht[0]，将ht[1]设置为ht[0]，并在ht[1]新创建一个空白哈希表，为下一次rehash做准备。 哈希表的扩展与收缩 ​ 当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的负载因子大于等于1。 服务器目前正在执行BGSAVE命令或者BSREWRITEAOF命令，并且哈希表的负载因子大于等于5。 其中哈希表的负载因子可以通过公式： 12#负载因子=哈希表已保存节点数量/哈希表大小load_factor = ht[0].used / ht[0].size； ​ 当哈希表的负载因子小于0.1时，程序自动开始对哈希表进行收缩操作。 4. 跳表​ 跳表(skiplist)是一种有序数据结构，它通过在每个节点维持多个指向其他节点的指针，从而达到快速访问节点的目的。 ​ Redis使用跳表作为有序集合键的底层实现之一，如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员是比较长的字符串时，Redis就会使用跳跃表来作为有序集合键的底层实现。 ​ Redis只在两个地方用到了跳表，一个是实现有序集合键，另一个是在集群节点中作用内部数据结构。 4.1 跳表的实现​ Redis的跳表由zskiplistNode和zskiplist两个结构定义，其中zskiplistNode结构用于表示跳表节点，而zskiplist结构则用于保存跳表节点的相关信息。 跳表节点： 123456789101112131415typedef struct zskiplistNode&#123; //层 struct zskiplistLevel&#123; //前进指针 struct zskiplistNode *forward; //跨度 unsigned int span; &#125;level[]; //后退指针 struct zskiplistNode *backward; //分值 double score; //成员对象 robj *obj;&#125;zskiplistNode; 层 ​ 跳表节点的level数组可以包含的多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。 ​ 每次创建一个新跳表节点的时候，程序都根据幂次定律（power law，越大的数出现的概率越小）随机生成一个介于1和32之间的值作为level数组的大小，这个大小就是层的“高度”。 前进指针 ​ 每个层都有一个指向表尾方向的前进指针(level[i].forward属性)，用于从表头向表尾方向访问节点。 跨度 ​ 层的跨度(level[i].span属性)用于记录两个节点之间的距离： 两个节点之间的跨度越大，他们相距得就越远。 指向NULL的所有前进指针的跨度都为0，因为它们没有连向任何节点。 后退指针 ​ 节点的后退指针(backward属性)用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。 分值和成员 ​ 节点的分值(score属性)是一个double类型的浮点数，跳表中的所有节点都按分值从小到大来排序。 ​ 节点的成员对象(obj属性)是一个指针，它指向一个字符串对象，而字符串对象则保存着一个SDS值。 4.2 跳表​ 仅靠多个跳表节点就可以组成一个跳表。 ​ 但通过使用一个zskiplist结构来持有这些节点，程序可以更方便地对整个跳表进行处理，比如快速访问跳表的表头节点和表尾节点，或者快速地获取跳表节点的数量（也即是跳表的长度）。 12345678typedef struct zskiplist&#123; //表头节点和表尾节点 structz skiplistNode *header, *tail; //表中节点的数量 unsigned long length; //表中层数最大的节点的层数 int level;&#125;zskiplist; ​ header和tail指针分别指向跳跃表的表头和表尾节点，通过这两个指针，程序定位表头节点和表尾节点的时间复杂度为O(1)。 ​ 通过使用length属性来记录节点的数量，程序可以在O(1)复杂度内返回跳跃表的长度。 ​ level属性泽用于在O(1)复杂度内获取跳跃表中层高最大的那个节点的层数量，表头节点的层高并不计算在内。 5. 整数集合​ 整数集合(intset)是集合健的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis就会使用整数集合作为集合键的底层实现。 5.1 整数集合的实现​ 整数集合(intset)是Redis用于保存整数值的集合抽象数据结构，它可以保存类型为int16_t、int32_t或者int64_t的整数值，并且保证集合中不会出现重复元素。 12345678typedef struct intset&#123; //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[];&#125;intset; ​ contents数组是整数集合的底层实现：整数集合的每个元素都是contents数组的一个数组项(item)，各个项在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复元素。 ​ length属性记录了整数集合包含的元素数量，也即是contents数组的长度。 ​ 虽然intset结构将contents属性声明为int8_t类型的数组，但实际上contents数组并不保存任何int8_t类型的值，contents数组的真正类型取决于encoding属性的值。 5.2 升级​ 每当要将新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级（upgrdae），然后才能将新元素添加到整数集合里面。 ​ 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性质不变。 将新元素添加到底层数组里面。 ​ 因为每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中已有的所有元素进行类型转换，所以向整数集合添加新元素的时间复杂度为O(N)。 5.3 升级的好处​ 整数集合的升级策略有两个好处，一个是提升整数集合的灵活性，另一个是尽可能地节约内存。 5.4 降级​ 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。 6. 压缩列表​ 压缩列表(ziplist)是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么Redis就会使用压缩列表来做列表键的底层实现。 ​ 当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么Redis就会使用压缩列表来做哈希键的底层实现。 6.1 压缩列表的构成​ 压缩列表是Redis为了节约内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型(sequential)数据结构。一个压缩列表可以包含任意多个节点(entry)，每个节点可以保存一个字节数组或者一个整数值。 ​ 压缩列表的各个组成部分 zlbytes｜zltail｜zllen｜ entry1 ｜entry2 ｜ … ｜ entryN ｜zlend ​ 各个部分的详细说明 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数 zltail uint32_t 4字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节 zllen uint16_t 2字节 记录了压缩列表包含的节点数量 entryX 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定 zlend uint8_t 1字节 特殊值0xFF(十进制255)，用于标记压缩列表的末端 6.2 压缩列表节点的构成​ 每个压缩列表节点可以保存一个字节数组或者一个整数值，其中，字节数组可以是以下三种长度的其中一种： 长度小于等于63(26-1)字节的字节数组； 长度小于等于16383(214-1)字节的字节数组； 长度小于等于4294967295(232-1)字节的字节数组； 而整数值则可以是以下六种长度的其中一种： 4位长，介于0至12之间的无符号整数； 1字节长的有符号整数； 3字节长的有符号整数； int16_t类型整数; int32_t类型整数; int64_t类型整数。 每个压缩列表节点都由previous_entry_length、encoding、content三个部分组成。 previous_entry_length 节点的previous_entry_length属性以字节为单位，记录了压缩列表中前一个节点的长度。previous_entry_length属性的长度可以是1字节或者5字节。 encoding 节点的encoding属性记录了节点的content属性所保存数据的类型以及长度 content 节点的content属性负责保存节点的值，节点值可以是一个字节数组或者整数，值的类型和长度由节点的encoding属性决定。 7. 对象​ Redis基础上述数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象。 7.1 对象的类型和编码​ Redis使用对象来表示数据库中的键和值，每当在Redis的数据库中新创建一个键值对时，至少会创建两个对象，一个对象作用于键值对的键（键对象），另一个对象用作键值对的值（值对象）。 ​ Redis中的每个对象都由一个redisObject结构表示，该结构中和保存数据有关的三个属性分别是type属性、encoding属性和ptr属性： 12345678typedef struct redisObject&#123; //类型 unsigned type:4; //编码 unsigned encoding:4; //指向底层实现数据结构的指针 void *prt;&#125;robj; 类型 对象的type属性记录了对象的类型。（字符串对象、列表对象、哈希对象、集合对象、有序集合对象） 编码和底层实现 对象的ptr指针指向对象的底层实现数据结构，而这些数据结构由对象的encoding属性决定。 7.2 字符串对象​ 字符串对象的编码可以是int、raw或者embstr。 ​ 如果字符串对象保存的是一个字符串值，并且这个字符串值的长度大于32字节，那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值，并将对象的编码设置为raw；如果这个字符串值的长度小于等于32字节，那么字符串对象将使用embstr编码的方式来保存这个字符串值。 ​ embstr编码是专门用于保存短字符串的一种优化编码方式，这种编码和raw编码一样，都会使用redisObject结构和sdshdr结构来表示字符串对象，但raw编码会调用两次内存分配函数来分别创建redisObject结构和sdshdr结构，而embstr编码则通过调用一次内存分配函数来分配一块连续的空间，空间中依次包含redisObject和sdshdr结构。 7.3 列表对象​ 列表对象的编码可以是ziplist或者linkedlist。 ​ ziplist编码的列表对象使用压缩列表作为底层实现，每个压缩列表节点（entry）保存了一个列表元素。 ​ linkedlist编码的列表对象使用双端列表作为底层实现，每个双端链表节点（node）都保存了一个字符串对象，而每个字符串对象都保存了一个列表元素。 ​ linkedlist编码的列表对象在底层的双端链表结构中包含了多个字符串对象，这种嵌套字符串对象的行为在哈希对象、集合对象和有序集合对象中都会出现，字符串对象是Redis五种类型的对象中唯一一种会被其他四种类型对象嵌套的对象。 7.3.1 编码转换 ​ 当列表对象可以同时满足以下两个条件时，列表对象使用ziplist编码： 列表对象保存的所有字符串元素的长度都小于64字节； 列表对象保存的元素数量小于512个；不能满足这两个条件的列表对象需要使用linkedlist编码。 7.4 哈希对象​ 哈希对象的编码可以是ziplist或者hashtable。 ​ ziplist编码的哈希对象使用压缩列表作为底层实现，每当有新的键值对要加入到哈希对象时，程序会先将保存了键的压缩列表节点推入到压缩列表表尾，然后再将保存了值的压缩列表节点推入到压缩列表表尾，因此： 保存了同一键值对的两个节点总是紧挨在一起，保存键的节点在前，保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向，而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 ​ hashtable编码的哈希对象使用字典作为底层实现，哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象，对象中保存了键值对的键； 字典的每个值都是一个字符串对象，对象中保存了键值对的值。 7.4.1 编码转换 ​ 当哈希对象可以同时满足以下两个条件时，哈希对象使用ziplist编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于64字节； 哈希对象保存的键值对的数量小于512个；不能满足这两个条件的哈希对象需要使用hashtable编码。 7.5 集合对象​ 集合对象的编码可以是intset或者hashtable。 ​ intset编码的集合对象使用整数集合作为底层实现，集合对象所包含的所有元素都被保存在整数集合里。 ​ hashtable编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，而字典的值则全部被设置为NULL。 7.5.1 编码转换 ​ 当集合对象可以同时满足以下两个条件时，对象使用intset编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过512个。 ​ 不能满足这两个条件的集合对象需要使用hashtable编码。 7.6 有序集合对象​ 有序集合对象的编码可以是ziplist或者skiplist。 ​ ziplist编码的压缩列表对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员（member），而第二个元素则保存元素的分值（score）。 ​ 压缩列表内的集合元素按分值从小到大排序，分值较小的元素被放置在靠近表头的方向，而分值较大的元素则被放置在靠近表尾的方向。 ​ skiplist编码的有序集合对象使用zset结构作为底层实现，一个zset结构同时包含一个字典和一个跳表: 1234typedef struct zset&#123; zskiplist *zsl; dict *dict;&#125;zset; ​ zset结构中的zsl跳表按分值从小到大保存了所有集合元素，每个跳表节点都保存了一个集合元素：跳表节点的object属性保存了元素的成员，而跳表节点score属性则保存了元素的分值。ZRANK，ZRANGE等命令给予跳表API实现的。 ​ 除此之外，zset结构的dict字典为有序集合创建了一个从成员到分值的映射，字典中的每个键值对都保存了一个集合元素：字典的键保存了元素的成员，字典的值则保存了元素的分值。通过字典，程序可以用O(1)复杂度查找给定成员的分值，ZSCORE命令根据这一特性实现的。 ​ 字典和跳表会共享元素的成员和分值，所以不会造成任何数据重复，也不会因此而浪费任何内存。 7.6.1 编码转换 ​ 当有序集合对象可以同时满足以下两个条件时，对象使用ziplist编码： 有序集合保存的元素数量小于128个； 有序集合保存的所有元素成员的长度都小于64字节； ​ 不能满足以上两个条件的有序集合将使用skiplist编码。 7.7 内存回收​ 因为C语言并不具备自动内存回收功能，所以Redis在字节的对象系统中构建了一个引用计数技术实现的内存回收机制。 ​ 每个对象的引用计数信息由redisObject结构的refcount属性记录 123456typedef struct redisObject&#123; //.. //引用计数 int refcount; //..&#125;robj; ​ 对象的引用计数信息会随着对象的使用状态而不断变化： 在创建一个新对象时，引用计数的值会被初始化为1； 当对象被一个新程序使用时，它的引用计数值会被增一； 当对象不再被一个程序使用时，它的引用计数值会被减一； 当对象的引用计数值变为0时，对象所占用的内存会被释放。","categories":[{"name":"redis","slug":"redis","permalink":"/categories/redis/"}],"tags":[]},{"title":"基于 Kafka 和 ZooKeeper 的分布式消息队列","slug":"基于-Kafka-和-ZooKeeper-的分布式消息队列","date":"2019-10-24T03:20:35.000Z","updated":"2019-11-06T12:01:12.466Z","comments":true,"path":"2019/10/24/基于-Kafka-和-ZooKeeper-的分布式消息队列/","link":"","permalink":"/2019/10/24/基于-Kafka-和-ZooKeeper-的分布式消息队列/","excerpt":"","text":"1.Kafka总体架构 ​ 一个典型的Kafka体系架构包括若干个Producer（消息生产者），若干broker（作为Kafka节点的服务器），若干Consumer（Group），以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群配置、选举Leader以及在consumer group发生变化时进行Rebalance（消费者负载均衡）。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费信息。 ​ Kafka内部结构如下： Producer：生产者，即消息发送者，push消息到Kafka集群中的broker中； Broker：Kafka集群由多个Kafka实例（Server）组成，每个实例构成一个broker； Topic：producer向Kafka集群push的消息会被归于某一类别，即Topic，这本质上只是一个逻辑概念，面向对象的是producer和consumer，producer只需要关注将消息push到哪一个Topic中，而consumer只需要关心自己订阅了哪个Topic； Partition：每一个Topic又被分为多个Partitions，即物理分区；出于负载均衡的考虑。同一个Topic中的Partitions分别存储与Kafka集群的多个broker上；而为了提高可靠性，这些Patitions可以由Kafka机制中的replicas来设置备份的数量； Consumer：消费者，从Kafka集群的broker中pull消息、消费消息； Consumer group：high-level consumer API中，每个consumer都属于一个consumer-group，每条消息只能被consumer-group中的一个consumer消费，但可以被多个consumer-group消费； replicas：partition的副本，保障partition的高可用； leader：replicas中的一个角色，producer和consumer只跟leader交互； follower：replicas中的一个角色，从leader中复制数据，作为副本，一个leader挂掉，会从它的followers中选举出一个新的leader继续提供服务； controller：Kafka集群中的其中一个服务器，用来进行leader election以及各种failover； ZooKeeper：Kafka通过ZooKeeper来存储集群的meta信息等。 1.1 Topic &amp; Partition​ 一个topic可以认为是一类消息，每个topic将被分成多个partition，每个partition在存储层面是append log文件。任何发布到此partition的消息都会被追加到log文件的尾部，每条消息在文件中的位置称为offset(偏移量)，offset为一个long型的数字，它唯一标记一条信息。Kafka机制中，producer push来的消息是追加到partition中的，这是一种随机写磁盘的机制，效率远高于随机写内存。 1.2Kafka为什么要将Topic进行分区​ 负载均衡、水平扩展。 ​ 在创建topic时可以在$KAFKA_HOME/config/server.properties 中指定这个partition的数量，也可以在topic创建之后去修改partition的数量。 1234# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=3 ​ 在发送一条消息时，可以指定这个消息的key，producer根据这个key和partition机制来判断这个消息发送到哪个partition。partition机制可以通过指定producer的partition.class这一参数来指定（即支持自定义），该class必须实现Kafka.producer.Partitioner接口。 2.Kafka高可靠性实现基础解读2.1Kafka文件存储机制​ partition还可以细分为segment，一个partition物理上由多个segment组成。在Kafka文件存储中，同一个topic下有多个不同的partition，每个partition为一个目录，partition的名称规则为：topic名称+有序序号，第一个序号从0开始计，最大的序号为partition数量减1，partition是实际物理上的概念，而topic是逻辑上的概念。 ​ 每个partition（目录）相当于一个巨型文件被平均分配到多个大小相等的segment（段）数据文件中（每个segment文件中消息数量不一定相等），这种特性也方便old segment的删除，即方便已被消费的消息的清理，提高磁盘的利用率。每个partition只需要支持顺序读写就行，segment的文件生命周期由服务端配置参数（log.segment.bytes，log.roll.{ms,hours}等若干参数）决定。 ​ segment文件由两部分组成，分别为”.index”文件和”.log”文件，分别表示为segment索引文件和数据文件。这两个文件的命令规则为：partition全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值，数值大小为64位，20位数字字符长度，没有数字用0填充。 2.2复制原理和同步方式​ 为了提高消息的可靠性，Kafka每个topic的partition有N个副本，其中N（大于等于1）是topic的复制因子的个数。Kafka通过多副本机制实现故障自动转移，当Kafka集群中出现broker失效时，副本机制可保证服务可用。对于任何一个partition,它的N个replicas中，其中一个replica为leader，其他都为follower，leader负责处理partition的所有读写请求，follower则负责被动地去复制leader上的数据。 ​ Kafka机制中，leader将负责维护和跟踪一个ISR（In-Sync Replicas）列表，即同步副本队列，这个列表里面的副本与leader保持同步，状态一致。如果新的leader从ISR列表中的副本中选出，那么就可以保证新leader为优选。 2.3同步副本ISR​ 默认情况下Kafka的replica数量为1，即每个partition都只有唯一的leader，无follower，没有容灾能力。为了确保消息的可靠性，生成环境中，通常将其值（由broker的参数offsets.topic.replication.factor指定）大小设置为大于1。所有的副本统称为Assigned Replicas，即AR。ISR是AR中的一个子集，由leader维护ISR列表，follower从leader同步数据有一些延迟（由参数replica.lag.max.ms 设置超时阈值），超过阈值的follower将被剔除出ISR，存入OSR(Outof-Sync Replicas)列表，新加入的follower也会先存放在OSR中。AR = ISR + OSR。 3.全程解析（Producer-kafka-consumer）3.1 producer发布消息​ producer采用push模式将消息发布到broker，每条消息都被append到patition中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要搞，保障kafka吞吐率）。produccer发送消息到broker时，会根据分区算法选择将其存储到哪一个partition。 其路由机制为： 指定了partition，则直接使用； 未指定partition但指定key，通过对key进行hash选出一个partition； partition和key都未指定，使用轮询选出一个partition。 写入流程： producer先从Zookeeper的”/brokers/…/state”节点找到该partition的leader； producer将消息发送给该leader； leader将消息写入本地log； follower从leader pull消息，写入本地log后leader发送ACK； leader收到所有ISR中的replica的ACK后，增加HW(high watermark，最后commit的offset)并向producer发送ACK。 3.2 Broker存储消息​ 物理上把topic分成一个或多个partition，每个partition物理上对应一个文件夹（该文件存储该partition的所有消息和索引文件） 3.3 Consumer消费消息​ high-level cocnsumer API提供了consumer group的语义，一个消息只能被group内的一个consumer所消费，且consumer消费信息时不关注offset，最后一个offset由ZooKeeper保存（下次消费时，该group中的consumer将从offset记录的位置开始消费）。 注意： 如果消费线程大于partition数量，则有些线程将收不到消息； 如果partition数量大于消费线程数，则有些线程多收到多个partition的消息； 如果一个线程消费多个partition，则无法保证收到的消息的顺序，而一个partition内的消息是有序的。 consumer采用pull模式从broker中读取数据。 ​ push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络堵塞。而pull模式则可以根据consumer的消息能力以适当的速率消费消息。 ​ 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"/categories/学习笔记/"}],"tags":[]},{"title":"Java内存区域","slug":"Java内存区域","date":"2019-10-11T10:21:47.000Z","updated":"2021-08-06T06:38:58.671Z","comments":true,"path":"2019/10/11/Java内存区域/","link":"","permalink":"/2019/10/11/Java内存区域/","excerpt":"","text":"1. 运行时数据区域 1.1 程序计数器（Program Counter Register）​ 程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。 ​ 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，这类内存区域为”线程私有“的内存。 ​ 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是一个Native方法，这个计数器值则为空(Undefined)。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 1.2 Java虚拟机栈(Java Virtual Machine Stacks)​ 是线程私有的，生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 ​ 局部变量表存放了编译期可知的各种基本数据类型(boolean, byte, char, short, int, long, float, double)、对象引用(reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)和returnAddress类型(指向了一条字节码指令的地址)。 ​ 局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 ​ 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展(当前大部分的Java虚拟机都可以动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈)，如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 1.3 本地方法栈(Native Method Stack)​ 本地方法栈与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法(也就是字节码)服务，而本地方法栈则为虚拟机使用到的Native方法服务。有的虚拟机(譬如Sun HotSpot虚拟机)直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 1.4 Java堆(Java Heap)​ Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 ​ Java堆是垃圾收集器管理的主要区域，所以也称为GC堆(Garbage Collected Heap)。由于现在收集器基本采用分代收集算法，所以Java堆也可细分为：新生代和老年代。 ​ Java堆可以处理物理上不连续的空间中，只要逻辑上是连续的即可。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 1.5 方法区(Method Area)​ 方法区与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 ​ Java虚拟机规范对方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载。 ​ 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 1.6 运行时常量池(Runtime Constant Pool)​ 运行时常量池是方法区的一部分。Class文件中有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 ​ 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这个特性被利用的比较多的是String类的intern()方法。 ​ 当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 1.7 直接内存​ 直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现。 ​ 在JDK 1.4中新加入了NIO(New Input/Output)类，引入了一种基于通道(Channel)与缓冲区 (Buffer)的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和N at ive堆中来回复制数据。 2. HotSpot虚拟机2.1 对象的创建​ 当Java虚拟机遇到一条字节码new指令时，首先将去检查这个指令的参数是否能在常量池中定位到 一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 ​ 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务实际上便等同于把一块确定大小的内存块从Java堆中划分出来。 2.2 对象的内存布局​ 在HotSpot虚拟机里，对象在堆内存中的存储布局可以划分为三个部分：对象头（Header）、实例数据（Instance Data）和对象填充（Padding）。 ​ HotSpot虚拟机对象的对象头部分包括两类信息。第一类是用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分戴年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位虚拟机（未开启压缩指针）中分别为32位和64位，官方称它为“Mark Word”。例如在32位的HotSpot虚拟机中，如对象未被同步锁锁定的状态 下，Mark Word的32位存储空间中的25位用于存储对象哈希码，4位用于存储对象分代年龄，2位用于存储锁标志位，1位固定为0，在其他状态(轻量级锁定、重量级锁定、GC标记、可偏向)下对象的存储内容如表所示。 ​ 对象头的另一部分是类型指针，即对象指向它的类型元数据的指针，Java虚拟机通过这个指针来确定该对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不一定要经过对象本身。如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是如果数组的长度是不确定的，将无法通过元数据中的信息推断出数组的大小。 ​ 实例数据部分是对象真正存储的有效信息，即我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的字段都必须记录起来。 ​ 对象的第三部分是对齐填充，这并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍。 2.3 对象的访问定位​ 对象访问方式是由虚拟机实现而定的，主流的访问方式主要有使用句柄和直接指针两种： 如果使用句柄访问的话Java堆中将可能会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息。 如果使用直接指针访问的话，Java堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。 ​ 这两种对象访问方式各有优势，使用句柄来访问的最大好处就是reference中存储的是稳定句柄地址，在对象被移动(垃圾收集时移动对象是非常普遍的行为)时只会改变句柄中的实例数据指针，而reference本身不需要被修改。 ​ 使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象访问在Java中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本。","categories":[{"name":"JVM笔记","slug":"JVM笔记","permalink":"/categories/JVM笔记/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"/tags/JVM/"}]},{"title":"python爬虫-解析库的使用","slug":"python爬虫-解析库的使用","date":"2019-08-15T02:54:57.000Z","updated":"2019-08-19T08:43:59.233Z","comments":true,"path":"2019/08/15/python爬虫-解析库的使用/","link":"","permalink":"/2019/08/15/python爬虫-解析库的使用/","excerpt":"","text":"1.使用XPath1.XPath常用规则 表达式 描述 nodename 选取此节点的所有子节点 / 从当前节点选择直接子节点 // 从当前节点选取子孙节点 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 2.使用1.转换123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = etree.tostring(html)print(result.decode('utf-8')) 结果： 12345678910&lt;html&gt;&lt;body&gt;&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2.所有节点123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//*')print(result) 结果: 1[&lt;Element html at 0x7fa7683c9148&gt;, &lt;Element body at 0x7fa7683c90c8&gt;, &lt;Element div at 0x7fa7683c9088&gt;, &lt;Element ul at 0x7fa7683c9188&gt;, &lt;Element li at 0x7fa7683c91c8&gt;, &lt;Element a at 0x7fa7683c9248&gt;, &lt;Element li at 0x7fa7683c9288&gt;, &lt;Element a at 0x7fa7683c92c8&gt;, &lt;Element li at 0x7fa7683c9308&gt;, &lt;Element a at 0x7fa7683c9208&gt;, &lt;Element li at 0x7fa7683c9348&gt;, &lt;Element a at 0x7fa7683c9388&gt;, &lt;Element li at 0x7fa7683c93c8&gt;, &lt;Element a at 0x7fa7683c9408&gt;, &lt;Element ul at 0x7fa7683c9448&gt;] 3.子节点123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li/a')print(result) 结果： 1[&lt;Element a at 0x7fa8000e4408&gt;, &lt;Element a at 0x7fa8000e43c8&gt;, &lt;Element a at 0x7fa8000e44c8&gt;, &lt;Element a at 0x7fa8000e4508&gt;, &lt;Element a at 0x7fa8000e4548&gt;] 4.父节点使用..或者parent::来获取父节点 12345678910111213141516from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)# result = html.xpath('//a[@href=\"link4.html\"]/../@class')result = html.xpath('//a[@href=\"link4.html\"]/parent::*/@class')print(result) 结果： 1[&apos;item-1&apos;] 5.属性匹配123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[@class=\"item-0\"]')print(result) 结果： 1[&lt;Element li at 0x7fc52029c4c8&gt;, &lt;Element li at 0x7fc52029c448&gt;] 6.文本获取使用text()来获取节点中的文本 123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[@class=\"item-0\"]/a/text()')print(result) 结果： 1[&apos;first item&apos;, &apos;fifth item&apos;] 7.属性获取123456789101112131415from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li/a/@href')print(result) 结果： 1[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link3.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;] 8.属性多值匹配使用contains() 12345678910111213141516from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;li class=\"li li-first\"&gt;&lt;a href=\"link.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class,\"li\")]/a/text()')print(result) 结果： 1[&apos;first item&apos;] 9.多属性匹配12345678910111213141516from lxml import etreetext = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;li class=\"li li-first\" name=\"item\"&gt;&lt;a href=\"link.html\"&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;ul&gt;&lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class,\"li\") and @name=\"item\"]/a/text()')print(result) 结果： 1[&apos;first item&apos;] 10.按序选择序号是从1开头，不是以0开头 123456789101112131415161718192021from lxml import etreetext = &apos;&apos;&apos;&lt;div&gt;&lt;ul&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;ul&gt;&lt;/div&gt;&apos;&apos;&apos;html = etree.HTML(text)result = html.xpath(&apos;//li[1]/a/text()&apos;)print(result)result = html.xpath(&apos;//li[last()]/a/text()&apos;)print(result)result = html.xpath(&apos;//li[position()&lt;3]/a/text()&apos;)print(result)result = html.xpath(&apos;//li[last()-2]/a/text()&apos;)print(result) 结果： 1234[&apos;first item&apos;][&apos;fifth item&apos;][&apos;first item&apos;, &apos;second item&apos;][&apos;third item&apos;] 2.使用Beautiful Soup1.基本用法12345678910111213141516from bs4 import BeautifulSouphtml = '''&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body &gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story’'&gt;Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;! - Elsie ...&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;'''soup = BeautifulSoup(html, 'lxml')print(soup.prettify())print(soup.title.string) 结果： 123456789101112131415161718192021222324252627282930&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"title\" name=\"dromouse\"&gt; &lt;b&gt; The Dormouse's story &lt;/b&gt; &lt;/p&gt; &lt;p class=\"story’'&amp;gt;Once upon a time there were three little sisters; and their names were &amp;lt;a href=\" http:=\"\" id=\"link1\"&gt; , &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt; Lacie &lt;/a&gt; and &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt; Tillie &lt;/a&gt; ;and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt; ... &lt;/p&gt; &lt;/body&gt;&lt;/html&gt;The Dormouse's story 2.节点选择器1.选择元素节点选择只会选择第一个匹配的节点，其他的后面节点都会忽略 12345678910111213141516171819from bs4 import BeautifulSouphtml = '''&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body &gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story’'&gt;Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;! - Elsie ...&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;'''soup = BeautifulSoup(html, 'lxml')print(soup.title)print(type(soup.title))print(soup.title.string)print(soup.head)print(soup.p) 结果为： 12345&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;class &apos;bs4.element.Tag&apos;&gt;The Dormouse&apos;s story&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt; 2.提取信息 提取名称 1print(soup.title.name) 提取属性 12print(soup.p.attrs)print(soup.p.attrs['name']) 也可以不写attrs，直接在节点元素后面加中括号，传入属性名就可以获取属性值了。 1print(soup.p['name']) 获取内容 1print(soup.p.string) 3.嵌套选择123print(soup.head.title)print(type(soup.head.title))print(soup.head.title.string) 4.关联选择 子节点和子孙节点 选择节点元素之后，如果想要获取它的直接子节点，可以调用contents属性。 同样可以调用children属性得到相应的结果。 如果要得到所有的子孙节点的话，可以调用descendants属性。 父节点和祖先节点 如果要获取某个节点元素的父节点，可以调用parent属性。 如果要获取所有的祖先节点，可以调用parents属性。 兄弟节点 next_sibling和previous_sibling分别获取节点的下一个和上一个兄弟元素， next_siblings和previous_siblings则分别返回所有前面和后面的兄弟节点的生成器。 3.方法选择器 find_all() 查询所有符合条件的元素。 find_all(name, attrs, recursive, text, **kwargs) find 查询第一个匹配的元素 4.CSS选择器调用select()方法，传入相应的CSS选择器即可。 获取文本时，使用get_text()方法 3.使用pyquery1.初始化1.字符串初始化1234567891011121314from pyquery import PyQuery as pqhtml = '''&lt;div&gt;&lt;ul&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;'''doc = pq(html)print(doc('li')) 结果是： 12345&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 2.url初始化123456from pyquery import PyQuery as pqimport requestsdoc = pq('https://www.taobao.com')#或者使用以下方式#doc = pq(requests.get('https://www.taobao.com').text)print(doc('title')) 结果为： 1&lt;title&gt;淘宝网 - 淘！我喜欢&lt;/title&gt; 3.文件初始化123from pyquery import PyQuery as pqdoc = pq(fileName='demo.html')print(doc('li')) 2.基本CSS选择器123456789101112131415from pyquery import PyQuery as pqhtml = '''&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;'''doc = pq(html)print(doc('#container .list li'))print(type(doc('#container .list li'))) 结果为： 1234567&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; 3.查找节点1.子节点查找子节点时，需要用到find()方法，此时传入的参数是CSS选择器。 123456789101112131415161718192021from pyquery import PyQuery as pqimport requestshtml = '''&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;'''doc = pq(html)items = doc('.list')print(type(items))print(items)lis = items.find('li')print(type(lis))print(lis) 结果是： 1234567891011121314&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;ul class=&quot;list&quot;&gt;&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; find()的查找范围是节点的所有子孙节点，如果只想查找子节点，可以用children()方法。 2.父节点12345678910111213141516171819from pyquery import PyQuery as pqhtml = '''&lt;div class=\"wrap\"&gt;&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class =\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;'''doc = pq(html)items = doc('.list')container = items.parent()print(type(container))print(container) 结果是： 12345678910&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;div id=\"container\"&gt;&lt;ul class=\"list\"&gt;&lt;li class=\"item-O\"&gt;first item&lt;/li&gt;&lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt; parents()方法会返回所有的祖先节点。 3.兄弟节点调用siblings()方法。 4.遍历调用items()方法。 123456from pyquery import PyQuery as pqdoc = pq(html)lis = doc('li').items()print(type(lis))for li in lis: print(li, type(li)) 结果： 1234567891011&lt;class &apos;generator&apos;&gt;&lt;li class=&quot;item-O&quot;&gt;first item&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; 5.获取信息1.获取属性提取到某个PyQuery类型的节点后，就可以调用attr()方法来获取属性。 1234567from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a, type(a))print(a.attr('href'))#也可使用attr属性来获取属性print(a.attr.href) 结果为： 12&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;link3.html 2.获取文本12345from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.text()) 结果： 12&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;third item 6.节点操作addClass、removeClass、attr、text和html等。","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[{"name":"python爬虫","slug":"python爬虫","permalink":"/tags/python爬虫/"}]},{"title":"python爬虫-基本库的使用","slug":"python爬虫-基本库的使用","date":"2019-06-26T12:22:36.000Z","updated":"2019-11-22T07:06:22.055Z","comments":true,"path":"2019/06/26/python爬虫-基本库的使用/","link":"","permalink":"/2019/06/26/python爬虫-基本库的使用/","excerpt":"","text":"1.使用urllib在python3中，统一使用urllib。它是python内置的HTTP请求库，不需要额外安装即可使用。它包含4个模块。 request：它是最基本的HTTP请求模块，可以用来模拟发送请求。 error：异常处理模块，如果出现请求错误，可以捕获这些异常，然后进行充实或其他操作以bao证程序不会意外终止。 parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。 robotparser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得较少。 1.1发送请求1.urlopen()urllib.request模块提供了最基本的构造HTTP请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理授权验证、重定向、浏览器cookies以及其他内容。 123456import urllib.requestimport sslssl._create_default_https_context = ssl._create_unverified_contextresponse = urllib.request.urlopen('https://www.baidu.com')print(response.read()) 在进行https爬取时，需要设置ssl，进行全局取消证书验证。 2.Request如果请求中需要加入Headers等信息，就可以利用更强大的Request类来构建。 1234567import urllib.requestimport sslssl._create_default_https_context = ssl._create_unverified_contextrequest = urllib.request.Request('https://www.baidu.com')response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) Request的构造方法如下： 1urllib.request.Request(url,data=None,headers&#123;&#125;,origin_req_host=None, unverifiable=False,method=None) 第一个参数url用于请求URL，这是必传参数，其他都是可选参数。 第二个参数data如果要传，必须传bytes类型的。如果它时字典，可以先用urllib.parse模块里的urlencode()编码。 第三个参数headers是一个字典，它就是请求头，可以在构造请求是通过headers参数直接构造，也可以通过调用请求实例的add_header()方法添加。 第四个参数origin_req_host指的是请求方的host名称或者IP地址。 第五个参数unverifiable表示这个请求是否是无法验证的，默认是False，意思是说用户没有足够权限来选择接收这个请求的结果。例如请求一个HTML文档中的图片，但是没有自动抓取图像的权限，这时unverifiable的值就是True。 第六个参数method是一个字符串，用来指示请求使用的方法，比如GET、POST和PUT等。 传入多个参数的请求： 12345678910111213from urllib import request, parseheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36', 'Host': 'httpbin.org'&#125;dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf-8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8')) 3.高级用法urllib.request模块里的BaseHandler类，它是所有其他Handler的父类，它提供了最基本的方法，例如default_open()、protocol_request()等。 HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误都会抛出HTTPErro t类型的异常。 HTTPRedirectHandler：用于处理重定向。 HTTPCookieProcessor：用于处理Cookies。 ProxyHandler：用于设置代理，默认代理为空。 HTTPPasswordMgr：用于管理mi码，它维护了用户名和mi码的表。 HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。 1234567# 生成Cookies文件filaName = 'cookies.txt'cookie = http.cookiejar.LWPCookieJar(filaName)handler = request.HTTPCookieProcessor(cookie)opener = request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True,ignore_expires=True) 1234567#读取Cookies文件并使用cookie = http.cookiejar.LWPCookieJar()cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)handler = request.HTTPCookieProcessor(cookie)opener = request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8')) 1.2处理异常1.URLErrorURLError类来自urllib库的error模块，它继承自OSError类，是error异常模块的基类，由request模块产生的一场都可以通过捕获这个类来处理。 123456import urllib.errorfrom urllib import requesttry: response = request.urlopen('http://bububu.com/index.htm')except urllib.error.URLError as e: print(e.reason) 2.HTTPError它是URLError的子类，专门用来处理HTTP请求错误，比如认证请求失败等。它有如下3个属性。 code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误。 reason：同父类一样，用于返回错误的原因。 headers：返回请求头。 12345678910import urllib.errorfrom urllib import requesttry: response = request.urlopen('http://bububu.com/index.htm')except urllib.error.HTTPError as e: print(e.reason, e.code, e.headers)except urllib.error.URLError as e: print(e.reason)else: print('Request Successfuly') 3.解析链接1.urlparse()该方法可以实现URL的识别和分段。它的api用法： 1urllib.parse.urlparse(url, scheme='', allow_fragments=True) url：必填项，即待解析的URL。 scheme：它是默认的协议（比如http或https等）。假如这个链接没有带协议信息，会将这个作为默认的协议。 allow_fragments：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，它会被解析为path、paramenters或者query的一部分，而fragment部分为空。 2.urlunparse()它接受的参数是一个可迭代对象，但是它的长度必须是6，否则会抛出参数数量不足或者过多的问题。 3.urlsplit()和urlparse()方法非常类似，只不过它不再单独解析params这一部分，只返回5个结果。params会合并到path中。 4.urlunsplit()与urlunparse()类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元祖等，唯一的区别是长度必须为5. 5.urljoin()提供一个base_url作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url的schem、netloc和path这3个内容对新链接缺失的部分进行补充，最后返回结果。 6.urlencode()在构造GET请求参数的时候非常有用，可以将参数转化为GET请求参数。 12345from urllib.parse import urlencodeparams = &#123;'name':'germey', 'age':22&#125;base_url = 'http://www.baidu.com?'url = base_url + urlencode(params)print(url) 结果为：http://www.baidu.com?name=germey&amp;age=22 7.parse_qs()如果有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典。 123from urllib.parse import parse_qsquery = 'name=germey&amp;age=22'print(parse_qs(query)) 结果为：{‘name’: [‘germey’], ‘age’: [‘22’]} 8.parse_qsl()它用于将参数转化为元组组成的列表。 123from urllib.parse import parse_qslquery = 'name=germey&amp;age=22'print(parse_qsl(query)) 结果为：[(‘name’, ‘germey’), (‘age’, ‘22’)] 9.quote()该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码。 1234from urllib.parse import quotekeyword = '壁纸'url = 'http://www.baidu.com/s?wd=' + quote(keyword)print(url) 结果为：http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8 10.unquote()它可以进行URL解码。 123from urllib.parse import unquoteurl = 'http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'print(unquote(url)) 结果为：http://www.baidu.com/s?wd=壁纸 4.分析Robots协议1.Robots协议Robots协议也称作爬虫协议，机器人协议，它的全名叫做网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做robots.txt的文本文件，一般放在网站的根目录下。 2.robotparser使用robotparser模块来解析robots.txt。该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。该类只需要在构造方法里传入robots.txt的链接即可，声明如下： 1urllib.robotparser.RobotFileParser(url='') 常用方法： set_url()：用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，那么就不要再使用这个方法设置了。 read()：读取robots.txt文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。 parse()：用来解析robots.txt，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。 can_fetch()：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。 mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓去的搜索爬虫是很有必要的，可能需要定期检查来抓取最新的robots.txt。 midified()：它同样对长时间分析和抓取的搜索爬虫很厚帮助，将当前时间设置为上次抓取和分析robots.txt的时间。 12345from urllib.robotparser import RobotFileParserrp = RobotFileParser()rp.set_url('http://www.jianshu.com/robots.txt')rp.read()print(rp.can_fetch('*','http://www.jianshu.com/p/b67554025d7d')) 2.使用requests2.1基本用法1.实例1234567import requestsr = requests.get('https://www.baidu.com')print(type(r))print(r.status_code)print(type(r.text))print(r.text)print(r.cookies) 2.GET请求 基本实例 123import requestsr = requests.get('http://httpbin.org/get')print(r.text) 结果： 1234567891011&#123; \"args\": &#123;&#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.21.0\" &#125;, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/get\"&#125; 返回结果中包含请求头、URL、IP等信息。 附加额外信息的写法 1234import requestsdata = &#123;'name':'geremy','age':22&#125;r = requests.get('http://httpbin.org/get', params=data)print(r.text) 结果为： 1234567891011121314&#123; \"args\": &#123; \"age\": \"22\", \"name\": \"geremy\" &#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.21.0\" &#125;, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/get?name=geremy&amp;age=22\"&#125; 调用json()方法，可将字符串的结果转化为字典 抓取网页 123456789import requestsimport reheaders = &#123; 'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'&#125;r = requests.get('https://www.zhihu.com/explore',headers=headers)pattern = re.compile('explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;', re.S)titles = re.findall(pattern, r.text)print(titles) 3.POST请求1234import requestsdata = &#123;'name':'germey','age':22&#125;r = requests.post('https://httpbin.org/post', data=data)print(r.text) 结果为： 1234567891011121314151617181920&#123; \"args\": &#123;&#125;, \"data\": \"\", \"files\": &#123;&#125;, \"form\": &#123; \"age\": \"22\", \"name\": \"germey\" &#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"18\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.21.0\" &#125;, \"json\": null, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/post\"&#125; form部分就是提交的数据，证明POST请求成功发送了。 2.1高级方法1.文件上传1234import requestsfiles = &#123;'file':open('favicon.ico','rb')&#125;r = requests.post('https://httpbin.org/post', files=files)print(r.text) 2.Cookies1234import requestsr = requests.get('https://www.baidu.com')for key, value in r.cookies.items(): print(key + '=' + value) 3.会话维持12345import requestss = requests.session()s.get('http://httpbin.org/cookies/set/number/123456789')r = s.get('http://httpbin.org/cookies')print(r.text) 运行结果为： 12345&#123; \"cookies\": &#123; \"number\": \"123456789\" &#125;&#125; 4.SSL证书验证12345import requestsimport logginglogging.captureWarnings(True)response = requests.get('https://www.12306.cn', verify=False)print(response.status_code) 5.代理设置12345678import requestsproxies=&#123; 'http':'http://124.207.82.166:8008', 'https':'http://124.207.82.166:8008'&#125;r = requests.get('https://www.baidu.com',proxies=proxies)print(r.status_code) 6.超时设置123import requestsr = requests.get('https://www.baidu.com',timeout=1)print(r.status_code) 7.身份认证123import requestsr = requests.get('http://localhost:9090/',auth=('username','password'))print(r.status_code) 8.Prepared Request1234567891011from requests import Request, sessionurl = 'http://httpbin.org/post'data = &#123;'name': 'germey'&#125;headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'&#125;s = Session()req = Request('POST', url, data=data, headers=headers)prepped = s.prepare_request(req)r = s.send(prepped)print(r.text) 引入Request，然后用url、data和headers参数构造了一个Request对象，这时需要再调用Session的prepare_request()方法将其转换为一个Prepared Request对象，然后调用send()方法发送即可，运行结果如下： 12345678910111213141516171819&#123; \"args\": &#123;&#125;, \"data\": \"\", \"files\": &#123;&#125;, \"form\": &#123; \"name\": \"germey\" &#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"11\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36\" &#125;, \"json\": null, \"origin\": \"125.35.5.254, 125.35.5.254\", \"url\": \"https://httpbin.org/post\"&#125; 3.抓取猫眼排行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import osimport requestsimport reimport jsonimport timefrom requests.exceptions import RequestExceptiondef get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko)' + 'Chrome/76.0.3809.100 Safari/537.36' &#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text return None except RequestException: return Nonedef parse_one_page(html): pattern = re.compile('&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=\"(.*?)\".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;' + '(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html) for item in items: yield &#123; 'index': item[0], 'image': item[1], 'title': item[2].strip(), 'actor': item[3].strip()[3:], 'time': item[4].strip()[5:], 'score': item[5].strip() + item[6].strip() &#125;def write_to_file(content): with open('reulst.txt', 'a', encoding='utf-8') as f: f.write(json.dumps(content, ensure_ascii=False)+'\\n')def main(offset): url = 'https://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) for item in parse_one_page(html): print(item) write_to_file(item)if __name__ == '__main__': for i in range(10): main(offset = i*10) time.sleep(1)","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[{"name":"python爬虫","slug":"python爬虫","permalink":"/tags/python爬虫/"}]},{"title":"正则表达式","slug":"正则表达式","date":"2019-06-18T11:55:04.000Z","updated":"2019-09-19T11:03:32.892Z","comments":true,"path":"2019/06/18/正则表达式/","link":"","permalink":"/2019/06/18/正则表达式/","excerpt":"","text":"一.Egrep元字符使用·表示空格 行的起始和结束「^」代表一行的开始，「$」代表结束。 「^cat$」：匹配的条件是，行开头，然后是字符c·a·t，然后是行末尾。只包含cat的行—没有多余的单词、空白字符……只有’cat’。 「^$」：匹配的条件是行开头，然后就行末尾。表示是空行。 「^」：匹配条件是行的开头。应用意义为无意义，每一行都有开头，所以都能匹配，空行也可以。 字符组匹配若干字符之一使用正则表达式结构体[…]，它容许使用者列出在某处期望匹配的字符，通常被称作字符组。正则表达式[ea]能匹配a或者e。 在字符组以外，普通字符(例如gr[ae]y中的g和r)都有接下来是的意思—首先匹配g，接下来是r。字符组的内容是在同一个位置能够匹配的若干字符，它的意思是或。 在字符组内部，字符组元字符’-‘表示一个范围：在&lt;H[1-6]&gt;与&lt;H[123456]&gt;是完全一样的。只有在字符组内部，连字符才是元字符，否则它就只能匹配普通的连字符号。如果连字符出现在字符组的开头，它表示的就只是一个普通字符，而不是一个范围。 排除型字符组用[^]取代[]，这个字符组就会匹配任何未列出的字符。例如[^1-6]匹配除了1到6以外的任何字符。 排除型字符组表示””匹配一个未列出的字符””，而不是”不要匹配列出的字符”。 用点号匹配任意字符元字符「.」是用来匹配任意字符的字符组的简便写法。 例如：需要搜索03/19/76、03-19-76或03.19.76，不怕麻烦的话用一个明确容许’/‘、’-‘、’.’的字符组来构建正则表达式，例如「03[-./]19[-./]76」。也可以使用「03.19.76」。 多选结构匹配任意子表达式「|」是一个非常简捷的元字符，它的意思是”或”。 「Bob」,「Robert」是两个正则表达式，「Bob|Robert」表示可以同时匹配其中任意一个的正则表达式。 「gr[ae]y」，可以写成「gray|grey」或者「gr(a|e)y」。 忽略大小写把-i写在正则表达式之前。 单词分界符「\\ &lt;」和「\\ &gt;」可以使用它们来匹配单词分界的位置。「&lt;」和「&gt;」本身并不是元字符，只有它们与斜线结合起来的时候，整个序列才具有特殊意义。 某些版本的egrp可能不支持。 可选项元素「?」代表可选项，把它加在一个字符的后面，就表示此处容许出现这个字符，它的出现并非匹配成功的必要条件。 color和colour可以用「colou?r」进行匹配。「u?」只作用于之前紧邻的元素。 其他量词：重复出现「+」和「」的作用与问号类似。元字符「+」表示”之前紧邻的元素出现一次或多次”，「 」表示”之前紧邻的元素出现任意多次，或者不出现”。换种说法就是「··· *」表示”匹配尽可能多的次数，如果实在无法匹配，也不要紧”。「···+」的意思与之类似，也是匹配尽可能多的次数，但如果连一次匹配都无法完成，就报告失败。 &lt;HR·SIZE=14&gt;匹配：「&lt;HR·+SIZE· = · [0-9]+· *&gt;」 如果size也是可选的，则为「&lt;HR(·+SIZE·=· [0-9]+)？· *&gt;」 规定重现次数的范围：区间某些版本的egrep能够使用元字符序列来自定义重现次数的区间:「···{min,max}」，这成为”区间量词”。例如，「···{3,12}」能够容许的重现次数在3到12之间。 括号及反向引用括号能够”记住”它们包含的子表达式匹配的文本。如果确切知道重复单词的第一个单词（比如是”the”），就能够明确无误的找到它，例如「the the」，但是还是会匹配到the theory的情况，所以加上单词分界符「\\ 」。 反向引用是正则表达式的特性之一，它容许我们匹配与表达式先前部分匹配的同样的文本。 先把「\\ 」中的第一个「the」替换为能够匹配任意单词的正则表达式「[A-Za-z]+」；然后在两端加上括号；最后把后一个’the’替换为特殊的元字符序列「\\1」，就得到了「\\ &lt;([A-Za-z]+)·+\\1\\ &gt;」。如果一个单词在某行末尾，另一个单词在下一行的开头，该表达式无法找到。 也可以在一个表达式中使用多个括号，再用「\\1」「\\2」「\\3」来表示第一、第二、第三组括号匹配的文本。「([a-z])([0-9])\\1\\2」中的「\\1」代表[a-z]匹配的内容，「\\2」代表[0-9]匹配的内容。 神奇的转义反斜线\\为转义符，它作用的元字符会失去特殊意义，成了普通字符。 如匹配ega.att.com，则为「ega\\ .att\\ .com」 还可以使用「\\ ([a-zA-Z]+\\ )」来匹配一个括号内的单词，如(very)。 二.常用的元字符和特性字符组简记法\\d 数字：等价于「[0-9]」。 \\D 非数字字符：等价于「[^\\d]」。 \\w 单词中的字符：一般等价于「[a-zA-Z0-9_]」。 \\W 非单词字符：等价于「[^\\w]」。 ​ \\s 空白字符：在支持ASCII的系统中，它通常等价于「[·\\f\\n\\r\\r\\t\\v]」。 \\S 非空白字符：等价于「[^\\s]」。 锚点及其他”零长度断言”行/字符串的起始位置：^、\\A行/字符串的结束为止：$、\\Z和\\z匹配的起始位置（或者是上一次匹配的结束位置）：\\G单词分界符：\\b、\\B、\\ &lt;、\\ &gt;单词分界符可以分成两类，一类中单词起始位置分界符和结束位置分界符是相同的(通常是\\ &lt;和\\ &gt;)，另一类则以统一的分界符来匹配（通常是\\b）。两类都提供了非单词分界符序列（通常是\\B）。 注释和模式修饰符模式修饰符：(?modifier)，例如(?i)和(?-i)「(?i)」会启用不区分大小写的匹配，而「(?-i)」会停用此功能。 模式作用范围：(?modifier:…)，例如(?i:···)「(?:(?i)very)」可以简写成「(?i:very)」 分组，捕获，条件判断和控制捕获/分组括号：(…)和\\1,\\2,…仅用于分组的括号：(?:…)命名捕获：(?&lt; Name&gt;…)Python和PHP的preg引擎，以及.NET引擎，都能够为捕获内容命名。Python和PHP使用的语法是「(?P&lt; name&gt;…)」，而.NET引擎使用「(?&lt; name&gt;…)」。 「\\b(?P&lt; Area&gt;\\d\\d\\d)-(?P&lt; Exch&gt;\\d\\d\\d)-(?P&lt; Num&gt;\\d\\d\\d\\d)\\b」 这个表达式会用美国电话号码的各个部分’’填充”Area、Exch和Num命名的内容。 固化分组：(?&gt;…)可以提高效率。 二.正则表达式实用技巧正则表达式的平衡法则好的正则表达式必须在这些方面求得平衡： 只匹配期望的文本，排除不期望的文本。 必须易于控制和理解。 如果是呀NFA引擎，必须保证效率（如果能够匹配，必须很快地返回匹配结果，如果不能匹配，应该在尽可能短的时间内报告匹配失败）。 若干简单的例子1.匹配连续行(续前)^\\w+=([ ^\\n\\ ]||\\.)* 2.匹配IP地址^([01]?\\d\\d?|2[0-4]\\d|25[0-5]).([01]?\\d\\d?|2[0-4]\\d|25[0-5]).([01]?\\d\\d?|2[0-4]\\d|25[0-5]).([01]?\\d\\d?|2[0-4]\\d|25[0-5])$ 3.处理文件名去掉文件名开头的路径 Unix: ^.*/ windows: ^.*\\ \\ 从路径中获取文件名 [^/]*$ 所在路径和文件名 ^(. )/( [ ^/ ] )$","categories":[{"name":"正则表达式","slug":"正则表达式","permalink":"/categories/正则表达式/"}],"tags":[]},{"title":"(九)Executor框架","slug":"Executor框架","date":"2019-04-11T13:18:59.000Z","updated":"2021-01-15T02:40:07.014Z","comments":true,"path":"2019/04/11/Executor框架/","link":"","permalink":"/2019/04/11/Executor框架/","excerpt":"","text":"Executor简介Executor框架的两级调度模型​ 在HotSpot VM的线程模型中，Java线程(java.lang.Thread)被一对一映射为本地操作系统线程。Java线程启动时会创建一个本地操作系统线程；当该Java线程终止时，这个操作系统线程也会被收回。操作系统会调度所有线程并将它们分配给可用的CPU。 ​ 在上层，Java多线程程序通常把应用分解为若干个任务，然后使用用户级的调度器(Executor框架)将这些任务映射为固定数量的线程；在底层，操作系统内核将这些线程映射到硬件处理器上。 ​ 应用程序通过Executor框架控制上层的调度；而下层的调度由操作系统内核控制，下层的调度不受应用程序的控制。 Executor框架的结构​ Executor框架主要由3大部分组成。 任务。包括被执行任务需要实现的接口：Runnable接口或Callable接口。 任务的执行。包括任务执行机制的核心接口Executor，以及继承自Executor的ExecutorService接口。Executor框架有两个关键类实现了ExecutorService接口(ThreadPoolExecutor和ScheduledThreadPoolExecutor) 异步计算的结果。包括接口Future和实现Future接口的FutureTask类。 类和接口的简介。 Executor是一个接口，它是Executor框架的基础，它将任务的提交与任务的执行分离开来。 ThreadPoolExecutor是线程池的核心实现类，用来执行被提交的任务。 ScheduledThreadPoolExecutor是一个实现类，可以在给定的延迟后运行命令，或者定期执行命令。ScheduledThreadPoolExecutor比Timer更灵活，功能更强大。 Futrue接口和实现Future接口的FutureTask类，代表异步计算的结果。 Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。 Executor框架的成员(1)ThreadPoolExecutor ​ ThreadPoolExecutor通常使用工厂类Executors来创建。Executors可以创建3种类型的ThreadPoolExecutor:SingleThreadExecutor、FixedThreadPool和CachedThreadPool。 ​ 1)FixedThreadPool。适用于为了满足资源管理的需求，而需要限制当前线程数量的应用场景，它适用于负载比较重的服务器。 ​ 2)SingleThreadExecutor。适用于需要保证顺序地执行各个任务；并且在任意时间点，不会有多个线程是活动的应用场景。 ​ 3)CachedThreadPool。是大小无界的线程池，适用于执行很多的短期异步任务的小程序，或者是负载较轻的服务器。 (2)ScheduledThredPoolExecutor ​ ScheduledThreadPoolExecutor通常使用工厂类Executor来创建。Executors可以创建两种类型的ScheduledThreadPoolExecutor。 ​ 1)ScheduledThreadPoolExecutor。包含若干个线程的ScheduledThreadPoolExecutor。适用于需要多个后台线程执行周期任务，同时为了满足资源管理的需求而需要限制后台线程的数量的应用场景。 ​ 2)SingleThreadScheduledExecutor。只包含一个线程的ScheduledThreadPoolExecutor。适用于需要单个后台线程执行周期任务，同时需要保证顺序地执行各个任务的应用场景。 (3)Future接口 ​ Future接口和实现Future接口的FutureTask类用来表示异步计算的结果。当把Runnable接口或Callable接口的实现类提交(submit)给ThreadPoolExecutor或ScheduledThreadPoolExecutor时，会返回一个FutureTask对象。 (4)Runnable接口和Callable接口 ​ Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。它们之间的区别是Runnable不会返回结果，而Callable可以返回结果。 ThreadPoolExecutor详解​ Executor框架最核心的类是ThreadPoolExecutor，它是线程池的实现类，主要由4个组件构成。 corePool：核心线程池的大小。 maximumPool：最大线程池的大小。 BlockingQueue：用来暂时保存任务的工作队列。 RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和时(达到了最大线程池大小且工作队列已满)，execute()方法将要调用的Handler。 FixedThreadPool详解​ FixedThreadPool被称为可重用固定线程数的线程池。 ​ FixedThreadPool的corePoolSize和maximumPoolSize都被设置为创建FixedThreadPool时指定的参数nThreads。 ​ 当线程池中的线程数大于corePoolSize时，keepAliveTiem为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止。这里把keepAliveTime设置为0L，意味着多余的空闲线程会被立即终止。 SingleThreadExecutor详解​ SingleThreadExecutor是使用单个worker线程的Executor。SignleThreadExecutor的corePoolSize和maximumPoolSize被设置为1。其他参数与FixedThreadPool相同。SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工作队列(队列的容量为Integer.MAX_VALUE)。 CachedThreadPool详解​ CachedThreadPool是一个会根据需要创建新线程的线程池。 ​ CachedThreadPool的corePoolSize被设置为0，即corePool为空；maximumPoolSize被设置为Integer.MAX_VALUE，即maximumPool是无界的。这里把keepAliveTime设置为60L，意味着CachedThreadPool中的空闲线程等待新任务的最长时间为60秒，空闲线程超过60秒后将会被终止。 ​ FixedThreadPool和SingleThreadExecutor使用无界队列LinkeBlockingQueue作为线程池的工作队列。CachedThreadPool使用没有容量的SynchronousQueue作为线程池的工作队列，但CachedThreadPool的maximumPool是无界的。意味着如果主线程提交任务的速度高于maximumPool中线程处理任务的速度时，CachedThreadPool会不断创建新线程。极端情况下，CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。 ScheduledThreadPoolExecutor详解​ ScheduledThreadPoolExecutor继承自ThreadPoolExecutor。它主要用来在给定的延迟之后运行任务，或者定期执行任务。ScheduledThreadPoolExecutor的功能与Timer类似，Timer对应的是单个后台线程，而ScheduledThreadPoolExecutor可以在构造函数中指定多个对应的后台线程数。 ScheduledThreadPoolExecutor的运行机制​ ScheduledThreadPoolExecutor的执行主要分为两大部分。 当调用ScheduledThreadPoolExecutor的scheduleAtFixedRate()方法或者schedWithFixedDelay()方法时，会向ScheduledThreadPoolExecutor的DalayQueue添加一个实现了RunnableScheduledFutur接口的ScheduledFutureTask。 线程池中的线程从DelayQueue中获取ScheduledFutureTask，然后执行任务。 ScheduledThreadPoolExecutor的实现​ ScheduledFutureTask主要包含3个成员变量。 long型成员变量time，表示这个任务将要被执行的具体时间。 long型成员变量sequenceNumber，表示这个任务被添加到ScheduledThreadPoolExecutor中的序号。 long型成员变量period，表示任务执行的间隔周期。 DelayQueue封装了一个PriorityQueue，这个PriorityQueue会对队列中的ScheduledFutureTask进行排序。排序时，time小的排在前面(时间早的任务将被先执行)。如果两个ScheduledFutureTask的time相同，就比较sequenceNumber，sequenceNumber小的排在前面(也就是说，如果两个任务的执行时间相同，那么先提交的任务将被先执行)。 FutureTask详解​ Future接口和实现FutureTask类，代表异步计算的结果。 FutureTask简介​ FutureTask除了实现Future接口外，还实现了Runnable接口。因此，FutureTask可以交给Executor执行，也可以由调用线程直接执行(FutureTask.run())。根据FutureTask.run()方法被执行的时机，有3种状态。 未启动。FutureTask.run()方法还没有被执行之前，FutureTask处于未启动状态。当创建一个FutureTask，且没有执行FutureTask.run()方法之前，这个FutureTask处于未启动状态。 已启动。FutureTask.run()方法被执行的过程中，FutureTask处于已启动状态。 已完成。FutureTask.run()方法执行完后正常结束，或被取消(FutureTask.cancel(…))，或执行FutureTask.run()方法时抛出异常而异常结束，FutureTask处于已完成状态。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(八)Java中的线程池","slug":"Java中的线程池","date":"2019-04-10T07:45:25.000Z","updated":"2021-01-14T09:37:05.511Z","comments":true,"path":"2019/04/10/Java中的线程池/","link":"","permalink":"/2019/04/10/Java中的线程池/","excerpt":"","text":"使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的课管理型。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的实现原理​ 当提交一个新任务到线程池时，线程池的处理流程如下。 线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。如果核心线程池都在执行任务，则进入下个流程。 线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。 线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。 ThreadPoolExecutor执行execute方法分下面4种情况。 如果当前运行的线程少于corePoolSize，则创建新线程来执行任务(执行这一步骤需要获取全局锁)。 如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue。 如果无法将任务加入BlockingQueue(队列已满)，则创建新的线程来处理任务(执行这一步骤需要获取全局锁)。 如果创建新线程将使当前运行的线程超出maximumPoolSize，任务将被拒绝，并调用RejectedExecutionHandler.rejectedExecution()方法。 工作线程：线程池创建线程时，会将线程封装成工作线程Worker，Worker在执行完任务后，还会循环获取工作队列里的任务来执行。 ​ 线程池 中的线程执行任务分两种情况： 在execute()方法中创建一个线程时，会让这个线程执行当前任务。 这个线程执行完1的任务后，会反复从BlockingQueue获取任务来执行。 线程池的使用线程池的创建​ 通过ThreadPoolExecutor来创建一个线程池。 1new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, milliseconds, runnableTaskQueue, handler); 1.corePoolSize(线程池的基本大小)：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。 2.runnableTaskQueue(任务队列)：用于保存等待执行的任务的阻塞队列。可以选择以下几个阻塞队列。 ArrayBlockingQueue:是一个基于数组结构的有界阻塞队列，此队列按FIFO(先进先出)原则对元素进行排序。 LinkedBlockingQueue:一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。 SynchrnonousQueue:一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。 PriorityBlockingQueue:一个具有优先级的无限阻塞队列。 3.maximumPoolSize(线程池最大数量)：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。 4.ThreaFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建的线程设置更有意义的名字。使用开源框架guava提供的ThreadFactoryBuilder可以快速给线程池里的线程设置有意义的名字。 1new ThreadFactoryBuilder().setNameFormat(\"XX-task-%d\").build(); 5.RejectedExecutionHandler(饱和策略)：当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。在JDK1.5中Java线程池框架提供了以下4种策略。 AbortPolicy:直接抛出异常。 CallerRunsPolicy:只用调用者所在线程来运行任务。 DiscardOldestPolicy:丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy:不处理，丢弃掉。 可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录日志或持久化存储不能处理的任务。 keepAliveTime(线程活动保持时间):线程池的工作线程空闲后，保持存活的时间。所以，如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率。 TimeUnit(线程活动保持时间的单位):可选的单位有天(DAYS)、小时(HOURS)、分钟(MINUTES)、毫秒(MILLISESECONDS)、微秒(MISCROSECONDS，千分之一毫秒)和纳秒(NANOSECONDS，千分之一微秒)。 向线程池提交任务​ 可以使用两个方法向线程池提交任务，分别为execute()和submit()方法。 ​ execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。 123456threadsPool.execute(new Runnable()&#123; @Override public void run()&#123; //TODO Auto-generated method stub &#125; &#125;); ​ submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future()的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get(long timeout, TimeUnit unit)方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 1234567891011Future&lt;Object&gt; future = executor.submit(harReturnValuetask); try &#123; Object s = future.get(); &#125; catch (InterruptedException e) &#123; //处理中断异常 &#125; catch (ExecutionException e)&#123; //处理无法执行任务异常 &#125; finally &#123; //关闭线程池 executor.shutdown(); &#125; 关闭线程池​ 可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，而shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。 合理地配置线程池​ 要想合理地配置线程池，就必须首先分析任务特性，可以从以下几个角度来分析。 任务的性质：CPU密集型任务、IO密集型任务和混合型任务。 任务的优先级：高、中和低。 任务的执行时间：长、中和短。 任务的依赖性：是否依赖其他系统资源，如数据库连接。 性质不同的任务可以用不同规模的线程池分开处理。CPU密集型任务应配置尽可能小的线程，如配置Ncpu+1个线程的线程池。由于IO密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如2*Ncpu。混合型的任务，如果可以拆分，将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐量将高于串行执行的吞吐量。如果这两个任务执行时间相差太大，则没必要进行分解。可以通过Runtime.getRuntime().availableProcessors()方法获取当前设备的CPU个数。 ​ 优先级不同的任务可以使用优先级队列PriorityBlockingQueue来处理。它可以让优先级高的任务先执行。 ​ 执行时间不同的任务可以交给不同规模的线程池来处理，或者可以使用优先级队列，让执行时间短的任务先执行。 ​ 依赖数据库连接池的任务，因为线程提交SQL后需要等待数据库返回结果，等待的时间越长，则CPU空闲时间就越长，那么线程数应该设置得越大，这样才能更好地利用CPU。 ​ 建议使用有界队列。有界队列能增加系统的稳定性和预警能力，可以根据需要设大一点。 线程池的监控​ 可以通过以下属性队线程池进行监控。 taskCount：线程池需要执行的任务数量。 completedTaskCount：线程池在运行过程中已完成的任务数量，小于或等于taskCount。 largesPoolSize：线程池里曾经创建过的最大线程数量。通过这个数据可以知道线程池是否曾经满过。如该数值等于线程池的最大大小，则表示线程池曾经满过。 getPoolSize：线程池的线程数量。如果线程池不销毁的话，线程池里的线程不会自动销毁，所以这个大小只增不减。 getActiverCount：获取活动的线程数。 corePoolSize(线程池的基本大小)：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。 runnableTaskQueue(任务队列)：用于保存等待执行的任务的阻塞队列。 maximunPoolSize(线程池最大数量)：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。 ThreadFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。使用开源框架guava提供的ThreadFactoryBuilder可以快速给线程池里的线程设置有意义的名字。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(七)Java中的并发工具类","slug":"Java中的并发工具类","date":"2019-04-09T08:16:06.000Z","updated":"2021-01-14T06:32:14.957Z","comments":true,"path":"2019/04/09/Java中的并发工具类/","link":"","permalink":"/2019/04/09/Java中的并发工具类/","excerpt":"","text":"等待多线程完成的CountDownLatch​ CountDownLatch允许一个或多个线程等待其他线程完成操作。 12345678910111213141516public class CountDownLatchTest &#123; public static void main(String[] args) throws Exception&#123; CountDownLatch c = new CountDownLatch(2); new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(1); c.countDown(); System.out.println(2); c.countDown(); &#125; &#125;).start(); c.await(); System.out.println(\"3\"); &#125;&#125; ​ CountDonwLatch的构造函数接收一个int类型的参数作为计数器，如果想等待N个点完成，就传入N。 ​ 计数器必须大于等于0，只是等于0的时候，计数器就是零，调用await方法时不会阻塞当前线程。CountDownLatch不可能重新初始化或者修改CountDownLatch对象的内部计数器的值。一个线程调用countDown方法happen-before，另外一个线程调用await方法。 同步屏障CyclicBarrier​ CyclicBarrier要做的事情是，让一组线程到达一个屏障(也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。 1.CyclicBarrier简介​ CyclicBarrier默认的构造方法是CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达了屏障，然后当前线程被阻塞。 1234567891011121314151617181920212223242526public class CyclicBarrierTest &#123; public static void main(String[] args) &#123; CyclicBarrier c = new CyclicBarrier(2); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(1); &#125; &#125;).start(); try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(2); &#125;&#125; ​ 因为主线程和子线程的调度是由CPU决定的，两个线程都有可能先执行。如果把new CyclicBarrier(2)修改成new CyclicBarrier(3)，则主线程和子线程会永远等待，因为没有第三个线程执行await方法，即没有第三个线程到达屏障，所以之前到达屏障的两个线程都不会继续执行。 ​ CyclicBarrier还提供一个更高级的构造函数CyclicBarrier(int parties, Runnable barrierAction)，由于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景。 1234567891011121314151617181920212223242526272829303132public class CyclicBarrierTest2 &#123; public static void main(String[] args) &#123; CyclicBarrier c = new CyclicBarrier(2, new A()); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(1); &#125; &#125;).start(); try &#123; c.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(2); &#125; static class A implements Runnable&#123; @Override public void run() &#123; System.out.println(3); &#125; &#125;&#125; ​ 因为CyclicBarrier设置了拦截线程的数量是2，所以必须等代码中的第一个线程和线程A都执行完后，才会继续执行主线程，然后输出2。 2.CyclicBarrier的应用场景​ CyclicBarrier可以用于多线程计算数据，最后合并计算结果的场景。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 银行流水处理服务类 * */public class BankWaterService implements Runnable &#123; /** * 创建4个屏障，处理完后执行当前类的run方法 */ private CyclicBarrier c = new CyclicBarrier(4, this); /** * 假设只有4个sheet，所以只启动4个线程 */ private Executor executor = Executors.newFixedThreadPool(4); /** * 保存每个sheet计算出的银流结果 */ private ConcurrentHashMap&lt;String, Integer&gt; sheetBankWaterCount = new ConcurrentHashMap&lt;&gt;(); private void count() &#123; for (int i = 0; i &lt; 4; i++) &#123; executor.execute(() -&gt; &#123; sheetBankWaterCount.put(Thread.currentThread().getName(), 1); try &#123; c.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125; @Override public void run() &#123; int result = 0; for (Map.Entry&lt;String, Integer&gt; sheet : sheetBankWaterCount.entrySet()) &#123; result += sheet.getValue(); &#125; // 将结果输出 sheetBankWaterCount.put(\"result\", result); System.out.println(result); &#125; public static void main(String[] args) &#123; BankWaterService bankWaterCount = new BankWaterService(); bankWaterCount.count(); &#125;&#125; 3.CyclicBarrier和CountDownLatch的区别​ CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置。 ​ CyclicBarrier还提供其他有用的方法，比如getNumberWaiting方法可以获得CyclicBarrier阻塞的线程数量。isBroken()方法用来了解阻塞的线程是否被中断。 1234567891011121314151617181920212223public class CyclicBarrierTest3 &#123; static CyclicBarrier c = new CyclicBarrier(2); public static void main(String[] args) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; c.await(); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); thread.start(); thread.interrupt(); try &#123; c.wait(); &#125; catch (Exception e) &#123; System.out.println(c.isBroken()); &#125; &#125;&#125; 控制并发线程数的Semaphore1.应用​ Semaphore是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。 12345678910111213141516171819202122public class SemaphoreTest &#123; private static final int THREAD_COUNT = 30; private static ExecutorService threadPool = Executors.newFixedThreadPool(THREAD_COUNT); private static Semaphore s = new Semaphore(10); public static void main(String[] args) &#123; for (int i = 0; i &lt; THREAD_COUNT; i++) &#123; threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; s.acquire(); System.out.println(\"save data\"); s.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; threadPool.shutdown(); &#125;&#125; ​ 虽然有30个线程在执行，但是只允许10个并发执行。Semaphore的构造方法Semaphore(int permits)接受一个整型的数字，表示可用的许可证数量。Semaphore的用法很简单，首先线程使用Semaphore的acquire()方法获取一个许可证，使用完之后调用release()方法归还许可证。还可以用tryAcquire()方法尝试获取许可证。 2.其他方法 intavailablePermits():返回此信号量中当前可用的许可证数。 intgetQueueLength():返回正在等待获取许可证的线程数。 booleanhasQueuedThreads():是否有线程正在等待获取许可证。 void reducePermits(int reduction):减少reduction个许可证，是个protected方法。 Collection getQueuedThreads():返回所有等待获取许可证的线程集合，是个proetectedf昂发。 线程间交换数据的Exchanger​ Exchanger是一个用于线程间协作的工具类。Exchanger用于进行线程间的数据交换。它提供一个同步点，在这个同步点，两个线程可以交换彼此的数据。这两个线程通过exchange方法交换数据，如果第一个线程先执行exchange()方法，它会一致等待第二个线程也执行exchange方法，当两个线程到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据传递给对方。 12345678910111213141516171819202122232425262728293031public class ExchangerTest &#123; public static void main(String[] args) &#123; Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;(); ExecutorService threadPool = Executors.newFixedThreadPool(2); threadPool.execute(new Runnable() &#123; @Override public void run() &#123; //A录入银行流水数据 String A = \"银行流水A\"; try &#123; exchanger.exchange(A); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); threadPool.execute(new Runnable() &#123; @Override public void run() &#123; String B = \"银行流水B\"; try &#123; String A = exchanger.exchange(\"B\"); System.out.println(\"A和B数据是否一致:\" + A.equals(B) + \",A录入的是:\" + A + \",B录入的是\" + B); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); threadPool.shutdown(); &#125;&#125;","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(六)Java中的12个原子操作类","slug":"Java中的12个原子操作类","date":"2019-04-09T03:39:46.000Z","updated":"2021-01-04T08:51:59.681Z","comments":true,"path":"2019/04/09/Java中的12个原子操作类/","link":"","permalink":"/2019/04/09/Java中的12个原子操作类/","excerpt":"","text":"原子更新基本类型类​ 使用原子的方式更新基本类型，Atomic包提供了以下3个类。 AtomicBoolean：原子更新布尔型 AtomicInteger：原子更新整型 AtomicLong：原子更新长整型 方法 描述 int addAndGet(int delta) 以原子方式将输入的数值与实例中的值相加，并返回结果 boolean compareAndSet(int expect, int update) 如果输入的数值等于预期值，则以原子方式将该值设置为输入的值 int getAndIncrement() 以原子方式将当前值加1，注意，这里返回的是自增前的值 void lazySet(int newValue) 最终会设置成new Value，使用lazySet设置值后，可能导致其他线程在之后的一小段时间内还是可以读到旧的值 int getAndSet(int newValue) 以原子方式设置为newValue的值，并返回旧值 12345678910public class AtomicIntegerTest &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger = new AtomicInteger(1); System.out.println(atomicInteger.getAndIncrement()); System.out.println(atomicInteger.get()); atomicInteger.addAndGet(5); System.out.println(atomicInteger.get()); &#125;&#125; 原子更新数组​ 通过原子的方式更新数组里的某个元素，Atomic包提供了以下3个类。 AtomicIntegerArray：原子更新整型数组里的元素 AtomicLongArray：原子更新长整型数组里的元素 AtmoicReferenceArray：原子更新引用类型数组里的元素 方法 描述 int addAndGet(int i, int delta) 以原子方式将输入值与数组中索引i的元素相加 boolean compareAndSet(int i, int expect, int update) 如果当前值等于预期值，则以原子方式将数组位置i的元素设置成update值 12345678910public class AtomicIntegerArrayTest &#123; public static void main(String[] args) &#123; int[] value = new int[]&#123;1, 2&#125;; AtomicIntegerArray integerArray = new AtomicIntegerArray(value); integerArray.getAndSet(0, 3); System.out.println(integerArray.get(0)); System.out.println(value[0]); &#125;&#125; ​ 数组value通过构造方法传递进去，然后AtomicIntegerArray会将当前数组复制一份，所以当AtomicIntegerArray对内部的数组元素进行修改时，不会影响传入的数组。 原子更新引用类型​ 原子更新基本类型的AtomicInteger，只能更新一个变量，如果要原子更新多个变量，就需要使用这个原子更新引用类型提供的类。Atomic包提供了以下3个类。 AtomicReference：原子更新引用类型 AtomicReferenceFieldUpdater：原子更新引用类型里的字段 AtomicMarkableReference：原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和引用类型。构造方法是AtomicMarkableReference(V initialRef, boolean initialMark)。 12345678910111213141516171819202122232425public class AtomicReferenceTest &#123; public static void main(String[] args) &#123; AtomicReference&lt;User&gt; userAtomicReference = new AtomicReference&lt;&gt;(); User user = new User(\"张三\", 18); userAtomicReference.set(user); User updateUser = new User(\"李四\", 20); userAtomicReference.compareAndSet(user, updateUser); System.out.println(userAtomicReference.get().getName()); System.out.println(userAtomicReference.get().getOld()); &#125; static class User&#123; private String name; private int old; public User(String name, int old)&#123; this.name = name; this.old = old; &#125; public String getName() &#123; return name; &#125; public int getOld() &#123; return old; &#125; &#125;&#125; 原子更新字段类​ 如果需原子地更新某个类里的某个字段时，就需要使用原子更新字段类，Atomic包提供了以下3个类进行原子字段更新。 AtomicIntegerFieldUpdater：原子更新整型的字段的更新器 AtomicLongFieldUpdate：原子更新整型字段的更新器 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于原子的更新数据和数据的版本号，可以解决使用CAS进行原子更新时可能出现的ABA问题 要想原子地更新字段需要两步。第一步，因为原子更新字段类都是抽象类，每次使用的时候必须使用静态方法newUpdater()创建一个更新器，并且需要设置想要更新的类和属性。第二步，更新类的字段(属性)必须使用public volatile修饰符。 12345678910111213141516171819202122public class AtomicIntegerFieldUpdaterTest &#123; public static void main(String[] args) &#123; AtomicIntegerFieldUpdater&lt;User&gt; a = AtomicIntegerFieldUpdater.newUpdater(User.class, \"old\"); User user = new User(\"张三\", 18); System.out.println(a.getAndIncrement(user)); System.out.println(a.get(user)); &#125; public static class User &#123; private String name; public volatile int old; public User(String name, int old) &#123; this.name = name; this.old = old; &#125; public String getName() &#123; return name; &#125; public int getOld() &#123; return old; &#125; &#125;&#125;","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(五)Java并发容器和框架","slug":"Java并发容器和框架","date":"2019-04-03T06:20:05.000Z","updated":"2020-12-31T09:07:11.416Z","comments":true,"path":"2019/04/03/Java并发容器和框架/","link":"","permalink":"/2019/04/03/Java并发容器和框架/","excerpt":"","text":"ConcurrentHashMap的实现原理与使用ConcurrentHashMap的结构​ ConcurrentHashMap是由Segment数组结构和HashEntry数据结构组成。Segment是一种可重入锁(ReentrantLock)，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素,当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁。 ConcurrentHashMap的操作1.get操作 ​ Segment的get操作实现非常简单和高校。先经过一次再散列，然后使用这个散列值通过散列运算定位到Segment，再通过散列算法定位到元素。get操作的高效之处在于整个get过程不需要加锁，除非读到的值是空才会加锁重读。因为get方法里将要使用的共享变量都定义为volatile类型。定义成volatile的变量，能够在线程之间保持可见性，能够被多线程同时读，并保证不会读到过期的值，但是只能被单线程写(有一种情况可以被多线程写，就是写入的值不依赖于原值)，在get操作里只需要读不需要写共享变量count和value，所以可以不用加锁。之所以不会读到过期的值，是因为根据Java内存模型的happen before原则，对volatile字段的写入操作优先于读操作，即使两个线程同时修改和获取volatile变量，get操作也能拿到最新的值。 2.put操作 ​ 由于put方法里需要对共享变量进行写入操作，所以为了线程安全。在操作共享变量时必须加锁。put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作需要经历两个步骤，第一步判断是否需要对Segment里的HashEntry数据进行扩容，第二步定位添加元素的位置，然后将其放在HashEntry数组里。 ​ (1)是否需要扩容 ​ 插入元素前会先判断Segment里的HashEntry数组是否超过容量(threshold)，如果超过阈值，则对数组进行扩容。 ​ (2)如何扩容 ​ 在扩容的时候，首先会创建一个容量是原来容量两倍的数组，然后将原数组里的元素进行再散列后插入到新的数组里。为了高效，ConcurrentHashMap不会对整个容器进行扩容，而只对某个segment进行扩容。 ConcurrentLinkedQueue​ 在并发编程中，有时候需要使用线程安全的队列。如果要实现一个线程安全的队列有两种方式：一种是使用阻塞算法，另一种是使用非阻塞算法。使用阻塞算法的队列可以用一个锁（入队和出队用同一把锁）或两个锁（入队和出队用不同的锁）等方式来实现。非阻塞的实现方式则可使用循环CAS的方式来实现。 ​ ConcurrentLinkedQueue是一个基于链接节点的无界线程安全队列，它采用先进先出的规则对节点进行排序。当添加一个元素的时候，会添加到队列的尾部；当获取一个元素时，会返回队列头部的元素。采用了“wait-free”算法（即CAS算法）来实现。 ConcurrentLinkedQueue的结构​ ConcurrentLinkedQueue由head节点和tail节点组成，每个节点(Node)由节点元素(item)和指向下一个节点(next)的引用组成，节点与节点之间通过next关联起来，从而组成一张链表结构的队列。默认情况下head节点存储的元素为空，tail节点等于head节点。 1private transient volatile Node&lt;E&gt; tail = head; 入队列1.入队列的过程 ​ 入队列就是将入队节点添加到队列的尾部。 2.定位尾节点 ​ tail节点并不总是尾节点，所以每次入队都必须先通过tail节点来找到尾节点。尾节点可能是tail节点，也可能是tail节点的next节点。 3.设置入队节点为尾节点 ​ p.casNext(null,n)方法用于将入队节点设置为当前队列尾节点的next节点，如果p是null，表示p是当前队列的尾节点，如果不为null，表示有其他线程更新了尾节点，则需要重新获取当前队列的尾节点。 出队列​ 出队列的就是从队列里返回一个节点元素，并清空该节点对元素的引用。 ​ 首先获取头节点的元素，然后判断头节点是否为空，如果为空，表示另外一个线程已经进行了一次出队操作将该节点的元素取走，如果不为空，则使用CAS的方式将头节点的引用设置成null，如果CAS成功，则直接返回头节点的元素，如果不成功，表示另外一个线程已经进行了一次出队操作更新了head节点，导致元素发生了变化，需要重新获取头节点。 Java中的阻塞队列定义​ 阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作支持阻塞的插入和移除方法。 ​ 1)支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。 ​ 2)支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。 ​ 阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器。 方法/处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e, time, unit) 移除方法 remove() poll() take() poll(time, unit) 检查方法 element() peek() 不可用 不可用 抛出异常：当队列满时，如果再往队列里插入元素，会抛出IllegalStateException(“Queue full”)异常。当队列空时，从队列里获取元素会抛出NoSuchElementException异常。 返回特殊值：当往队列插入元素时，会返回元素是否插入成功，成功返回true。如果是移除方法，则是从队列里取出一个元素，如果没有返回null。 一直阻塞：当阻塞队列满时，如果生产者线程往队列里put元素，队列会一直阻塞生产者线程，直到队列可用或者响应中断退出。当队列空时，如果消费者线程从队列里take元素，队列会阻塞住消费者线程，直到队列不为空。 超时退出：当阻塞队列满时，如果生产者线程往队列里插入元素，队列会阻塞生产者线程一段时间，如果超过了指定时间，生产者线程就会退出。 Java里的阻塞队列1.ArrayBlockingQueue ​ ArrayBlockingQueue是一个用数组实现的有界阻塞队列。此队列按照先进先出(FIFO)的原则对元素进行排序。默认情况下不保证线程公平的访问队列，所谓公平访问队列是指阻塞的线程，可以按照阻塞的先后顺序访问队列，即先阻塞线程先访问队列。 2.LinkedBlockingQueue ​ LinkedBlockingQueue是一个用链表实现的有界阻塞队列。此队列的默认和最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。 3.PriorityBlockingQueue ​ PriorityBlockingQueue是一个支持优先级的无界阻塞队列。默认情况下元素采取自然顺序升序排序。也可以自定义实现compareTo()方法来指定元素排序规则，或者初始化PriorityBlockingQueue时，指定构造参数Comparator来对元素进行排序。 4.DelayQueue ​ DelayQueue是一个支持延时获取元素的无界阻塞队列。队列使用PriorityQueue来实现。对垒中的元素必须实现Delayed接口，在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。 5.SynchronousQueue ​ SynchronousQueue是一个不存储元素的阻塞队列。每一个put操作必须等待一个take操作，否则不能继续添加元素。它支持公平访问队列，默认情况下线程采用非公平策略访问队列。SynchronousQueue的吞吐量高于LinkedBlockingQueue和ArrayBlockingQueue。 6.LinkedTransferQueue ​ LinkedTransferQueue是一个由链表结构组成的无界阻塞队列TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了tryTransfer和transfer方法。 ​ (1)transfer方法 ​ 如果当前有消费者正在等待接收元素(消费者使用take()方法或带时间限制的poll()方法时)，transfer方法可以把生产者传入的元素立刻transfer(传输)给消费者。如果没有消费者在等待接收元素，transfer方法会将元素放在队列的tail节点，并等到该元素被消费者消费了才返回。 ​ (2)tryTransfer方法 ​ tryTransfer方法是用来试探生产者传入的元素是否能直接传给消费者。如果没有消费者等待接收元素，则返回false。和transfer方法的区别是tryTransfer方法无论消费者是否接收，方法立即返回，而transfer方法是必须等到消费者消费了才返回。 7.LinkedBlockingQueue ​ LinkedBlockingQueue是一个由链表结构组成的双向阻塞队列。所谓双向队列指的是可以从队列的两端插入和移出元素。 Fork/Join框架工作窃取算法​ 工作窃取算法是指某个线程从其他队列里窃取任务来执行。 ​ 工作窃取算法的优点：充分利用线程进行并行计算，减少了线程间的竞争。 ​ 工作窃取算法的缺点：在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且该算法会消耗了更多的系统资源，比如创建多个线程和多个双端队列。 Fork/Join框架​ 1.ForkJoinTask：要使用ForkJoin框架，必须首先创建一个ForkJoin任务。它提供在任务中执行fork()和join()操作的机制。通常情况下，我们不需要直接继承ForkJoinTask类，只需要继承它的子类，Fork/Join框架提供以下两个子类。 ​ RecursiveAction：用于没有返回结果的任务。 ​ RecursiveTask：用于有返回结果的任务。 ​ 2.ForkJoinPool：ForkJoinTask需要通过ForkJoinPool来执行。 ​ 任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进行队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class CountTask extends RecursiveTask&lt;Integer&gt; &#123; // 阈值 private static final int THRESHOLD = 2; private int start; private int end; public CountTask(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; int sum = 0; // 如果任务足够小就计算任务 boolean canCompute = (end - start) &lt;= THRESHOLD; if (canCompute) &#123; for (int i = start; i &lt;= end; i++) &#123; sum += i; &#125; &#125; else &#123; // 如果任务大于阈值，就分裂成两个子任务计算 int middle = (start + end) / 2; CountTask leftTask = new CountTask(start, middle); CountTask rightTask = new CountTask(middle + 1, end); // 执行子任务 leftTask.fork(); rightTask.fork(); // 等待子任务执行完，并得到其结果 int leftResult = leftTask.join(); int rightResult = rightTask.join(); // 合并子任务 sum = leftResult + rightResult; &#125; return sum; &#125; public static void main(String[] args) throws Exception &#123; ForkJoinPool forkJoinPool = new ForkJoinPool(); CountTask countTask = new CountTask(1, 4); ForkJoinTask&lt;Integer&gt; result = forkJoinPool.submit(countTask); System.out.println(result.get()); &#125;&#125; Fork/Join框架的异常处理​ ForkJoinTask在执行的时候可能会抛出异常，但是没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。 123if(task.isCompletedAbnormally())&#123; System.out.println(task.getException()); &#125; ​ getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。 Fork/Join框架的实现原理​ ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责将存放程序提交给ForkJoinPool的任务，而ForkJoinWorkerThread数组负责执行这些任务。 ​ (1)ForkJoinTask的fork方法实现原理 ​ 当调用ForkJoinTask的fork方法时，程序会调用ForkJoinWorkerThread的pushTask方法异步地执行这个任务，然后立即返回结果。 ​ pushTask方法把当前任务存放在ForkJoinTask数组队列里。然后再调用ForkJoinPool的signalWork()方法唤醒或创建一个工作线程来执行任务。 ​ (2)ForkJoinTask的join方法实现原理 ​ Join方法的主要作用是阻塞当前线程并等待获取结果。首先，它调用了doJoin()方法，通过doJoin()方法得到当前任务的状态判断返回什么结果，任务状态有4种：已完成(NORMAL)、被取消(CANCELLED)、信号(SIGNAL)和出现异常(EXCEPTIONAL)。 如果任务状态是已完成，则直接返回任务结果。 如果任务状态是被取消，则直接抛出CancellationException。 如果任务状态是抛出异常，则直接抛出对应的异常。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(四)Java中的锁","slug":"Java中的锁","date":"2019-04-01T02:00:49.000Z","updated":"2020-12-17T09:55:34.601Z","comments":true,"path":"2019/04/01/Java中的锁/","link":"","permalink":"/2019/04/01/Java中的锁/","excerpt":"","text":"Lock接口​ 锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源(但是有些锁可以允许多个线程并发的访问共享资源，比如读写锁)。 ​ Lock的使用： 1234567Lock lock = new ReentrantLock();lock.lock();tyr&#123;&#125; finally&#123;lock.unlock();&#125; ​ 在finally块中释放锁，目的是保证在获取到锁之后，最终能够被释放。 ​ 不要将获取锁的过程写在try块中，因为如果在获取锁(自定义锁的实现)时发生了异常，异常抛出的同时，也会导致锁无故释放。 ​ Lock接口提供的synchronized关键字所不具备的主要特性。 特性 描述 尝试非阻塞地获取锁 当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁 能被中断地获取锁 与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放 超时获取锁 在指定的截止时间之前获取锁，如果截止时间到了仍旧无法获取锁，则返回 ​ Lock是一个接口，它定义了锁获取和释放的基本操作。 方法名称 描述 void lock() 获取锁，调用该方法当前线程将会获取锁，当锁获得后，从该方法返回 void lockInterruptibly() throws InterruptedException 可中断地获取锁，和lock()方法的不同之处在于该方法会响应中断，即在锁的获取中可以中断当前线程 boolean tryLock() 尝试非阻塞的获取锁，调用该方法后立刻返回，如果能够获取则返回true，否则返回false boolean tryLock(long time, TimeUnit unit) throws InterruptedException 超时的获取锁，当前线程在以下3种情况下会返回：1.当前线程在超时时间内获得了锁 2.当前线程在超时时间内被中断 3.超时时间结束，返回false void unlock() 释放锁 Condition newCondition() 获取等待通知组件，该组件和当前的锁绑定，当前线程只有获得了锁，才能调用该组件的wait()方法，而调用后，当前线程将释放锁 队列同步器​ 队列同步器AbstractQueuedSynchronized，是用来构建锁或者其他同步组件的基础框架，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。 ​ 同步器的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来 管理同步状态，在抽象方法的实现过程中免不了要对同步状态进行更改，这时就需要使用同步器提供的3个方法(getState()、setState(int newState)和compareAndSetSate(int expect,int update))来进行操作，因为它们能够保证状态的改变是安全的。同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态。（ReentrantLock、ReentrantReadWriteLock和CountDownLatch等）。 ​ 同步器是实现锁（也可以是任意同步组件）的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义。可以这样理解二者之间的关系：锁是面向使用者的，它定义了使用者与锁交互的接口，隐藏了实现细节；同步器面向的是锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。 队列同步器的接口​ 重写同步器制定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态。 ​ 1.getState()：获取当前同步状态。 ​ 2.setState(int newState)：设置当前同步状态。 ​ 3.compareAndSetState(int expecting update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性。 ​ 同步器可重写的方法 方法名称 描述 protected boolean tryAcquire(int arg) 独占式获取同步状态，实现该方法需要查询当前状态并判断同步状态是否符合预期，然后再进行CAS设置同步状态 protected boolean tryRelease(int arg) 独占式释放同步状态，等待获取同步状态的线程将有机会获取同步状态 protected int tryAcquireShared(int arg) 共享式获取同步状态，返回大于等于0的值，表示获取成功，反之，获取失败 protected boolean tryReleaseShared(int arg) 共享式释放同步状态 protected boolean isHeldExclusively() 当前同步器是否在独占模式下被线程占用，一般该方法表示是否被当前线程所独占 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class Mutex implements Lock &#123; // 静态内部类，自定义同步器 private static class Sync extends AbstractQueuedSynchronizer &#123; // 是否处于占用状态 @Override protected boolean isHeldExclusively() &#123; return getState() == 1; &#125; // 当状态为0的时候获取锁 @Override protected boolean tryAcquire(int arg) &#123; if(compareAndSetState(0, 1))&#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; // 释放锁，锁状态设置为0 @Override protected boolean tryRelease(int arg) &#123; if(getState() == 0)&#123; throw new IllegalArgumentException(); &#125; setExclusiveOwnerThread(null); setState(0); return true; &#125; // 返回一个Condition，每个condition都包含了一个condition队列 Condition newCondition() &#123; return new ConditionObject(); &#125; &#125; // 仅需要将操作代理到Sync上即可 private final Sync sync = new Sync(); @Override public void lock() &#123; sync.acquire(1); &#125; @Override public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; @Override public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(time)); &#125; @Override public void unlock() &#123; sync.release(1); &#125; @Override public Condition newCondition() &#123; return sync.newCondition(); &#125; &#125; ​ 独占锁Mutex是一个自定义同步组件，它在同一时刻只允许一个线程占有锁。Mutex中定义了一个静态内部类，该内部类继承了同步器并实现了独占式获取和释放同步状态。 同步队列​ 同步器依赖内部的同步队列(一个FIFO双向队列)来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。 ​ 同步队列中的节点用来保存获取同步状态失败的线程引用、等待状态以及前驱和后继节点。 属性类型与名称 描述 init waitStatus 等待状态。包含如下状态：1.CANCELLED，值为1，由于在同步队列中等待的线程超时或者被中断，需要从同步队列中取消 等待，节点进入该状态将不会发生变化。2.SIGNAL，值为-1，后继节点的线程处于等待状态，而当前节点的线程如果释放了同步状态或者被取消，将会通知后继节点，使后继节点的线程得以运行。3.CONDITION，值为-2，节点在等待队列中，节点线程等待在Condition上，当其他线程对Condition调用了Signal()方法后，该节点将会从等待队列中转移到同步队列中，加入到对同步状态的获取中。4.PROPAGETE，值为-3，表示下一次共享式同步状态获取将会无条件地传播下去。5.INITIAL，值为0，初始状态。 Node prev 前驱节点，当节点加入同步队列时被设置（尾部添加） Node next 后继节点 Node nextWaiter 等待队列中的后继节点。如果当前节点是共享的，那么这个字段将是一个SHARED常量，也就是说节点类型（独占和共享）和等待队列中的后继节点公用同一个字段 Thread thread 获取同步状态的线程 ​ 节点是构成同步队列的基础，同步器拥有首节点和尾节点，没有成功获取同步状态的线程将会成为节点加入该队列的尾部。同步器提供了一个基于CAS的设置尾节点的方法：compareAndSetTail(Node expect,Node update)，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式与之前的尾节点建立关联。 ​ 同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点。 ​ 设置首是通过 获取同步状态成功的线程来完成的，由于只有一个线程能够成功获取到同步状态，因此设置头节点的方法并不需要使用CAS来保证，它只需要将首节点设置为原首节点的后继节点并断开原首节点的next引用即可。 独占式同步状态获取与释放​ 通过调用同步器的acquire(int arg)方法可以获取同步状态，该方法对中断不敏感，也就是由于线程获取同步状态失败后进入同步队列中，后续对线程进行中断操作时，线程不会从同步队列中移出。 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; ​ 当前线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释放了同步状态之后，会唤醒其后继节点(进而使后继节点重新尝试获取同步状态)。 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; ​ 在获取同步状态时，同步器维护一个同步队列，获取状态失败的线程都会被加入到队列中并在队列中进行自旋；移出队列（或停止自旋）的条件是前驱节点为透节点并且成功获取了同步状态。在释放同步状态时，同步器调用tryRelease(int arg)方法释放同步状态，然后唤醒头节点的后继节点。 共享式同步状态获取与释放​ 通过调用同步器的acquireShared(int arg)方法可以共享式地获取同步状态。在acquireShared(int arg)方法中，同步器调用tryAcquireShared(int arg)方法尝试获取同步状态，tryAcqiureShared(int arg)方法返回值为int类型，当返回值大于等于0时，表示能够获取到同步状态。因此，在共享式获取的自旋过程中，成功获取到同步状态并退出自旋的条件就是tryAcquireShared(int arg)方法返回值大于等于0。在doAcquireShared(int arg)方法的自旋过程中，如果当前节点的前驱为头节点时，尝试获取同步状态，如果返回值大于等于0，表示该次获取同步状态成功并从自旋过程中退出。 1234567891011121314151617181920212223242526272829public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125;private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean interrupted = false; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node)) interrupted |= parkAndCheckInterrupt(); &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125; finally &#123; if (interrupted) selfInterrupt(); &#125;&#125; ​ 通过调用realeaseShared(int arg)方法可以释放同步状态。该方法在释放同步状态之后，将会唤醒后续处于等待状态的节点。对于能够支持多个线程同时访问的并非组件（比如Semaphore），它和独占式主要区别在于tryReleaseShared(int arg)方法必须确保同步状态（或者资源数）线程安全释放，一般是通过循环和CAS来保证的，因为释放同步状态的操作会同时来自多个线程。 1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 独占式超时获取同步状态​ 通过调用同步器的doAcquireNanos(int arg, long nanosTimeout)方法可以超时获取同步状态，即在指定的时间段内获取同步状态，如果获取到同步状态则返回true，否则，返回false。 123456789101112131415161718192021222324252627282930private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (nanosTimeout &lt;= 0L) return false; final long deadline = System.nanoTime() + nanosTimeout; final Node node = addWaiter(Node.EXCLUSIVE); try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC return true; &#125; nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L) &#123; cancelAcquire(node); return false; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; SPIN_FOR_TIMEOUT_THRESHOLD) LockSupport.parkNanos(this, nanosTimeout); if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125; &#125; ​ 该方法在自旋过程中，当节点的前驱节点为头节点时尝试获取同步状态，如果获取成功则从该方法返回，这个过程和独占式同步获取的过程类似，但是在同步状态获取失败的处理上有所不同。如果当前线程获取同步状态失败，则判断是否超时（nanosTimeout小于等于0表示已经超时），如果没有超时，重新计算超时间隔nanosTimeout，然后使当前线程等待nanosTimeout纳秒（当已到设置的超时时间，该线程会从LockSupport.parkNanos(Object blocker,long nanos)方法返回）。 ​ 如果nanosTimeout小于等于spinForTimeoutThreshold(1000纳秒)时，将不会使该线程进行超时等待，而是进入快速的自旋过程。 重入锁​ 重入锁ReentrantLock，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。除此之外，该锁还支持获取锁时的公平和非公平选择。 ​ synchronized关键字隐式的支持重进入，比如一个synchronzied修饰的递归方法，在方法执行时，执行线程在获取了锁之后仍能够连续多次地获得该锁。 ​ ReentrantLock虽然没能像synchronized关键字一样支持隐式的重进入，但是在调用lock()方法时，已经获取到锁的线程，能够再次调用lock()方法获取锁而不被阻塞。 ​ 如果在绝对时间上，先对锁进行获取的请求一定先被满足，那么这个锁是公平的，反之，是不公平的。公平的获取锁，也就是等待时间最长的线程最优先获取锁，也可以说锁获取是顺序的。ReentrantLock提供了一个构造函数，能够控制锁是否是公平的。 ​ 事实上，公平的锁机制往往没有非公平的效率高，但是，并不是任何场景都是以TPS作为唯一的指标，公平锁能够减少“饥饿”发生的概率，等待越久的请求越是能够得到优先满足。 实现重进入​ 重进入是指人意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞。 123456789101112131415161718final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; ​ 通过判断当前线程是否为获取锁的线程来决定获取操作是否成功，如果是获取锁的线程再次请求，则将同步状态值进行增加并返回true，表示获取同步状态成功。成功获取锁的线程再次获取锁，只是增加了同步状态值，这也就要求ReentrantLock在释放同步状态时减少同步状态值。 123456789101112protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 公平与非公平获取锁的区别​ 公平性与否是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是FIFO。 12345678910111213141516171819protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; ​ 该方法与nonfairTryAcquire(int acquires)比较，唯一不同的位置为判断条件多了hasQueuedPredcessorts()方法，即加入了同步队列中当前节点是否有前驱节点的判断，如果该方法返回true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并放释放锁之后才能继续获取锁。 读写锁​ 读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 ​ 一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量。Java并发包提供读写锁的实现是ReentrantReadWriteLock。 特性 说明 公平性选择 支持非公平(默认)和公平的锁获取方式，吞吐量还是非公平优于公平 重进入 该锁支持重进入，以读写线程为例：读线程在获取了读锁之后，能够再次获取读锁。而写线程在获取了写锁之后能够再次获取读写锁，同时也可以获取读锁 锁降级 遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class CacheReadWriterLock &#123; static Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); static ReentrantReadWriteLock rw = new ReentrantReadWriteLock(); static Lock r = rw.readLock(); static Lock w = rw.writeLock(); /** * 获取一个key对应的value * * @param key * @return */ public static final Object get(String key) &#123; r.lock(); try &#123; return map.get(key); &#125; finally &#123; r.unlock(); &#125; &#125; /** * 设置key对应的value，并返回旧的value * * @param key * @param obj * @return */ public static final Object put(String key, Object obj) &#123; w.lock(); try &#123; return map.put(key, obj); &#125; finally &#123; w.unlock(); &#125; &#125; /** * 清空所有内容 */ public static final void clear() &#123; w.lock(); try &#123; map.clear(); &#125; finally &#123; w.unlock(); &#125; &#125;&#125; 读写状态的设计​ 在一个整型变量上维护多种状态，需要“按位切割使用”这个变量，读写锁将变量切分成两个部分，高16位表示读，低16位表示写。读写锁通过位运算迅速确定读和写各自的状态。当前状态为S，当写状态增加1时，等于S+1，当读状态增加1时，等于s+(1&lt;&lt;16)。 写锁的获取与释放​ 写锁是一个支持重进入的排他锁。如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取(读状态不为0)或者该线程不是已经获取写锁的线程，则当前线程进入等待状态。 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; // (Note: if c != 0 and w == 0 then shared count != 0) if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); // Reentrant acquire setState(c + acquires); return true; &#125; if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;&#125; ​ 该方法除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断（exclusiveCount方法）。如果存在读锁，则写锁不能被获取，原因在于：读写锁要确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。 ​ 写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，从而等待的读写线程能够继续访问读写锁，同时前次写线程的修改对后续读写线程可见。 读锁的获取释放​ 读锁是一个支持重进入的共享锁，它能够被多个线程同时获取，在没有其他写线程访问(或者写状态为0)时，读锁总会被成功地获取，而所做的也只是(线程安全的)增加读状态。如果当前线程已经获取了读锁，则增加读状态。如果当前线程在获取读锁时，写锁已被其他线程获取，则进入等待状态。 锁降级​ 锁降级指的是写锁降级成为读锁。锁降级是指把持住(当前拥有的)写锁，再获取到读锁，随后释放(先前拥有的)写锁的过程。 LockSupport工具​ LockSupport定义了一组的公共静态方法，这些方法提供了最基本的线程阻塞和唤醒功能，而LockSupport也成为构建同步组件的基础工具。LockSupport定义了一组以park开头的方法用来阻塞线程，以及unpark(Thread thread)方法来唤醒一个被阻塞的线程。 方法名称 描述 void park() 阻塞当前线程，如果调用unpark(Thread thread)方法或者当前线程被中断，才能从park()方法返回 void parkNanos(long nanos) 阻塞当前线程，最长不超过nanos纳秒，返回条件在park()的基础上增加了超时返回 Void parkUntil(long deadline) 阻塞当前线程，直到deadline时间（从1970年开始到deadline时间的毫秒数） void unpark(Thread thread) 唤醒处于阻塞状态的线程thread Condition接口​ Condition定义了等待/通知两种类型的方法，当前线程调用这些方法时，需要提前获取到Condition对象关联的锁。Condition对象时由Lock对象(调用Lock对象的newCondition()方法)创建出来的，Condition是依赖Lock对象的。 等待队列​ 等待队列是一个FIFO的队列，在队列的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程会释放锁、构造成节点加入等待队列并进入等待状态。 等待​ 调用Condition的await()方法，会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从await()方法返回时，当前线程一定获取了Condition相关联的锁。 通知​ 调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点(首节点)，在唤醒节点之前，会将节点移到同步队列中。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(三)Java并发编程基础","slug":"Java并发编程基础","date":"2019-03-28T13:28:20.000Z","updated":"2020-11-28T07:44:31.581Z","comments":true,"path":"2019/03/28/Java并发编程基础/","link":"","permalink":"/2019/03/28/Java并发编程基础/","excerpt":"","text":"线程简介什么是线程​ 现代操作系统调度的最小单元是线程，也叫轻量级进程，在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量。 线程优先级​ 现代操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配。线程分配到的时间片多少也就决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。 ​ 在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1～10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程。设置线程优先级时，针对频繁阻塞(休眠或者I/O操作)的线程需要设置较高优先级，而偏重计算(需要较多CPU时间或者偏运算)的线程舍之间较低的优先级，确保处理器不会被独占。在不同的JVM以及操作系统上，线程规划会存在差异，有些操作系统甚至会忽略对线程优先级的设定。 线程状态 状态名称 说明 NEW 初始状态，线程被构建，但是还没有调用start()方法 RUNNABLE 运行状态，Java线程将操作系统中的就绪和运行两种状态笼统地称作”运行中” BLOCKED 阻塞状态，表示线程阻塞于锁 WAITING 等待状态，表示线程进入等待状态，进入该状态表示当前线程需要等待其他线程做出一些特定动作(通知或中断) TIME_WATING 超时等待状态，该状态不同于WATING，它是可以在指定的时间自行返回的 TERMINATED 终止状态，表示当前线程已经执行完毕 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ThreadState &#123; public static void main(String[] args) &#123; new Thread(new TimeWaiting(), \"TimeWaitingThread\").start(); new Thread(new Waiting(), \"WaitingThread\").start(); new Thread(new Blocked(), \"BlocckThread-1\").start(); new Thread(new Blocked(), \"BlocckThread-2\").start(); &#125; static class TimeWaiting implements Runnable &#123; @Override public void run() &#123; while (true) &#123; SleepUtils.second(100); &#125; &#125; &#125; static class Waiting implements Runnable &#123; @Override public void run() &#123; while (true) &#123; synchronized (Waiting.class) &#123; try &#123; Waiting.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; static class Blocked implements Runnable &#123; @Override public void run() &#123; synchronized (Blocked.class) &#123; while (true) &#123; SleepUtils.second(100); &#125; &#125; &#125; &#125;&#125;class SleepUtils &#123; public static final void second(long seconds) &#123; try &#123; TimeUnit.SECONDS.sleep(seconds); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 1234567891011\"TimeWaitingThread\" #13 prio=5 os_prio=31 cpu=0.68ms elapsed=26.39s tid=0x00007f87640b0800 nid=0x9d03 waiting on condition [0x0000700002899000] java.lang.Thread.State: TIMED_WAITING (sleeping)\"WaitingThread\" #14 prio=5 os_prio=31 cpu=0.13ms elapsed=26.39s tid=0x00007f87670c6000 nid=0x6003 in Object.wait() [0x000070000299c000] java.lang.Thread.State: WAITING (on object monitor)\"BlocckThread-1\" #15 prio=5 os_prio=31 cpu=0.14ms elapsed=26.38s tid=0x00007f87668d5000 nid=0x6303 waiting on condition [0x0000700002a9f000] java.lang.Thread.State: TIMED_WAITING (sleeping)\"BlocckThread-2\" #16 prio=5 os_prio=31 cpu=0.75ms elapsed=26.38s tid=0x00007f87638ce800 nid=0x6503 waiting for monitor entry [0x0000700002ba2000] java.lang.Thread.State: BLOCKED (on object monitor) Daemon线程​ Dameon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这意味着，当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。可以通过调用Thread.setDaemon(true)将线程设置为Daemon线程。 ​ Daemon线程需要在启动线程之前设置，不能在启动线程之后设置。 ​ Daemon线程被用作完成支持性工作，但是在Java虚拟机退出时Daemon线程中的finally块并不一定会执行。 12345678910111213141516171819public class Daemon &#123; public static void main(String[] args) &#123; Thread thread = new Thread(new DaemonRunner(), \"DaemonRunner\"); thread.setDaemon(true); thread.start(); &#125; static class DaemonRunner implements Runnable &#123; @Override public void run() &#123; try &#123; SleepUtils.second(100); &#125; finally &#123; System.out.println(\"DaemonThread finally run.s\"); &#125; &#125; &#125;&#125; ​ 运行Daemon程序，可以看到没有任何输出。main线程(非Daemon线程)在启动了线程DaemonRunner之后随着main方法执行完毕而终止，而此时Java虚拟机中已经没有非Daemon线程，虚拟机需要退出。Java虚拟机中的所有Daemon线程都需要立即终止，因此DaemonRunner立即终止，但是DaemonRunner中的finally块并没有执行。 ​ 在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑。 启动和终止线程启动线程​ 线程对象在初始化完成之后，调用start()方法就可以启动这个线程。线程start()方法的含义是：当前线程(即parent线程)同步告知Java虚拟机，只要线程规划器空闲，应立即启动调用start()方法的线程。 理解中断​ 中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。其他线程通过调用该线程的interrupt()方法对其进行中断操作。 ​ 线程通过检查自身是否被中断来进行响应，线程通过方法isInterrupted()来进行判断是否被中断，也可以调用静态方法Thread.interrupted()对当前线程的中断标识位进行复位。如果该线程已经处于终结状态，即使该线程被中断过，在调用该线程对象的isInterrupted()时依旧会返回false。 ​ 许多声明抛出InterruptedExcepiton的方法（例如Thread.sleep(longmills)方法）这些方法在抛出InterruptedException之前，Java虚拟机会先将该线程的中断标识位清除，然后抛出InterruptedException，此时调用isInterrupted()方法将返回false。 1234567891011121314151617181920212223242526272829303132333435public class Interrupted &#123; public static void main(String[] args) &#123; Thread sleepThread = new Thread(new SleepRunner(), \"SleepThread\"); sleepThread.setDaemon(true); Thread busyThread = new Thread(new BusyRunner(), \"BusyThread\"); busyThread.setDaemon(true); sleepThread.start(); busyThread.start(); SleepUtils.second(5); sleepThread.interrupt(); busyThread.interrupt(); System.out.println(\"SleepThread interrupted is \" + sleepThread.isInterrupted()); System.out.println(\"BusyThread interrupted is \" + busyThread.isInterrupted()); SleepUtils.second(2); &#125; static class SleepRunner implements Runnable &#123; @Override public void run() &#123; while (true) &#123; SleepUtils.second(10); &#125; &#125; &#125; static class BusyRunner implements Runnable &#123; @Override public void run() &#123; while (true) &#123; &#125; &#125; &#125;&#125; 123456789java.lang.InterruptedException: sleep interrupted at java.base/java.lang.Thread.sleep(Native Method) at java.base/java.lang.Thread.sleep(Thread.java:339) at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446) at other.SleepUtils.second(ThreadState.java:59) at other.Interrupted$SleepRunner.run(Interrupted.java:30) at java.base/java.lang.Thread.run(Thread.java:834)SleepThread interrupted is falseBusyThread interrupted is true ​ 抛出InterruptedException的线程SleetThread，其中断标识位被清除了，而一致忙碌运作的线程BusyThread，中断标识位没有被清除。 过期的suspend()、resume()和stop()​ suspend()、resume()和stop()方法完成了线程的暂停、恢复和终止工作。但是这些API是过期的。 ​ 不建议使用的原因主要有：以suspend()方法为例，在调用后，线程不会释放已经占有的资源（比如锁），而是占有着资源进入睡眠状态，这样容易引发死锁问题。同样，stop()方法在终结一个线程时不会保证线程的资源正常释放，通常是没有给予线程完成资源释放工作的机会，因此会导致程序可能工作在不确定状态下。 安全地终止线程​ 中断操作是一种简便的线程间交互方式，而这种交互方式最适合用来取消或停止任务。除了中断以外，还可以利用一个boolean变量来控制是否需要停止任务并终止该线程。 1234567891011121314151617181920212223242526272829303132333435public class ShutDown &#123; public static void main(String[] args) throws Exception&#123; Runner one = new Runner(); Thread countThread = new Thread(one, \"CountThread\"); countThread.start(); //睡眠1秒，main线程对CountThread进行中断，使CountThread能够感知中断而结束 TimeUnit.SECONDS.sleep(1); countThread.interrupt(); Runner two = new Runner(); countThread = new Thread(two, \"CountThread\"); countThread.start(); //睡眠1秒，main线程对Runner two进行取消，使CountThread能够感知on为false而结束 TimeUnit.SECONDS.sleep(1); two.cancel(); &#125; private static class Runner implements Runnable &#123; private long i; private volatile boolean on = true; public void run() &#123; while (on &amp;&amp; !Thread.currentThread().isInterrupted()) &#123; i++; &#125; System.out.println(\"Count i = \" + i); &#125; public void cancel() &#123; on = false; &#125; &#125;&#125; 线程间通信volatile和synchronized关键字​ 关键字volatile可以用来修饰字段(成员变量)，就是告知程序任何对该变量的访问均需要从共享内存中获取，而对它的改变必须同步刷新回共享内存，它能保证所有线程对变量访问的可见性。 ​ 关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，它保证了线程对变量访问的可见性和排他性。 12345678910111213public class SynchronizedTest &#123; public static void main(String[] args) &#123; synchronized (SynchronizedTest.class) &#123; &#125; m(); &#125; public static synchronized void m() &#123; &#125;&#125; 12345678910111213141516171819202122232425public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: ldc #2 // class other/SynchronizedTest 2: dup 3: astore_1 4: monitorenter //监视器进入，获取锁 5: aload_1 6: monitorexit //监视器退出，释放锁 7: goto 15 10: astore_2 11: aload_1 12: monitorexit 13: aload_2 14: athrow 15: invokestatic #3 // Method m:()V 18: returnpublic static synchronized void m(); descriptor: ()V flags: (0x0029) ACC_PUBLIC, ACC_STATIC, ACC_SYNCHRONIZED Code: stack=0, locals=0, args_size=0 0: return ​ 上面class信息中，对于同步块的实现使用了monitorenter和monitorexit指令，而同步方法则是依靠方法修饰符上的ACC_SYNCHRONIZED来完成的。无论采用哪种方式，其本质是对一个对象的监视器（monitor）进行获取，而这个获取过程是排他的，也就是同一时刻只能有一个线程获取到由synchronized所保护对象的监视器。 ​ 任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用时，执行方法的线程必须先获取到该对象的监视器才能进入同步块或者同步方法，而没有获取到监视器（执行该方法）的线程会被阻塞在同步块和同步方法的入口处，进入BLOCKED状态。 等待/通知机制 方法名称 描述 notify() 通知一个在对象上等待的线程，使其从wait()方法返回，而返回的前提是该线程获取到了对象的锁 notifyAll() 通知所有等待在该对象上的线程 wait() 调用该方法的线程进入WAITING状态，只有等待另外线程的通知或被中断才会返回，需要注意，调用wait()方法后，会释放对象的锁 wait(long) 超时等待一段时间，这里的参数时间是毫秒，也就是等待长达n毫秒，如果没有通知就超时返回 wait(long,int) 对于超时时间更细粒度的控制，可以达到纳秒 ​ 等待/通知机制，是指一个线程A调用了对象O的wait()方法进入等待状态， 而另一个线程B调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 等待/通知 * */public class WaitNotify &#123; static boolean flag = true; static Object lock = new Object(); public static void main(String[] args) throws Exception &#123; Thread waitThread = new Thread(new Wait(), \"WaitThread\"); waitThread.start(); TimeUnit.SECONDS.sleep(1); Thread notifyThread = new Thread(new Notify(), \"NotifyThread\"); notifyThread.start(); &#125; static class Wait implements Runnable &#123; @Override public void run() &#123; //加锁，拥有lock的monitor synchronized (lock) &#123; //当条件不满足时，继续wait，同时释放了lock的锁 while (flag) &#123; try &#123; System.out.println(Thread.currentThread() + \" flag is true. wait @ \" + LocalTime.now()); lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //条件满足时，完成工作 System.out.println(Thread.currentThread() + \" flag is false. running @ \" + LocalTime.now()); &#125; &#125; &#125; static class Notify implements Runnable &#123; @Override public void run() &#123; //加锁，拥有lock的monitor synchronized (lock) &#123; //获取lock的锁，然后进行通知，通知时不会释放lock的锁 //直到当前线程释放了lock后，waitThread才能从wait方法中返回 System.out.println(Thread.currentThread() + \" hold lock. notify @ \" + LocalTime.now()); lock.notifyAll(); flag = false; SleepUtils.second(5); &#125; //再次加锁 synchronized (lock) &#123; System.out.println(Thread.currentThread() + \" hold lock again. sleep @ \" + LocalTime.now()); SleepUtils.second(5); &#125; &#125; &#125;&#125; 1234Thread[WaitThread,5,main] flag is true. wait @ 14:56:31.799002Thread[NotifyThread,5,main] hold lock. notify @ 14:56:32.777289Thread[NotifyThread,5,main] hold lock again. sleep @ 14:56:37.783622Thread[WaitThread,5,main] flag is false. running @ 14:56:42.789056 ​ 1.使用wait()、notify()和notifyAll()时需要先对调用对象加锁。 ​ 2.调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的等待队列。 ​ 3.notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或notifyAll()的线程释放锁之后，等待线程才有机会从wait()返回。 ​ 4.notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll()方法则是将等待队列中所有的线程全部移到同步队列，被移动的线程状态由WAITING变为BLOCKED。 ​ 5.从wait()方法返回的前提是获得了调用对象的锁。 等待/通知的经典范式​ 从WaitNotify示例中可以提炼出等待/通知的经典范式，该范式分为两部分，分别针对等待方（消费者）和通知方（生产者）。 ​ 等待方遵循如下原则： ​ 1.获取对象的锁。 ​ 2.如果条件不满足，那么调用对象的wait()方法，被通知后仍要检查条件。 ​ 3.条件满足则执行对应的逻辑。 123456synchronized(obj)&#123; while(false)&#123; obj.wait(); &#125; //对应的处理逻辑&#125; ​ 通知方遵循如下原则： ​ 1.获得对象的锁。 ​ 2.改变条件。 ​ 3.通知所有等待在对象上的流程。 1234synchronized(obj)&#123; //改变条件 obj.notifyAll();&#125; 管道输入/输出流​ 管道输入/输出流和普通的文件输入/输出流或者网络输入/输出流不同之处在于，它主要用于线程之间的数据传输，而传输的媒介为内存。 ​ 管道输入/输出流主要包括了4种具体实现:PipedOutputStream、PipedInputStream、PipedReader和PipedWriter，前两种面向字节，后两种面向字符。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 管道输入/输出流 * */public class Piped &#123; public static void main(String[] args) throws Exception &#123; PipedWriter out = new PipedWriter(); PipedReader in = new PipedReader(); //将输出流和输入流进行连接，否则在使用时会抛出IOException out.connect(in); Thread printThread = new Thread(new Print(in), \"PrintThread\"); printThread.start(); int receive = 0; try &#123; while ((receive = System.in.read()) != -1) &#123; out.write(receive); &#125; &#125; finally &#123; out.close(); &#125; &#125; static class Print implements Runnable &#123; private PipedReader in; public Print(PipedReader in) &#123; this.in = in; &#125; @Override public void run() &#123; int receive = 0; try &#123; while ((receive = in.read()) != -1) &#123; System.out.print((char) receive); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; ​ 对于Piped类型的流，必须先要进行绑定，也就是调用connec()方法，如果没有将输入/输出流绑定起来，对于该流的访问将会抛出异常。 Thread.join()的使用​ 如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后从thread.join()返回。线程Thread除了提供join()方法之外，还提供了join(long millis)和join(long millis,int nanos)两个具备超时特性的方法。这两个超时方法表示，如果线程thread在给定的超时时间里没有终止，那么将会从该超时方法中返回。 123456789101112131415161718192021222324252627282930313233343536/** * join方法 * */public class Join &#123; public static void main(String[] args) throws Exception &#123; Thread previous = Thread.currentThread(); for (int i = 0; i &lt; 10; i++) &#123; Thread thread = new Thread(new Domino(previous), String.valueOf(i)); thread.start(); previous = thread; &#125; TimeUnit.SECONDS.sleep(5); System.out.println(Thread.currentThread().getName() + \" terminate.\"); &#125; static class Domino implements Runnable &#123; private Thread thread; public Domino(Thread thread) &#123; this.thread = thread; &#125; @Override public void run() &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \" terminate.\"); &#125; &#125;&#125; 1234567891011main terminate.0 terminate.1 terminate.2 terminate.3 terminate.4 terminate.5 terminate.6 terminate.7 terminate.8 terminate.9 terminate. ThreadLocal的使用​ ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。可以通过set(T)方法来设置一个值，在当前线程下再通过get()方法获取到原先的值。 123456789101112131415161718192021222324252627/** * ThreadLocal使用 * */public class Profiler &#123; //第一次get()方法调用时会进行初始化(如果set方法没有调用)，每个线程会调用一次 private static final ThreadLocal&lt;Long&gt; TIME_THREADLOCAL = new ThreadLocal&lt;&gt;() &#123; @Override protected Long initialValue() &#123; return System.currentTimeMillis(); &#125; &#125;; public static final void begin() &#123; TIME_THREADLOCAL.set(System.currentTimeMillis()); &#125; public static final long end() &#123; return System.currentTimeMillis() - TIME_THREADLOCAL.get(); &#125; public static void main(String[] args) throws Exception &#123; Profiler.begin(); TimeUnit.SECONDS.sleep(1); System.out.println(\"Cost: \" + Profiler.end() + \" mills\"); &#125;&#125; ​ Profiler可以被复用在方法调用耗时统计的功能上，在方法的入口前执行begin()方法，在方法调用后执行end()方法，好处是两个方法的调用不用在一个方法或者类中。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(二)Java内存模型","slug":"Java内存模型","date":"2019-03-22T07:11:55.000Z","updated":"2020-11-28T03:55:20.105Z","comments":true,"path":"2019/03/22/Java内存模型/","link":"","permalink":"/2019/03/22/Java内存模型/","excerpt":"","text":"Java内存模型的基础并发编程模型的两个关键问题​ 在并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 ​ 在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过发送消息来显式进行通信。 ​ 同步是指程序中用于控制不同线程间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 ​ Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行。 Java内存模型的抽象结构​ 在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享。局部变量，方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 ​ Java线程之间的通信由Java内存模型(JMM)控制，JMM定义了一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 从源代码到指令序列的重排序 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作上看上去可能是在乱序执行。 上述的1属于编译器重排序，2和3属于处理器重排序。这些重排序可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求Java编译器再生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序。 并发编程模型的分类​ 为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。 屏障类型 指令示例 说明 LoadLoad Barriers Load1;LoadLoad;Load2 确保Load1数据的装载先于Load2及所有后续装载指令的装载 StoreStore Barriers Store1;StoreStore;Store2 确保Store1数据对其他处理器可见（刷新到内存）先于Store2及所有后续续存指令的存储 LoadStore Barriers Load1;LoadStore;Store2 确保Load1数据装载先于Store2及所有后续的存储指令刷新到内存 StoreLoad Barriers Store1;StoreLoad;Load2 确保Store1数据对其他处理器变得可见（指刷新到内存）先于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令 ​ StoreStore Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中。 happens-before简介​ 在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作直接必须要存在happens-before关系。两个操作即可以是在一个线程之内，也可以是在不同线程之间。 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 两个操作之间有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。 重排序​ 重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。 数据依赖性​ 如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。 ​ 编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序。 ​ 这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial语义​ as-if-serial语义的意思是：不管怎么重排序，（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。 顺序一致性顺序一致性内存模型两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 不管程序是否同步，所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 未同步程序的执行顺序​ JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。 ​ 未同步程序在JMM的执行时，整体上是无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有如下几个差异。 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序。 JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。 volatile的内存语义volatile的特性 可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种符合操作不具有原子性。 volatile写-读的内存语义写的语义如下：当写入一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 读的语义如下：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 volatile内存语义的实现 能否重排序 第二个操作 第二个操作 第二个操作 第一个操作 普通读/写 volatile读 volatile写 普通读/写 NO volatile读 NO NO NO volatile写 NO NO 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 锁的内存含义​ 锁是Java并发编程中最重要的同步机制。锁除了让临界区互斥执行外，还可以让释放锁的线程向获取同一个锁的线程发送消息。 锁的释放和获取的内存含义 当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。 锁内存语义的实现12345678910111213141516171819202122232425public class ReentrantLockExample &#123; int a = 0; ReentrantLock lock = new ReentrantLock(); public void writer()&#123; //获取锁 lock.lock(); try &#123; a++; &#125; finally &#123; //释放锁 lock.unlock(); &#125; &#125; public void reader()&#123; //获取锁 lock.lock(); try &#123; int i = a; //方法 &#125; finally &#123; //释放锁 lock.unlock(); &#125; &#125;&#125; ​ 在ReentrantLock中，调用lock()方法获取锁；调用unlock()方法释放锁。 ​ ReentrantLock的实现依赖于Java同步器框架AbstractQueuedSynchronizer。AQS使用一个整型的volatile变量(命名为state)来维护同步状态。 公平锁和非公平锁释放时，最后都要写一个volatile变量state。 公平锁获取时，首先会去读volatile变量。 非公平锁获取时，首先会用CAS更新volatile变量，这个操作同时具有volatile读和volatile写的内存语义。 final域的内存含义final域的重排序规则 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，让这两个操作之间不能重排序。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 写final域的重排序规则​ 写final域的重排序规则禁止把final域的写重排序到构造函数之外。这个规则的实现包含下面2个方面。 JMM禁止编译器把final域的写重排序到构造函数之外。 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外。 ​ 写final域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了，而普通域不具有这个保障。 读final域的重排序规则​ 读final域的重排序规则是，在一个线程中，初次读对象引用与初次读该对象包含的final域，JMM禁止处理器重排序这两个操作(仅针对处理器)。编译器会在读final域操作的前面插入一个LoadLoad屏障。 ​ 读final域的重排序规则可以确保：在读一个对象的final域之前，一定会读包含这个final域的对象的引用。 happens-beforehappens-before的定义 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这么重排序并不非法(JMM允许这种重排序)。 happnes-before规则happens-before规则如下： 程序员顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作 监视器规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()(启动线程)，那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从Thread.join()操作成功返回。 类初始化 通过在Class对象上同步(即获取Class对象的初始化锁)，来控制类或接口的初始化。这个获取锁的线程会一直等待，直到当前线程能够获取到这个初始化锁。 线程A执行类的初始化，同时线程B在初始化锁对应的Condition上等待。 线程A设置state=initialized，然后唤醒在condition中等待的所有线程。 线程B结束类的初始化处理。 线程C执行类的初始化的处理。 JMM的内存可见性保证 单线程程序。单线程程序不会出现内存可见性问题。编译器、runtime和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。 正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性(程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同)。这时JMM关注的重点，JMM通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。 未同步/未正确同步的多线程程序。JMM为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值(0、null、false)。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"(一)Java并发机制的底层实现原理","slug":"Java并发机制的底层实现原理","date":"2019-03-19T01:40:17.000Z","updated":"2020-11-09T08:27:19.784Z","comments":true,"path":"2019/03/19/Java并发机制的底层实现原理/","link":"","permalink":"/2019/03/19/Java并发机制的底层实现原理/","excerpt":"","text":"volatile​ volatile是轻量级的synchronized，它在多处理开发中保证了共享变量的“可见行”。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起上下文的切换和调度。 定义​ Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 术语 英文单词 术语描述 内存屏障 memory barriers 是一组处理器指令，用于实现对内存的顺序限制 缓存行 cache line 缓存中可以分配的最小存储单位。处理器填写缓存行时会加载整个缓存行，需要使用多个主内存读周期 原子操作 atomic operations 不可中断的一个或一系列操作 缓存行填充 cache line fill 当处理器识别到从内存中读取操作数是可缓存的，处理器读取整个缓存行到适当的缓存（L1，L2，L3的或所有） 缓存命中 cache hit 如果进行高度缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数，而不是内存读取 写命中 write hit 当处理器将操作数写回到一个内存缓存的区域时，它首先会检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器将这个操作数写回到缓存，而不是写回到内存，这个操作被称为写命中 写缺失 write misses the cache 一个有效的缓存行被写入到不存在的内存区域 ​ 如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的命令，将这个变量所在缓存行的数据写回到系统内存。在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议。 实现原则1）Lock前缀指令会引起处理器缓存回写到内存。 2）一个处理器的缓存回写到内存会导致其他处理器的缓存无效。 Volatile的使用和优化LinkedTransferQueue（jdk7） 1234567891011121314151617/** 队列中的头部节点 */private transient final PaddedAtomicReference&lt;QNode&gt; head;/** 队列中的尾部节点 */private transient final PaddedAtomicReference&lt;QNode&gt; tail;static final class PaddeeerAtomicReference &lt;T&gt; extends AtomicReference&lt;T&gt; &#123; // 使用很多4个字节的引用追加到64个字节 Object p0, p1, p2, p3, p4, p5, p6, p7, p8, p9, pa, pb, pc, pd, pe; PaddedAtomicReference(T r) &#123; super(r); &#125;&#125;public class AtomicReference &lt;V&gt; implements java.io.Serializable &#123; private volatile V value; // 省略其他代码 &#125; ​ 追加64字节能够提高并发编程的效率的原因：对于intel i7等一些处理器的L1、L2或L3缓存的高速缓存行是64个字节宽，不支持部门填充缓存行，这意味着，如果队列的头节点和尾节点都不足64字节的话，处理器会将它们都读到同一个高速缓存行中，在多处理器下每个处理器都会缓存同样的头、尾节点，当一个处理器试图修改头节点时，会将整个缓存行锁定，那么在缓存一致性机制的作用下，会导致其他处理器不能访问自己高速缓存中的尾节点，而队列的入队和出队操作则需要不停地修改头节点和尾节点，所以在多处理器的情况下将会严重影响到队列的入队和出队效率。使用追加到64字节的方式来填满高速缓冲区的缓存行，避免头节点和尾节电加载到同一个缓存行，使头、尾节点在修改时不会互相锁定。 ​ 在两种场景下不应该使用这种方式： 缓存行非64字节宽的处理器。如P6系列和奔腾处理器，它们的L1和L2高速缓存行是32个字节宽。 共享变量不会被频繁地写。因为使用追加字节的方式需要处理器读取更多的字节到高速缓冲区，这本身就会带来一定的性能消耗，如果共享变量不被频繁写的话，锁的几率也非常小，就没必要通过追加字节的方式来避免相互锁定。 Synchronized​ 利用Synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现在以下三种形式。 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchronized括号里配置的对象。 ​ JVM基于进入和退出Monitor对象来实现方法同步和代码块同步。monitorenter指令是在编译后插入到同步代码块开始的位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 Java对象头​ synchronized用的锁是存在Java对象头里的。如果对象是数组类型，则虚拟机用3个自宽(Word)存储对象头，如果对象是非数组类型，则用2自宽存储。在32位虚拟机中，1字宽澄宇4字节，即32bit。 ​ Java对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位。在运行期间，Mark Word里存储的数据会随着锁标记位的变化而变化。 锁的升级与对比​ 锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级(可以降级)，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。 偏向锁​ 大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当一个线程访问同步块获取锁时，会在对象头和栈桢中的锁记录里存储锁偏向的线程ID，以后该进程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 偏向锁的撤销​ 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁状态或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 关闭偏向锁​ 默认为启用状态，在应用程序启动几秒钟后才激活，可使用JVM参数来关闭延迟:-XX:BiasedLockingStartupDelay=0。确定如果程序里所有的锁通常情况下处于竞争状态，可通过JVM参数关闭偏向锁:-XX:-UseBiasedLocking=false，那么程序默认会进入轻量级锁状态。 轻量级锁轻量级锁加锁​ 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 轻量级锁解锁​ 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前存在锁竞争，锁就会膨胀成重量级锁。 ​ 因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住。 锁的优缺点对比 锁 优点 缺点 适用场景 偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距 如果线程间存在锁竞争，会带来额外的锁撤销的消耗 适用于只有一个线程访问同步块场景 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度 如果线程得不到锁竞争的线程，使用自旋会消耗CPU 追求响应时间 同步块执行速度非常快 重量级锁 线程竞争不使用自旋，不会消耗CPU 线程阻塞，响应时间缓慢 追求吞吐量 同步块执行速度较长 原子操作的实现原理​ 原子(atomic)本意是”不能背进一步分割的最小粒子”，而原子操作意为“不可被中断的一个或一系列操作”。 处理器如何实现原子操作​ 32位IA-32处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。 （1）使用总线锁保证原子性​ 所谓总线锁就是使用处理器提供的一个LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。 （2）使用缓存锁保证原子性​ 在同一时刻，我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，目前处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。 ​ 频繁使用的内存会缓存在处理器的L1、L2和L3高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行，并不需要声明总线锁。 有两种情况不会使用缓存锁定​ 第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line）时，则处理器会调用总线锁定。 ​ 第二种情况是：有些处理器不支持缓存锁定。 Java如何实现原子操作​ 在Java中可以通过锁和循环CAS的方式来实现原子操作。 （1）使用循环CAS实现原子操作​ JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令来实现的。 （2）CAS实现原子操作的三大问题1）ABA问题。因为CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。 2）循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。 3）只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以使用锁。还有一个办法，就是把多个共享变量合并成一个共享变量来操作。 （3）使用锁机制实现原子操作​ 锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。JVM内部实现了很多种锁机制，有偏向锁、轻量级锁和互斥锁。除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。","categories":[{"name":"并发编程笔记","slug":"并发编程笔记","permalink":"/categories/并发编程笔记/"}],"tags":[]},{"title":"设计模式","slug":"设计模式","date":"2019-03-03T07:20:28.000Z","updated":"2019-05-26T06:54:22.612Z","comments":true,"path":"2019/03/03/设计模式/","link":"","permalink":"/2019/03/03/设计模式/","excerpt":"","text":"观察者模式定义观察者模式定义了对象之间的的一对多依赖，这样一来，当一个对象改变状态时，它的所有依赖者都会收到通知并自动更新。 Java内置的观察者模式把对象编程观察者实现观察者接口（java.util.Observer），然后调用任何Observable对象的addObserver()方法。不想再当观察者时，调用deleteObserver()方法。 可观察者送出通知首先利用java.util.Observable接口产生“可观察者”类，然后，需要两个步骤： 先调用setChanged()方法，标记状态已经改变的事实。 然后调用两种notifyObservers()方法中的一个：notifyObservers()或notifyObservers(Object arg) 观察者接收通知实现update(Observable o, Object arg)。主题本身当作第一个变量，好让观察者知道是哪个主题通知它的。第二个参数是传入notifyObservers()的数据对象，如果没有则为空。 装饰者模式定义装饰者模式动态地将责任附加到对象上。若要扩展功能，装饰者提供了比继承更有弹性的替代方案。 装饰者和被装饰对象有相同的超类型 你可以用一个或多个装饰者包装一个对象 既然装饰者和被装饰对象有相同的超类型，所以在任何需要原始对象（被包装的）的场合，可以用装饰过的对象代替它 装饰者可以在所委托被装饰者的行为之前与/或之后，加上自己的行为，以达到特定的目的 对象可以在任何时候被装饰，所以可以在运行时动态地、不限量地用你喜欢的装饰者来装饰对象 开放-关闭原则类应该对扩展开放，对修改关闭 工厂模式定义工厂方法模式定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法让类把实例化推迟到子类。 依赖倒置原则要依赖抽象，不要依赖具体类。 抽象工厂模式抽象工厂模式提供一个接口，用于创建相关或依赖对象的家族，而不需要明确指定具体类。 要点 所有的工厂都是用来封装对象的创建。 简单工厂，虽然不是真正的设计模式，但仍不失为一个简单的方法，可以将客户程序从具体类解耦。 工厂方式使用继承：把对象的创建委托给子类，子类实现工厂方法来创建对象。 抽象工厂使用对象组合：对象的创建被实现在工厂接口所暴露出来的方法中。 所有工厂模式都通过减少应用程序和具体类之间的依赖促进松耦合。 工厂方法允许类将实例化延迟到子类进行。 抽象工厂创建相关的对象家族，而不需要依赖它们的具体类。 单例模式定义单例模式确保一个类只有一个实例，并提供一个全局访问点。 使用静态内部类实现的单例模式 12345678910111213public class Singleton &#123; private static class LazyHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton() &#123; &#125; public Singleton getSingleton() &#123; return LazyHolder.INSTANCE; &#125;&#125; 命令模式定义命令模式将”请求”封装成对象，以便使用不同的请求、队列或者日志来参数化其他对象。命令模式也支持可撤销的操作。 要点 命令模式将发出请求的对象和执行请求的对象解耦。 在被解耦的两者之间是通过命令对象进行沟通的。命令对象封装了接收者的动作被调用。 调用者可以接受命令当作参数，甚至在运行时动态地进行。 命令可以支持撤销，做法是实现一个undo()方法来回到execute()被执行前的状态。 宏命令是命令的一种简单的眼神，允许调用多个命令。宏方法也可以支持撤销。 命令也可以用来实现日志和事务系统。 适配器与外观模式定义适配器模式将一个类的接口，转换成客户期望的另一个接口。适配器让原本接口不兼容的类可以合作无间。 外观模式定义外观模式提供了一个统一的接口，用来访问子系统中的一群接口。外观定义了一个高层接口，让子系统更容易使用。 要点 当需要使用一个现有的类而其接口并不符合需要时，就使用适配器。 当需要简化并统一一个很大的接口或者一群复杂的接口时，使用外观。 适配器改变接口以符合客户的期望。 外观将客户从一个复杂的子系统中解耦。 适配器模式由两种形式：对象适配器和类适配器。类适配器需要用到多重继承。 适配器将一个对象包装起来以改变其接口；装饰者将一个对象包装起来以增加新的行为和责任；而外观将一群对象”包装”起来以简化其接口。 模版方法模式定义模版方法模式在一个方法中定义一个算法的骨架，而将一些步骤延迟到子类中。模版方式使得子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤。 要点 “模版方法”定义了算法的步骤，把这些步骤的实现延迟到子类。 模版方法模式提供了一种代码复用的重要技巧。 模版方法的抽象类可以定义具体方法、抽象方法和钩子。 抽象方法由子类实现。 钩子是一种方法，它在抽象类中不做事，或者只做默认的事情，子类可以选择要不要去覆盖它。 为了防止子类改变模版方法中的算法，可以讲模版方法声明为final。 策略模式和模版方法模式都封装算法，一个用组合，一个用继承。 工厂方法是模版方法的一个特殊版本。 迭代器与组合模式定义迭代器模式提供一种方法顺序访问一个聚合对象中的各个元素，而不暴露其内部的表示。 组合模式定义组合模式允许将对象组合成树形结构来表现”整体/部分”层次结构。组合能让客户以一致的方式处理个别对象以及对象组合。 状态模式定义状态模式允许对象在哪部状态改变时改变它的行为，对象看起来好像修改了它的类。 代理模式定义代理模式为另一个对象提供了一个替身或占位符以控制对这个对象的访问。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"/categories/设计模式/"}],"tags":[]},{"title":"Zuul","slug":"Zuul","date":"2019-02-14T13:25:44.000Z","updated":"2019-02-14T14:58:24.545Z","comments":true,"path":"2019/02/14/Zuul/","link":"","permalink":"/2019/02/14/Zuul/","excerpt":"","text":"为什么需要ZuulZuul作为路由网关组件体现在以下6个方面： Zuul、Ribbon以及Eureak相结合，可以实现智能路由和负载均衡的功能，Zuul能够将请求流量按某种策略分发到集群状态的多个服务实例。 网关将所有服务的API接口统一聚合，并统一对外暴露。 网关服务可以做用户身份认证和权限认证，防止非法请求操作API接口，对服务器起到保护作用。 网关可以实现监控功能，实时日志输出，对请求进行记录。 网关可以用来实现流量监控，在高流量的情况下，对服务进行降级。 API接口从内部服务分离出来，方便做测试。 Zuul的工作原理Zuul是通过Servlet实现的，Zuul通过自定义的ZuulServlet（类似于Spring MVC的DispatchServlet）来对请求进行控制。Zuul的核心是一系列过滤器，可以在Http请求的发起和响应返回期间执行一系列的过滤器。包括以下4种过滤器。 PRE过滤器：它是在请求路由到具体的服务之前执行的，这种类型的过滤器可以做安全验证，例如身份验证、参数验证等。 ROUTING过滤器：它用于将请求路由到具体的微服务实例。默认情况下使用Http Client进行网络请求。 POST过滤器：它是在请求已被路由到微服务后执行的。一般情况下，用作收集统计信息、指标，以及将响应传输到客户端。 ERROR过滤器：它是在其他过滤发生错误时执行的。 搭建Zuul服务引入Eureka Client的起步依赖spring-cloud-starter-eureka、Zuul的起步依赖spring-cloud-starter-zuul、Web功能的起步依赖spring-boot-starter-web以及Spring Boot测试的起步依赖spring-boot-starter-test，pom文件如下 1234567891011121314151617181920&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在程序的启动类EurekaZuulClientApplication加上@EnableEurekaClient注解，开启EurekaClient的功能；加上@EnableZuulProxy注解，开启Zuul的功能。 12345678910@EnableZuulProxy@EnableEurekaClient@SpringBootApplicationpublic class EurekaZuulClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaZuulClientApplication.class, args); &#125;&#125; 在application.yml中配置，配置注册中心eureka-client，程序端口号5000，程序名为servicce-zuul，zuul.routes.hiapi.path为“/hiapi/**”，zuul.routes.hiapi.serviceId为“eureka-client”，这两个配置就可以将以”/hiapi”开头的Url路由到eureka-client服务。 1234567891011121314151617181920eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/server: port: 5000spring: application: name: service-zuulzuul: routes: hiapi: path: /hiapi/** serviceId: eureka-client ribbonapi: path: /rubbibapi/** serviceId: eureka-ribbon-client feignapi: path: /feignapi/** serviceId: eureka-feign-client 在Zuul上配置API接口的版本号，只需在application.yml中增加 1zuul.prefix: /v1 在Zuul上配置熔断器，需实现ZuulFallbackProvider接口。实现该接口有两个方法，一个是getRoute()方法，用于指定熔断功能应用于哪些路由的服务；另一个方法fallbackResponse()为进入熔断功能时执行的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Componentpublic class MyFallbackProvider implements ZuulFallbackProvider &#123; @Override public String getRoute()&#123; return \"eureka-client\"; &#125; @Override public ClientHttpResponse fallbackResponse() &#123; return new ClientHttpResponse() &#123; @Override public HttpStatus getStatusCode() throws IOException &#123; return HttpStatus.OK; &#125; @Override public int getRawStatusCode() throws IOException &#123; return 200; &#125; @Override public String getStatusText() throws IOException &#123; return \"OK\"; &#125; @Override public void close() &#123; &#125; @Override public InputStream getBody() throws IOException &#123; return new ByteArrayInputStream(\"oooops!error, i'm the fallback.\".getBytes()); &#125; @Override public HttpHeaders getHeaders() &#123; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); return headers; &#125; &#125;; &#125;&#125; 在Zuul上使用过滤器，需要继承ZuulFilter，并实现其中的抽象方法，包括filterType()和filterOrder()以及IZuulFilter的shouldFilter()和object run()的两个方法。其中，filterType()即过滤器的类型，分别是“pre””post””routing”和”error”。filterOrder()是过滤顺序，它为一个Int类型的值，值越小，越早执行该过滤器。shouldFilter()表示该过滤器是否过滤逻辑，如果为true，则执行run()方法；false为不执行。run()方法写具体的过滤的逻辑。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class MyFilter extends ZuulFilter &#123; private static Logger log = LoggerFactory.getLogger(MyFilter.class); @Override public String filterType() &#123; return PRE_TYPE; &#125; @Override public int filterOrder() &#123; return 0; &#125; @Override public boolean shouldFilter() &#123; return true; &#125; @Override public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); Object accessToken = request.getParameter(\"token\"); if(accessToken == null)&#123; log.warn(\"token is empty\"); ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(401); try&#123; ctx.getResponse().getWriter().write(\"token is empty\"); &#125;catch (Exception e)&#123; return null; &#125; &#125; log.info(\"ok\"); return null; &#125;&#125;","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Hystrix","slug":"Hystrix","date":"2019-01-09T13:46:58.000Z","updated":"2019-01-09T15:37:18.047Z","comments":true,"path":"2019/01/09/Hystrix/","link":"","permalink":"/2019/01/09/Hystrix/","excerpt":"","text":"HystrixHystrix是Netflix公司开源的一个项目，它提供了熔断器功能，能够阻止分布式系统中出现联动故障。Hystrix是通过隔离服务的访问点阻止联动故障的，并提供了故障的解决方法，从而提高了整个分布式系统的弹性。 Hystrix的设计原则 防止单个服务的故障耗尽整个服务的Servlet容器(如Tomcat)的线程资源。 快速失败机制，如果某个服务出现了故障，则调用该服务的请求快速失败，而不是线程等待。 提供回退方案，在请求发生故障时，提供设定好的回退方案。 使用熔断机制，防止故障扩散到其他服务。 提供熔断器的监控组件Hystrix Dashboard，可以实时监控熔断器的状态。 Hystrix的工作机制当服务的某个API接口的失败次数在一定时间内小于设定的阀值时，熔断器处于关闭状态，该API接口正常提供服务。当该API接口处理请求的失败次数大于阀值时，Hystrix判断该API接口出现了故障，打开熔断器，这时请求该API接口会执行快速失败的逻辑(即fallback回退的逻辑)，不执行业务逻辑，请求的线程不会处于阻塞状态。处于打开状态的熔断器，一段时间后会处于半打开状态，并将一定数量的请求执行正常逻辑。剩余的请求会执行快速失败，若执行正常逻辑的请求失败了，则熔断器继续打开；若成功了，则将熔断器关闭。 在RestTemplate和Ribbon上使用熔断器在pom文件中引入Hystrix的起步依赖spring-cloud-starter-hystrix 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 在启动类加上@EnableHystrix注解开启Hystrix的熔断器功能 12345678910@SpringBootApplication@EnableEurekaClient@EnableHystrixpublic class EurekaRibbonClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaRibbonClientApplication.class, args); &#125;&#125; 修改RibbonService，在hi()方法上加@HystrixCommand注解。有了该注解，hi()方法就启用了Hystrix熔断器的功能。fallbackMethod为处理回退逻辑的方法。在熔断器打开的状态下，会执行fallback逻辑。fallback的逻辑最好是返回一些静态的字符串，不需要处理复杂的逻辑，也不需要远程调度其他服务，这样方便执行快速失败，释放线程资源。如果一定要在fallback逻辑中远程调度其他服务，最好在远程调度其他服务时，也加上熔断器。 1234567891011121314@Servicepublic class RibbonService &#123; @Autowired RestTemplate restTemplate; @HystrixCommand(fallbackMethod = \"hiError\") public String hi(String name)&#123; return restTemplate.getForObject(\"http://eureka-client/hi?name=\"+name, String.class); &#125; public String hiError(String name)&#123; return \"hi,\" + name + \",sorry,sorry!\"; &#125;&#125; 在Feign上使用熔断器在Feign的起步依赖已经引入了Hystrix的依赖，只需要在application.yml配置开启Hystrix的功能。 123feign: hystrix: enabled: true 修改EurekaClientFeign代码，在@FeignClient注解的fallback配置加上快速失败的处理类。 12345@FeignClient(value = \"eureka-client\", configuration = FeignConfig.class, fallback = HyStrix.class)public interface EurekaClientFeign &#123; @GetMapping(value = \"/hi\") String sayHiFromClientEureka(@RequestParam(value = \"name\") String name);&#125; Hystrix类需要实现EurekaCLientFeign接口，并写具体逻辑，以及加上@Component注解，注入IoC容器中。 1234567@Componentpublic class HyStrix implements EurekaClientFeign&#123; @Override public String sayHiFromClientEureka(String name) &#123; return \"hi,\" + name + \",sorry,sorry!\"; &#125;&#125; 使用Hystrix Dashboard监控熔断器的状态在pom文件中加上Actuator、Hystrix Dashboard和Hystrix的起步依赖。 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; 在程序的启动类加上@EnableHystrixDashboard注解。 1234567891011@SpringBootApplication@EnableEurekaClient@EnableHystrix@EnableHystrixDashboardpublic class EurekaRibbonClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaRibbonClientApplication.class, args); &#125;&#125; 使用Turbine聚合监控添加依赖 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-turbine&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 1234567891011121314spring: application.name: service-turbineserver: port: 8769security.basic.enabled: falseturbine: aggregator: clusterConfig: default appConfig: eureka-ribbon-client,eureka-feign-client clusterNameExpression: new String(&quot;default&quot;)eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ turbine.appConfig ：配置Eureka中的serviceId列表，表明监控哪些服务 turbine.aggregator.clusterConfig ：指定聚合哪些集群，多个使用”,”分割，默认为default。可使用http://.../turbine.stream?cluster={clusterConfig之一}访问 turbine.clusterNameExpression ： 1. clusterNameExpression指定集群名称，默认表达式appName；此时：turbine.aggregator.clusterConfig需要配置想要监控的应用名称；2. 当clusterNameExpression: default时，turbine.aggregator.clusterConfig可以不写，因为默认就是default；3. 当clusterNameExpression: metadata[‘cluster’]时，假设想要监控的应用配置了eureka.instance.metadata-map.cluster: ABC，则需要配置，同时turbine.aggregator.clusterConfig: ABC 启动类添加@EnableTurbine，激活对Turbine的支持 123456789@SpringBootApplication@EnableTurbinepublic class EurekaMonitorClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaMonitorClientApplication.class, args); &#125;&#125;","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Feign","slug":"Feign","date":"2019-01-08T12:42:41.000Z","updated":"2019-01-09T13:47:14.516Z","comments":true,"path":"2019/01/08/Feign/","link":"","permalink":"/2019/01/08/Feign/","excerpt":"","text":"编写Feign客户端增加Feign的起步依赖spring-cloud-starter-feig、Eureka Client的起步依赖spring-cloud-starter-eureka、Web功能的起步依赖spring-boot-starter-web，pom文件如下： 12345678910111213141516171819&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.netflix.feign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置application.yml。 123456789server: port: 8765spring: application: name: eureka-feign-clienteureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 在程序启动类EurekFeignClientApplication加上注解@EnableEurekaClient开启Eureka Client的功能，通过注解@EnableFeignClients开启Feign Client的功能。 12345678910@SpringBootApplication@EnableEurekaClient@EnableFeignClientspublic class EurekaFeignClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaFeignClientApplication.class, args); &#125;&#125; 通过以上3个步骤，该程序就具备了Feign的功能。 新建一个EurekaClientFeign的接口，在接口加上@FeignClient来声明一个Feign Client，其中value为远程调用其他服务的服务名，FeignConfig.class为Feign Client的配置类。在接口内部有一个sayHiFromClientEureka()方法，该方法通过Feign来调用eureka-client服务的”/hi”的API接口。 12345@FeignClient(value = \"eureka-client\", configuration = FeignConfig.class)public interface EurekaClientFeign &#123; @GetMapping(value = \"/hi\") String sayHiFromClientEureka(@RequestParam(value = \"name\") String name);&#125; 在FeignConfig类加上@Configuration注解，表明该类是一个配置类，并注入一个BeanName为feignRetryer的Retryer的Bean。注入该Bean之后，Feign在远程调用失败后会进行重试。 123456789import static java.util.concurrent.TimeUnit.SECONDS;@Configurationpublic class FeignConfig &#123; @Bean public Retryer feignRetryer()&#123; return new Retryer.Default(100, SECONDS.toMillis(1), 5); &#125;&#125; 在service层注入EurekaClientFeign的Bean。 12345678910@Servicepublic class HiService &#123; @Autowired EurekaClientFeign eurekaClientFeign; public String sayHi(String name)&#123; return eurekaClientFeign.sayHiFromClientEureka(name); &#125;&#125; 在HiController层注入并调用sayHi()方法。 1234567891011@RestControllerpublic class HiController &#123; @Autowired HiService hiService; @GetMapping(\"/hi\") public String sayHi(@RequestParam(defaultValue = \"zhangsan\", required = false) String name)&#123; return hiService.sayHi(name); &#125;&#125; 访问http://localhost:8765/hi 1hi zhangsan, i am from port 8762 Feign源码实现过程1.首先通过@EnableFeignClients注解开启FeignClient的功能。只有这个注解存在，才会在程序启动时开启对@FeignClient注解的包扫描。 2.根据Feign的规则实现接口，并在接口上面加上@FeignClient注解。 3.程序启动后，会进行包扫描，扫描所有的@FeignClient的注解的类，并将这些信息注入IoC容器中。 4.当接口被调用时，通过JDK的代理来生成具体的RequestTemplate模版对象。 5.根据RequestTemplate再生产Http请求的Request对象。 6.Request对象交给Client去处理，其中Client的网络请求框架可以是HttpURLConnection、HttpClient、OkHttp。 7.最后Client被封装到LoadBalanceClient类，这个类结合类Ribbon做到了负载均衡。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Ribbon","slug":"Ribbon","date":"2019-01-07T13:20:18.000Z","updated":"2019-01-08T12:41:06.141Z","comments":true,"path":"2019/01/07/Ribbon/","link":"","permalink":"/2019/01/07/Ribbon/","excerpt":"","text":"RestTemplateRestTemplate是Spring Resources中一个访问第三方RESTful API接口的网络请求框架。RestTemplate是用来消费REST服务的，所以RestTemplate的主要方法都与REST的Http协议的一些方法紧密相连，例如HEAD、GET、POST、DELETE和OPTIONS等方法，这些方法在RestTemplate类对应的方法为headForHeaders()、getForObject()、postForObject()、put()和delete()等。以下代码可以将返回的JSON字符串转换成一个User对象。 1User user = restTemplate.getForObject(\"https://www.xxx.com/\", User.class); Ribbon负载均衡是指将负载分摊到多个执行单元上，常见的负载均衡有两种方式。一种是独立进程单元，通过负责均衡策略，将请求转发到不同的执行单元上，例如Ngnix。另一种是将负载均衡逻辑以代码的形式封装到服务消费者的客户的，服务消费者客户端维护了一份服务提供者的信息列表。有了信息列表，通过负载均衡策略将请求分摊给多个服务提供者，从而达到负载均衡的目的。 目前NetFlix用于生产环境的Ribbon子模块： ribbon-loadbalancer：可以独立使用或与其他模块一起使用的负载均衡器API。 ribbon-eureka：Ribbon结合Eureka客户端的API，为负载均衡器提供动态服务注册列表。 ribbon-core：Ribbon的核心API。 使用RestTemplate和Ribbon来消费服务引入Eureka Client的起步依赖spring-cloud-starter-eureka、Ribbon的起步依赖spring-cloud-starter-ribbon，以及Web的起步依赖spring-boot-starter-web。 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 配置application.yml，指定程序名、端口号、服务注册地址。 123456789spring: application: name: eureka-ribbon-clientserver: port: 8764eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 作为Eureka Client需要在程序的入口类加上注解@EnableEurekaClient开启Eureka Client功能。 123456789@SpringBootApplication@EnableEurekaClientpublic class EurekaRibbonClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaRibbonClientApplication.class, args); &#125;&#125; 写一个RESTful API接口，在该API接口内部需要调用eureka-client的API接口”/hi”，即服务消费。这时需要将RestTemplate和Ribbon相结合，进行负载均衡。 需要在程序的IoC容器中注入一个restTemplate的Bean，并在这个Bean上加上@LoadBalanced注解，此时RestTemplate就结合了Ribbon开启了负载均衡功能。 12345678@Configurationpublic class RibbonConfig &#123; @Bean @LoadBalanced RestTemplate restTemplate()&#123; return new RestTemplate(); &#125;&#125; 写一个RibbonService类，在该类的hi()方法用restTemplate调用eureka-client的API接口，此时Uri不需要使用硬编码(如ip)，只需要写服务名即可。 123456789@Servicepublic class RibbonService &#123; @Autowired RestTemplate restTemplate; public String hi(String name)&#123; return restTemplate.getForObject(\"http://eureka-client/hi?name=\"+name, String.class); &#125;&#125; 写RibbonController，调用servcie的方法。 123456789101112@RestControllerpublic class RibbonController &#123; @Autowired RibbonService ribbonService; @GetMapping(\"/hi\") public String hi(@RequestParam(required = false, defaultValue = \"zhangsan\") String name)&#123; return ribbonService.hi(name); &#125;&#125; LoadBalanceClient负载均衡器的核心类为LoadBalanceClient，LoadBalanceClient可以获取负载均衡的服务提供者的实例信息。 获取Eureka Client的实例信息时： 123456789101112@RestControllerpublic class RibbonController &#123; @Autowired private LoadBalancerClient loadBalancerClient; @GetMapping(\"/testRibbon\") public String testRibbon()&#123; ServiceInstance instance = loadBalancerClient.choose(\"eureka-client\"); return instance.getHost() + \":\" + instance.getPort(); &#125;&#125; 不从Eureka Client获取注册列表信息时： 在启动类中增加@SpringBootApplication注解。 配置文件application.yml，配置ribbon.eureka.enable为false来禁止调用Eureka Client获取注册列表。配置程序名为stores的服务，通过stores.ribbon.listOfServers来配置这些服务实例的Url。 12345678stores: ribbon: listOfServers: example.com,google.comribbon: eureka: enabled: falseserver: port: 8769 新建RestController类。 123456789101112@RestControllerpublic class RibbonController &#123; @Autowired private LoadBalancerClient loadBalancerClient; @GetMapping(\"/testRibbon\") public String testRibbon()&#123; ServiceInstance instance = loadBalancerClient.choose(\"stores\"); return instance.getHost() + \":\" + instance.getPort(); &#125;&#125; 在浏览器上多次访问http://localhost:8769/testRibbon，浏览器会交替出现以下内容： 12example.com:80google.com:80 在Ribbon中的负载均衡客户端为LoadBalancerClient，在Spring Cloud项目中，负载均衡器Ribbon会默认从Eureka Client的服务注册列表中获取服务的信息，并缓存一份。如果禁止从Eureka获取注册列表信息，则需要自己维护一份服务注册列表信息。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Eureka","slug":"Eureka","date":"2019-01-05T13:13:26.000Z","updated":"2019-10-24T03:22:58.726Z","comments":true,"path":"2019/01/05/Eureka/","link":"","permalink":"/2019/01/05/Eureka/","excerpt":"","text":"Eureka简介什么是EurekaEureka是一个用于服务注册和发现的组件。Eureka分为Eureka Server和Eureka Client，Eureka Server为Eureka服务注册中心，Eureka Client为Eureka客户端。 Eureka的基本架构主要包括以下3种角色 Register Service:：服务注册中心，它是一个Eureka Server，提供服务注册和发现的功能 Provide Service：服务提供者，它是一个Eureka Client，提供服务 Consumer Service：服务消费者，它是一个Eureka Client，消费服务 Eureka Server在pom文件中引入Eureka Server依赖 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在application.yml中做配置。通过server.port制定Eureka Server的端口为8761。在默认情况下，Eureka Server 会向自己注册，这时需要配置eureka.client.registerWithEurkea和eureka.client.fetchRegistry为false，防止自己注册自己。 1234567891011server: port: 8761eureka: instance: hostname: localhost client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 在工程的启动类EurekaServerApplication加上注解@EnableEurekaServer，开启Eureka Server的功能。 123456789@EnableEurekaServer@SpringBootApplicationpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; Eureka Client在pom中引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 在bootstrap.yml做Eureka Client客户端的相关配置，配置了程序名为eureka-client，程序端口为8762，服务注册地址为http://localhost:8761/eureka/ 123456789server: port: 8762spring: application: name: eureka-clienteureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka 在程序的启动类加上@EnableEurekaClient开启Eureka Client功能 123456789@SpringBootApplication@EnableEurekaClientpublic class EurekaClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaClientApplication.class, args); &#125;&#125; Eureka概念 Register–服务注册 当Eureka Client向Eureka Server注册时，Eureka Client提供自身的元数据，比如IP地址、端口、运行状况指标的Url、主页地址等信息。 Renew–服务续约 Eureka Client在默认的情况下会每隔30秒发送一次心跳来进行服务续约。通过服务续约来告知Eureka Server该Eureka Client仍然可用，没有出现故障。 Fetch Registries–获取服务注册列表信息 Eureka Client从Eureka Server获取服务注册表信息，并将其缓存在本地。 Cancel–服务下线 Eureka Client在程序关闭时可以向Eureka Server发送下线请求。发送请求后，该客户端的实例信息将从Eureka Server的服务注册列表中删除。该下线请求不会自动完成，需要在程序中调用: 1DiscoveryManager.getInstance().shutdownComponent(); Eviction–服务剔除 在默认情况下，当Eureka Client连续90秒没有向Eureka Server发送服务续约(即心跳)时，Eureka Server会将该服务实例从服务注册列表中删除。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"/categories/Spring-Cloud/"}],"tags":[]},{"title":"Spring Boot","slug":"Spring-Boot","date":"2018-12-17T13:44:39.000Z","updated":"2019-01-05T14:10:39.215Z","comments":true,"path":"2018/12/17/Spring-Boot/","link":"","permalink":"/2018/12/17/Spring-Boot/","excerpt":"","text":"@SpringBootApplication注解@SpringBootApplication开启了Spring的组件扫描和Spring Boot的自动配置功能呢。实际上，它将三个有用的注解组合在了一起。 @Configuration:标明该类使用Spring基于Java的配置。 @ComponentScan:启用组件扫描。 @EnableAutoConfiguration:开启自动配置。","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"/categories/Spring-Boot/"}],"tags":[]}]}